<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.11" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.43" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://lileicc.github.io/blog/nlp/GAIN/"><meta property="og:site_name" content="Li-Lab Blog"><meta property="og:title" content="Document-level Relation Extraction"><meta property="og:description" content="The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications. Previo..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2024-05-25T23:17:24.000Z"><meta property="article:author" content="Runxin Xu"><meta property="article:tag" content="Relation Extraction"><meta property="article:published_time" content="2020-11-20T00:00:00.000Z"><meta property="article:modified_time" content="2024-05-25T23:17:24.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Document-level Relation Extraction","image":[""],"datePublished":"2020-11-20T00:00:00.000Z","dateModified":"2024-05-25T23:17:24.000Z","author":[{"@type":"Person","name":"Runxin Xu"}]}</script><title>Document-level Relation Extraction | Li-Lab Blog</title><meta name="description" content="The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications. Previo...">
    <link rel="preload" href="/blog/assets/style-Cq8eyGeZ.css" as="style"><link rel="stylesheet" href="/blog/assets/style-Cq8eyGeZ.css">
    <link rel="modulepreload" href="/blog/assets/app-DiCjo-Va.js"><link rel="modulepreload" href="/blog/assets/index.html-CFyAOJxa.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/blog/assets/index.html-n7qbu3dk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DsZ03cqp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-r3wL3lYW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BTmZnjzu.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B3lBlHt5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DkOZIlpg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Dy--DCvM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BGcY7GRM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CnEewnS_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B-7kV_Pa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Dxt46wjr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ailBi3IV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CLkHMPSR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BcxjbFyQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B5tQQqUs.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7GEx1xL_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-X1I--pzi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Cqxm4VEw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-dZRpMDcp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rdxcuHG-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B0QaSsCk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-lbmWsbm6.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BL20rJEw.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-DwhX0xCD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-eyD8xvL-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4T5RsOJr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C4oYB-TX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DT0Vw-X1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ChpH1XCa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CG4zWerc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cDcguak7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BLmhJl3H.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BUnffEMv.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-K8vN4nvk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-izeVRWR8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-p4XBto_y.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-l1eZfZeq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DggTdQeV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BqVwiDmI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ChpRCIc8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5hhedaTR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CIWPrBfh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BYBCTXZh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DWU1ODhk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BbKNZ18w.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BkwXzOb3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jhqGT4dL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CS6gUaSX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CGvZjib9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DkRkbNQ6.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Ic0xQiYz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Bi8bMppE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DPfrum7j.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BYsu2jJc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-chNhRanf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BXwcVWaJ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B6PjGDAO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nmkosgT0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C-1PzENR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BaWZvSMh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-iCm852ld.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BDc3S-C2.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CoxTWtqI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BW3VPrzj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DLbZgarF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DZPCdxOC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CvBuB7g8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BQK4OLG0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-fRbI1bsI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Byllwfsd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DNApUOep.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BHiIQnkE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BuHhtC_q.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DW1rLQdB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CFKdo3WH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B8BC64BL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xb2wQrjg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B6uJhqIN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DrA_NyPl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C7_gn4qD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DPhNyCil.js" as="script"><link rel="prefetch" href="/blog/assets/giscus--_FS5kYt.js" as="script"><link rel="prefetch" href="/blog/assets/auto-CAdRPfCH.js" as="script"><link rel="prefetch" href="/blog/assets/index-wZ-hXvzw.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-SzV8tJDW.js" as="script"><link rel="prefetch" href="/blog/assets/SearchResult-75TAtN8b.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container no-sidebar external-link-icon has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/blog/"><img class="vp-nav-logo" src="/blog/logo.svg" alt><!----><span class="vp-site-name hide-in-pad">Li-Lab Blog</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/" aria-label="Blog Home"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="home" width="1em" height="1em"></iconify-icon>Blog Home<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/category/" aria-label="Category"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="baseline-category" width="1em" height="1em"></iconify-icon>Category<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/tag/" aria-label="Tags"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="tag" width="1em" height="1em"></iconify-icon>Tags<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/timeline/" aria-label="Timeline"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="time" width="1em" height="1em"></iconify-icon>Timeline<!----></a></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/lileicc/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon" name="outlook"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="vp-outlook-dropdown"><!----></div></button></div><!--[--><button type="button" class="search-pro-button" aria-label="Search"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon" name="search"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">Search</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!----><ul class="vp-sidebar-links"></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Document-level Relation Extraction</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Runxin Xu</span></span><span property="author" content="Runxin Xu"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2020-11-20T00:00:00.000Z"></span><span class="page-category-info" aria-label="Category🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color5 clickable" role="navigation">IE</span><!--]--><meta property="articleSection" content="IE"></span><span class="page-tag-info" aria-label="Tag🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color1 clickable" role="navigation">Relation Extraction</span><!--]--><meta property="keywords" content="Relation Extraction"></span><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 5 min</span><meta property="timeRequired" content="PT5M"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!----><div class="vp-toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#challenges">Challenges</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#our-proposed-model-gain">Our proposed Model: GAIN</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#experiments">Experiments</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#dataset">Dataset</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#main-results">Main Results</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#ablation-study">Ablation Study</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#further-analysis">Further Analysis</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#case-study">Case Study</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#conclusion">Conclusion</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#reference">Reference</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content"><p>The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications. Previous methods focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, <strong>extracting relations at the document-level is necessary</strong> for a holistic understanding of knowledge in text. This blog describes a recent work on document-level relation extraction by Zeng et al. EMNLP 2020.</p><p><img src="/blog/assets/image1-Crg46b1B.png" alt="image1"></p><p>Paper: <a href="https://arxiv.org/abs/2009.13752" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2009.13752</a> Code: <a href="https://github.com/DreamInvoker/GAIN" target="_blank" rel="noopener noreferrer">https://github.com/DreamInvoker/GAIN</a></p><h2 id="challenges" tabindex="-1"><a class="header-anchor" href="#challenges"><span>Challenges</span></a></h2><p>There are several major challenges in effective relation extraction at the document-level. The figure below shows an example.</p><div align="center"><img src="/blog/assets/image2-kN-Hs7Ce.png" width="50%" height="50%"></div><ol><li>The subject and object entities involved in a relation may appear in different sentences, e.g., the relation between <strong>Baltimore</strong> and <strong>U.S.</strong>, as well as <strong>Eldersburg</strong> and <strong>U.S</strong>.</li><li>The same entity may be mentioned multiple times in different sentences.</li><li>The identification of many relations requires techniques of logical reasoning, e.g., <strong>Eldersburg</strong> belongs to <strong>U.S.</strong> because <strong>Eldersburg</strong> is located in <strong>Maryland</strong>, and <strong>Maryland</strong> belongs to <strong>U.S.</strong>.</li></ol><h2 id="our-proposed-model-gain" tabindex="-1"><a class="header-anchor" href="#our-proposed-model-gain"><span>Our proposed Model: GAIN</span></a></h2><p>To tackle the challenges, we propose <strong>G</strong>raph <strong>A</strong>ggregation-and-<strong>I</strong>nference <strong>N</strong>etwork (<strong>GAIN</strong>). GAIN consists of double graph, i.e., mention-level graph and entity-level graph. Our intuitions are that: 1) Mention-level graph can model the interactions among mentions across sentences, so that global context can be better captured. 2) Entity-level graph can conduct logical reasoning for certain entity pairs over entities.</p><p><img src="/blog/assets/image3-DWntYVu0.png" alt="image3"></p><p>Our model contains the following four modules.</p><p><strong>Encoding module</strong>. Tokens of the document is represented as the concatenation of word embedding, entity type embedding, and entity id embedding. Then they are fed into the encoder (e.g., LSTM or BERT) to obtain the contextualized representation.</p><p><strong>Mention-level Graph Aggregation Module</strong>. To model the document-level information and interactions among mentions, a heterogeneous mention-level graph is constructed followed by graph convolution network. The graph has two kinds of nodes: 1) <em>Mention node</em>, which refers to one specific entity mention in the document; 2) <em>Document node</em>, which aims to model the overall document information and serves as a pivot for interactions among different mentions. Three types of edges are leveraged to connect these nodes:</p><ul><li><p><em>Intra-Entity Edge</em>: Mentions referring to the same entity are fully connected with intra-entity edges. In this way, the interaction among different mentions of the same entity could be modeled.</p></li><li><p><em>Inter-Entity Edge</em>: Two mentions of different entities are connected with an inter-entity edge if they co-occur in a single sentence. In this way, interactions among entities could be modeled by co-occurrences of their mentions.</p></li><li><p><em>Document Edge</em>: All mentions are connected to the document node with the document edge. With such connections, the document node can attend to all the mentions and enable interactions between document and mentions. Besides, the distance between two mention nodes is at most two with the document node as a pivot. Therefore long-distance dependency can be better modeled.</p></li></ul><p><strong>Entity-level Graph Inference Module</strong>. To explicitly capture the logic reasoning chain of entity pairs over all the entities, we constuct an entity-level graph by merging mention nodes referring to the same entity in the mention-level graph into an entity node. Concretely, to model the logical chain between a certain entity pair, we find out all the two-hop paths between them, in which a path is represented as the concatenation of both forward and backward edges. Then we levelrage attention mechanism to aggregate multiple paths into a reasoning-aware path representation.</p><p><strong>Classification Module</strong>. Since a pair of entities may contain multiple relations, we formulate the task as a multi-label classification. Concretely, we first concatenate the entity, document, and path representations. Then we feed it into a MLP and use sigmoid function to predict the score for all possible relations.</p><p><img src="/blog/assets/image4-D9hjVdcf.png" alt="image4"></p><h2 id="experiments" tabindex="-1"><a class="header-anchor" href="#experiments"><span>Experiments</span></a></h2><h3 id="dataset" tabindex="-1"><a class="header-anchor" href="#dataset"><span>Dataset</span></a></h3><p>We evaluate our model on DocRED (Yao et al., 2019), a large-scale human-annotated dataset for document-level RE constructed from Wikipedia and Wikidata. DocRED has 96 relations types, 132, 275 entities, and 56, 354 relational facts in total. Documents in DocRED contain about 8 sentences on average, and more than 40.7% relation facts can only be extracted from multiple sentences. Moreover, 61.1% relation instances require various inference skills such as logical inference (Yao et al., 2019). we follow the standard split of the dataset, 3, 053 documents for training, 1, 000 for development and 1, 000 for test.</p><h3 id="main-results" tabindex="-1"><a class="header-anchor" href="#main-results"><span>Main Results</span></a></h3><p>We compare the performance among the following models:</p><ul><li><strong>CNN</strong>, <strong>LSTM</strong>, <strong>BiLSTM</strong>, <strong>Context-Aware</strong>, <strong>BERT-RE</strong>, <strong>RoBERTa-RE</strong>, <strong>CorefBERT-RE</strong>, <strong>CorefRoBERTa-RE</strong>: Using different encoding mechanisms to simply encode the whole document and extract relations.</li><li><strong>HIN-Glove</strong>, <strong>HIN-BERT</strong>: Extracting relations through a hierarchical interaction network with either Glove embedding or BERT.</li><li><strong>GAT</strong>, <strong>GCNN</strong>, <strong>EOG</strong>, <strong>AGGCN</strong>, <strong>LSR-Glove</strong>, <strong>LSR-BERT</strong>: Previous graph-based methods, while our graph construction is totally different from theirs and they conduct logical reasoning only based on GCN.</li><li><strong>GAIN-Glove</strong>, <strong>GAIN-BERT</strong>: Our proposed model with either Glove embedding or BERT.</li></ul><p>The evaluation metrics we use are F1/AUC and Ign-F1/Ign-AUC. The latter means we do not consider the triples (i.e., head-relation-tail) that are already contained in the training set.</p><p><img src="/blog/assets/image5-CokimrTV.png" alt="image5"></p><p>The key observations are:</p><ul><li>Among the models not using BERT or BERT variants, GAIN-GloVe consistently outperforms all sequential-based and graph-based strong baselines by 0.9∼12.82 F1 score on the test set.</li><li>Among the models using BERT or BERT variants, GAIN-BERT base yields a great improvement of F1/Ign F1 on dev and test set by 2.22/6.71 and 2.19/2.03, respectively, in comparison with the strong baseline LSR-BERT base. GAIN-BERT large also improves 2.85/2.63 F1/Ign F1 on test set compared with previous state-of-the-art method, CorefRoBERTaRElarge.</li><li>GAIN can better utilize powerful BERT representation. LSR-BERT base improves F1 by 3.83 and 4.87 on dev and test set with GloVe embedding replaced with BERTbase while our GAIN-BERT base yields an improvement by 5.93 and 6.16.</li></ul><h3 id="ablation-study" tabindex="-1"><a class="header-anchor" href="#ablation-study"><span>Ablation Study</span></a></h3><p>We conduct ablation study by removing the mention-level graph, entity-level graph inference module, and the document node in the mention-level graph. The F1 scores on test set significantly decrease by 2.02~2.34/1.61~1.90 for GAIN-Glove/GAIN-BERT.</p><p><img src="/blog/assets/image6-BTkfFZTq.png" alt="image6"></p><h3 id="further-analysis" tabindex="-1"><a class="header-anchor" href="#further-analysis"><span>Further Analysis</span></a></h3><h4 id="cross-sentence-relation-extraction" tabindex="-1"><a class="header-anchor" href="#cross-sentence-relation-extraction"><span>Cross-sentence Relation Extraction</span></a></h4><p>We evaluate GAIN on relations within a single sentence (Intra-F1) and those involving multiple sentences (Inter-F1), respectively. GAIN outperforms other baselines not only in Intra-F1 but also Inter-F1. The removal of Mention-level Graph (hMG) leads to a more considerable decrease in Inter-F1 than Intra-F1, which indicates our hMG do help interactions among mentions, especially those distributed in different sentences with long-distance dependency.</p><div align="center"><img src="/blog/assets/image7-C540sNWw.png"></div><h4 id="logical-reasoning-for-relation-extraction" tabindex="-1"><a class="header-anchor" href="#logical-reasoning-for-relation-extraction"><span>Logical Reasoning for Relation Extraction</span></a></h4><p>We evaluate GAIN on relations requiring logical reasoning (Infer-F1), and the experimental results show GAIN can better handle relational inference. For example, GAIN-BERT base improves 5.11 Infer-F1 compared with RoBERTa-RE base. The inference module also plays an important role in capturing potential inference chains between entities, without which GAIN-BERT base would drop by 1.78 Infer-F1.</p><div align="center"><img src="/blog/assets/image8-Du0mm5sM.png"></div><h3 id="case-study" tabindex="-1"><a class="header-anchor" href="#case-study"><span>Case Study</span></a></h3><p>The figure above shows the case study of our proposed model GAIN, in comparison with other baselines. As is shown, BiLSTM can only identify two relations within the first sentence. Both BERT-RE base and GAIN-BERT base can successfully predict <strong>Without Me</strong> is part of <strong>The Eminem Show</strong>. But only GAIN-BERT base is able to deduce the performer and publication date of <strong>Without Me</strong> are the same as those of <strong>The Eminem Show</strong>, namely <strong>Eminem</strong> and <strong>May 26, 2002</strong>, where it requires logical inference across sentences.</p><p><img src="/blog/assets/image9-OpGWXM_z.png" alt="image9"></p><h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="#conclusion"><span>Conclusion</span></a></h2><p>Extracting inter-sentence relations and conducting relational reasoning are challenging in document-level relation extraction. In this paper, we introduce Graph Aggregationand-Inference Network (GAIN) to better cope with document-level relation extraction, which features double graphs in different granularity. GAIN utilizes a heterogeneous Mention-level Graph to model the interaction among different mentions across the document and capture document-aware features. It also uses an Entity-level Graph with a proposed path reasoning mechanism to infer relations more explicitly. Experimental results on the large-scale human annotated dataset, DocRED, show GAIN outperforms previous methods, especially in inter-sentence and inferential relations scenarios. The ablation study also confirms the effectiveness of different modules in our model.</p><h2 id="reference" tabindex="-1"><a class="header-anchor" href="#reference"><span>Reference</span></a></h2><ul><li>Shuang Zeng, Runxin Xu, Baobao Chang, Lei Li. Double Graph Based Reasoning for Document-level Relation Extraction. EMNLP 2020.</li><li>Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, Maosong Sun. 2019. DocRED: A Large-Scale Document-Level Relation Extraction Dataset. In Proceedings of ACL.</li></ul></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="vp-link vp-external-link-icon vp-meta-label" href="https://github.com/lileicc/blog/edit/main/nlp/GAIN/README.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page<!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">Last update: </span><!----></div><div class="contributors"><span class="vp-meta-label">Contributors: </span><!--[--><!--[--><span class="vp-meta-info" title="email: lileicc@gmail.com">Lei Li</span><!--]--><!--]--></div></div></footer><!----><div id="vp-comment" class="giscus-wrapper input-top" style="display:block;"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" preserveAspectRatio="xMidYMid" viewBox="0 0 100 100"><circle cx="28" cy="75" r="11" fill="currentColor"><animate attributeName="fill-opacity" begin="0s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></circle><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 47a28 28 0 0 1 28 28"><animate attributeName="stroke-opacity" begin="0.1s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 25a50 50 0 0 1 50 50"><animate attributeName="stroke-opacity" begin="0.2s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path></svg></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">Li Lab</div><div class="vp-copyright">Copyright © 2024 Runxin Xu </div></footer></div><!--]--><!--[--><!----><!----><!--]--><!--]--></div>
    <script type="module" src="/blog/assets/app-DiCjo-Va.js" defer></script>
  </body>
</html>
