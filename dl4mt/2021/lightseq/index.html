<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.0" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.1" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://lileicc.github.io/blog/dl4mt/2021/lightseq/"><meta property="og:site_name" content="Li-Lab Blog"><meta property="og:title" content="Accelerating the Computation on GPUs for Natural Language Processing"><meta property="og:description" content="A high performance open-source library for NLP Transformer model training and inferencing."><meta property="og:type" content="article"><meta property="og:image" content="https://lileicc.github.io/blog/"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2022-09-13T03:45:15.000Z"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image:alt" content="Accelerating the Computation on GPUs for Natural Language Processing"><meta property="article:author" content="Bowen Zhang"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="GPU Acceleration"><meta property="article:tag" content="CUDA"><meta property="article:published_time" content="2021-12-10T00:00:00.000Z"><meta property="article:modified_time" content="2022-09-13T03:45:15.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Accelerating the Computation on GPUs for Natural Language Processing","image":["https://lileicc.github.io/blog/"],"datePublished":"2021-12-10T00:00:00.000Z","dateModified":"2022-09-13T03:45:15.000Z","author":[{"@type":"Person","name":"Bowen Zhang"}]}</script><title>Accelerating the Computation on GPUs for Natural Language Processing | Li-Lab Blog</title><meta name="description" content="A high performance open-source library for NLP Transformer model training and inferencing.">
    <link rel="preload" href="/blog/assets/style-e7wIrRON.css" as="style"><link rel="stylesheet" href="/blog/assets/style-e7wIrRON.css">
    <link rel="modulepreload" href="/blog/assets/app-TCeo_ajs.js"><link rel="modulepreload" href="/blog/assets/index.html-aOh-NUrG.js"><link rel="modulepreload" href="/blog/assets/index.html-zWDW-dE0.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-x3n3nnut.js">
    <link rel="prefetch" href="/blog/assets/index.html-uWfE6SbM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nE6gO_7K.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rddA2LN3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wZ8zj1zx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PlQK4o96.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9Xh1KOjV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-VxJu2QC1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-U1VQl2Eb.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-gq5IQW34.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Eum09ixv.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-k_z2qdXd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nDgcuDE0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-n_sa__IF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-02N2Cr31.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-YsvHpHbM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SwXHc6hE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hkym9iFT.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-qCUSFaKS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--SW4lVd9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ejNKoiVJ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ezBeMfjC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Z1t3jQDg.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-RYhmq5TF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oWDVHkT8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ROvhtggM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-JUJ-rlbb.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Ptx4qWCt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-umnWKYQq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rP7ssLIy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-pXPA5v4g.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-vstHpLDr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7boXnOP-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-E_bwP14Y.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-II3fUI6m.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CQrB_jON.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mLIoTvBR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-n7XOGv9Z.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-zfNzFZSH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-i3ALVI_i.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0sOpWKpl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-1cll9k9d.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7HN2H0UB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZDqg7rQw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-pdN0LDKr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-eNcYnvl4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-t2PsU7-4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-G00w86XT.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-U5Qrn136.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bcJG7kH0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-IKJ4tLEW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BL0MJvIF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-8Z5PG7OQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-V5Y8hLj3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cRwt7pxi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-qsNk41eE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GOpL5FJN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_duZqh60.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OrLT7NEC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-lJG6txrw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oLHMXl4g.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-EMVGsE7D.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-vv4O3-5x.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-be7D4hpM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BT0eMqel.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SLiRoZsw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-1NcWQ1JA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ekyShiFQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-sgk1m2_n.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kb8GfeRD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xXnQV_mG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-INKfADYC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZCH3VU2M.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_hKQG08R.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bNOG1auN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mePQ6M8X.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_TMm0KxD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-gx5foYfd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5Mhhvs5z.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-95M6igPl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jk41IluR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-m9EbEEr8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-k7_BPpe1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Ex4rTbEQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-XXOQbmSB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-x11OWh_8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5jEY1mB9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-HKCgalpW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PxIjFQ4M.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Slp92PgS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-QXd7khlA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-w1429E4w.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-62MXtNkc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Tbf_mkls.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hJ-E417x.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FuZtKTKb.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Tv61iqe7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZeEjo3Po.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cw-4yGRl.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-bKNj4RYx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-iYZB-JbA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-RKPNe-DY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-pSPa82Ht.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-h4ofnM77.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-o5XIGgxU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-1KgcEp7d.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rgDmtxzC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-zptFsrns.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-L_Vpusdt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oFmu9Riz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C-F0YMcE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ePYWVr43.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-VGTCbEKH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-m613z7qy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-K3vd68-z.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Pe24msDw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GA1jJEr_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7tVkGo2o.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-h4DrqiSa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oLGgogi3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FuPET0ka.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-3ImcXxNM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-fAiBxV-C.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wfv4V-9y.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0NhC4spl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hubfQiTd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SaTzCUQS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4fMtu3Sn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nzkrpgLA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GKUqQ3rT.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-yDhhzzRG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--p6oz0Q4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jqd-2qdZ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Q2YhSatR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UY4YFkOl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xcmj5LbI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7I0SQuFR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TsTfjUqR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TvPj7CxK.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-uxnPR0Lp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-NcCekipx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-HL1RCy7q.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-JvCUhiEm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wfqH0YcV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9bvCn9s_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Z3xS8x1W.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-reiNbN8M.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jbYZ4as8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-WR6QlAin.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-WaFgOK5R.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-VUvPSPw2.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0b-Nw4qY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FSdd7GgV.js" as="script"><link rel="prefetch" href="/blog/assets/giscus-unEZQsJ0.js" as="script"><link rel="prefetch" href="/blog/assets/auto-HRhNfH8L.js" as="script"><link rel="prefetch" href="/blog/assets/index-rBp-GJb9.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-i2ohwMnJ.js" as="script"><link rel="prefetch" href="/blog/assets/SearchResult-qkUJdFfb.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container no-sidebar has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/blog/"><img class="vp-nav-logo" src="/blog/logo.svg" alt="Li-Lab Blog"><!----><span class="vp-site-name hide-in-pad">Li-Lab Blog</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="Blog Home" class="vp-link nav-link nav-link" href="/blog/"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="home" width="1em" height="1em"></iconify-icon>Blog Home<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Category" class="vp-link nav-link nav-link" href="/blog/category/"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="baseline-category" width="1em" height="1em"></iconify-icon>Category<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Tags" class="vp-link nav-link nav-link" href="/blog/tag/"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="tag" width="1em" height="1em"></iconify-icon>Tags<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Timeline" class="vp-link nav-link nav-link" href="/blog/timeline/"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="time" width="1em" height="1em"></iconify-icon>Timeline<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/lileicc/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" class="outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="outlook-dropdown"><!----></div></button></div><!--[--><button type="button" class="search-pro-button" role="search" aria-label="Search"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">Search</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Accelerating the Computation on GPUs for Natural Language Processing</h1><div class="page-info"><span class="page-author-info" aria-label="AuthorðŸ–Š" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Bowen Zhang</span></span><span property="author" content="Bowen Zhang"></span></span><!----><span class="page-date-info" aria-label="Writing DateðŸ“…" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2021-12-10T00:00:00.000Z"></span><span class="page-category-info" aria-label="CategoryðŸŒˆ" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category8 clickable" role="navigation">NLP</span><span class="page-category-item category1 clickable" role="navigation">DL4MT</span><!--]--><meta property="articleSection" content="NLP,DL4MT"></span><span class="page-tag-info" aria-label="TagðŸ·" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag1 clickable" role="navigation">Transformer</span><span class="page-tag-item tag8 clickable" role="navigation">GPU Acceleration</span><span class="page-tag-item tag4 clickable" role="navigation">CUDA</span><!--]--><meta property="keywords" content="Transformer,GPU Acceleration,CUDA"></span><span class="page-reading-time-info" aria-label="Reading TimeâŒ›" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 4 min</span><meta property="timeRequired" content="PT4M"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_1-what-is-lightseq">1. What is LightSeq?</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_1-1-nlp-models">1.1 NLP models</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_1-2-motivation">1.2 Motivation</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_1-3-lightseq">1.3 Lightseq</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_2-technique-details">2. Technique Details</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_2-1-operation-fusion">2.1 Operation Fusion</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_2-2-hierarchical-auto-regressive-search">2.2 Hierarchical Auto-Regressive Search</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_2-3-dynamic-gpu-memory-reuse">2.3 Dynamic GPU Memory Reuse</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_3-using-lightseq">3. Using LightSeq</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-1-installation">3.1 Installation</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-2-training-examples-using-lightseq">3.2 Training examples using LightSeq</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-3-inference-examples-using-lightseq">3.3 Inference examples Using LightSeq</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_4-performance">4. Performance</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_4-1-training-performance">4.1 Training Performance</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_4-2-inference-performance">4.2 Inference Performance</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_4-3-more-inference-performance-on-nvidia-p4-and-t4">4.3 More Inference Performance on Nvidia P4 and T4</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_4-4-real-world-cloud-computing-delay-test-on-gpt">4.4 Real-world Cloud Computing Delay Test on GPT</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_5-reference">5. Reference</a></li><!----><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><p>A high performance open-source library for NLP Transformer model training and inferencing.</p><!-- more --><h2 id="_1-what-is-lightseq" tabindex="-1"><a class="header-anchor" href="#_1-what-is-lightseq" aria-hidden="true">#</a> 1. What is LightSeq?</h2><p><img src="/blog/assets/lightseqlogo-SclCJUU5.png" alt="logo"></p><h3 id="_1-1-nlp-models" tabindex="-1"><a class="header-anchor" href="#_1-1-nlp-models" aria-hidden="true">#</a> 1.1 NLP models</h3><p>Transformers[1], BERT[2], or GPT[3] models are state-of-art models on natural language processing tasks. They are heavily used and breaking multiple records on sequence-to-sequence (Seq2Seq) tasks including machine translation, text summarization, and text generation, or even computer vision tasks by Vision Transformers (an image is just a sequence of pixels). However, those models are huge in size that needs large-scale training and inference. This makes it computationally expensive, so serving these models is a challenge for real industrial applications.</p><h3 id="_1-2-motivation" tabindex="-1"><a class="header-anchor" href="#_1-2-motivation" aria-hidden="true">#</a> 1.2 Motivation</h3><p>Due to the high complexity and large parameter size of transformer models, the latency for both training and inference is high. Here are the three comparisons between the current inference systems and LightSeq, and the reasons why they are not able to perform well for online tasks.</p><ol><li>Popular deep learning frameworks. Since those models have flexible model structures, both TensorFlow and PyTorch need additional memory allocation and extra overhead for training. Thus, they do not make full use of the hardware resource.</li><li>Inference optimization frameworks. Optimization frameworks like TensorFlow XLA, TVM, and TensorRT are not suitable for variable-length inputs, which require dynamic memory allocation that does not perform well for transformer models.</li><li>Similar acceleration frameworks. Faster-Transformer and TurboTransformers are similar to LightSeq. However, they do not have all components or features compared to LightSeq (Table 1).</li></ol><h3 id="_1-3-lightseq" tabindex="-1"><a class="header-anchor" href="#_1-3-lightseq" aria-hidden="true">#</a> 1.3 Lightseq</h3><p>LightSeq[4], is a high-performance open-source library for both training and inference that is directly built on top of CUDA official libraries (<a href="https://docs.nvidia.com/cuda/cublas/index.html" target="_blank" rel="noopener noreferrer">cuBLAS<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>,Â <a href="https://docs.nvidia.com/cuda/thrust/index.html" target="_blank" rel="noopener noreferrer">Thrust<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>,Â <a href="http://nvlabs.github.io/cub/" target="_blank" rel="noopener noreferrer">CUB<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>). It supports models in the Transformer family including BERT, GPT, and full encoder-decoder. It introduces new transformer encoders and decoders components after fusing and optimizing the existing models.</p><p>The applications of LightSeq include Machine Translation, Text Generation, Dialog, Language Modelling, Sentiment Analysis, and other related tasks with sequence data, which can be easily deployed to commercial products.</p><p>LightSeq improves the speed for both training and inference stages. Models like DeepSpeed[5] only accelerate the training, and tools like TensorRT, FasterTransformer, or TurboTransformers only support optimizing the inference. Here are the comparison tables on different features between LightSeq and other models.</p><p><img src="/blog/assets/feature_table-tAJL9cmc.png" alt="Features"></p><p>Table 1. The tables above are from the <a href="https://github.com/bytedance/lightseq" target="_blank" rel="noopener noreferrer">official Github repository<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>.</p><h2 id="_2-technique-details" tabindex="-1"><a class="header-anchor" href="#_2-technique-details" aria-hidden="true">#</a> 2. Technique Details</h2><p>There are three main methods that LightSeq uses to optimize the model, training speed, and inference speed. The image below shows the architecture of a sequence-to-sequence model using transformers.</p><p><img src="/blog/assets/Transformer-c697OoM3.png" alt="Transformer"></p><h3 id="_2-1-operation-fusion" tabindex="-1"><a class="header-anchor" href="#_2-1-operation-fusion" aria-hidden="true">#</a> 2.1 Operation Fusion</h3><p>Transformers model implemented by popular deep learning frameworks like Pytorch or Tensorflow just combine multiple fine-grained kernel functions for one layer. In this way, it needs to launch more kernel functions and uses lots of memory I/O that costs extra time for training and inference.</p><p>LightSeq uses general matrix multiply (GEMM) and custom kernel functions, so here are only six custom kernel functions and six GEMM in a Transformer encoder layer for LightSeq models. The right image shows the model structure of the LightSeq transformer encoder layer.</p><p><em>Need to add more intuitive description here</em><img src="/blog/assets/fusion-3PcqNI2p.png" alt="Fusion"></p><h3 id="_2-2-hierarchical-auto-regressive-search" tabindex="-1"><a class="header-anchor" href="#_2-2-hierarchical-auto-regressive-search" aria-hidden="true">#</a> 2.2 Hierarchical Auto-Regressive Search</h3><p>Searching usually happens in the last step of a transformer model. Redundant calculations often exist in output layers since we only need a few labels/tokens with the highest probability instead of all of them.</p><p>LightSeq optimizes this process by using a Hierarchical Auto Regressive Search method to erase redundant calculations and perform parallel computing illustrated as below (using beam search as an example).</p><p>the following steps happen for each beam.</p><ol><li>Randomly divide logits into k groups</li><li>Calculate the maximum of group <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>, denoted as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">m_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li><li>Calculate the minimum of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">m_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, denoted as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span>, which can be regarded as a rough top-k value of logits.</li><li>Select logits larger than <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span> and write them into GPU memory.</li></ol><p><img src="/blog/assets/softmax-dnxd9KKJ.png" alt="softmax"></p><h3 id="_2-3-dynamic-gpu-memory-reuse" tabindex="-1"><a class="header-anchor" href="#_2-3-dynamic-gpu-memory-reuse" aria-hidden="true">#</a> 2.3 Dynamic GPU Memory Reuse</h3><p>LightSeq pre-defines the maximum of dynamic shapes, such as the maximal sequence length, to avoid memory allocation time and save GPU memory occupancy. Also, GPU memory is shared for non-dependent intermediate results to reduce the memory usage.</p><p>By using LightSeq, users are able to 8 Transformer big models simultaneously on a NVIDIA Tesla T4 GPU.</p><h2 id="_3-using-lightseq" tabindex="-1"><a class="header-anchor" href="#_3-using-lightseq" aria-hidden="true">#</a> 3. Using LightSeq</h2><p>Running LightSeq requires one or more GPUs.</p><h3 id="_3-1-installation" tabindex="-1"><a class="header-anchor" href="#_3-1-installation" aria-hidden="true">#</a> 3.1 Installation</h3><p>LightSeq installation from PyPI only supports python 3.6 to 3.8 on Linux for now. Consider compiling from source if you have other environments.</p><p><code>pip install lightseq fairseq sacremoses transformers</code></p><h3 id="_3-2-training-examples-using-lightseq" tabindex="-1"><a class="header-anchor" href="#_3-2-training-examples-using-lightseq" aria-hidden="true">#</a> 3.2 Training examples using LightSeq</h3><p>Training a translation task on wmt14 en2de dataset by running the following command.</p><p><code>sh examples/training/fairseq/ls_fairseq_wmt14en2de.sh</code></p><p>If you want to run the training using FairSeq, run the following command.</p><p><code>sh examples/training/fairseq/fairseq_wmt14en2de.sh</code></p><h3 id="_3-3-inference-examples-using-lightseq" tabindex="-1"><a class="header-anchor" href="#_3-3-inference-examples-using-lightseq" aria-hidden="true">#</a> 3.3 Inference examples Using LightSeq</h3><p><code>pip install torch tensorflow transformers lightseq</code></p><p><code>cd examples/inference/python</code></p><p><code>python export/hf_bart_export.py</code></p><p><code>python test/ls_bart.py</code></p><p><a href="https://github.com/bytedance/lightseq/blob/master/docs/guide.md" target="_blank" rel="noopener noreferrer">Here<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> is a guide on using LightSeq for training and inference.</p><h2 id="_4-performance" tabindex="-1"><a class="header-anchor" href="#_4-performance" aria-hidden="true">#</a> 4. Performance</h2><h3 id="_4-1-training-performance" tabindex="-1"><a class="header-anchor" href="#_4-1-training-performance" aria-hidden="true">#</a> 4.1 Training Performance</h3><p>The plots below are the experiment results on WMT14 English to German translation tasks using Transformer-Big models. In all plots, FairSeq+LightSeq models are able to improve the performance to 3.5X maximum.</p><p><img src="/blog/assets/training_speed-oBD6ucX7.png" alt="Training"></p><p>The image above is from the <a href="https://github.com/bytedance/lightseq" target="_blank" rel="noopener noreferrer">official Github repository<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>.</p><h3 id="_4-2-inference-performance" tabindex="-1"><a class="header-anchor" href="#_4-2-inference-performance" aria-hidden="true">#</a> 4.2 Inference Performance</h3><p>Here are the inference results using LightSeq, TensorFlow, PyTorch, and FasterTransformer on neural machine translation using Transformer-base models with beam search methods.</p><p><img src="/blog/assets/inference_speed-NZpU36YR.png" alt="Inference"></p><p>The image above is from the <a href="https://github.com/bytedance/lightseq" target="_blank" rel="noopener noreferrer">official Github repository<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>.</p><h3 id="_4-3-more-inference-performance-on-nvidia-p4-and-t4" tabindex="-1"><a class="header-anchor" href="#_4-3-more-inference-performance-on-nvidia-p4-and-t4" aria-hidden="true">#</a> 4.3 More Inference Performance on Nvidia P4 and T4</h3><p>The three images below are from the <a href="https://segmentfault.com/a/1190000038523998" target="_blank" rel="noopener noreferrer">Volctrans Blog on segmentfault.com<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>.</p><p>X-axes are the Batch size and sequence length pairs, and Y-axes are the acceleration rates.</p><p><img src="/blog/assets/beamsearch-DXdUvRYU.png" alt="Beam Search T4"></p><p><img src="/blog/assets/beamsearchp4-rSZIzSUE.png" alt="Beam Search P4"></p><p><img src="/blog/assets/samplingp4-_RgZzJn_.png" alt="Sampling"></p><h3 id="_4-4-real-world-cloud-computing-delay-test-on-gpt" tabindex="-1"><a class="header-anchor" href="#_4-4-real-world-cloud-computing-delay-test-on-gpt" aria-hidden="true">#</a> 4.4 Real-world Cloud Computing Delay Test on GPT</h3><p>This plot shows the performance of deploying a GPT model to cloud computing. At 11:00, the delay performance decreased from 360 ms to 80 ms when LightSeq is turned on.</p><p><img src="/blog/assets/realworkload-CGme6lGe.png" alt="Real Work Load"></p><p>The image above is from the <a href="https://segmentfault.com/a/1190000038523998" target="_blank" rel="noopener noreferrer">Volctrans Blog on segmentfault.com<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>.</p><h2 id="_5-reference" tabindex="-1"><a class="header-anchor" href="#_5-reference" aria-hidden="true">#</a> 5. Reference</h2><p>[1] Vaswani, Ashish, et al. &quot;Attention is all you need.&quot; Advances in neural information processing systems. 2017.</p><p>[2] Devlin, Jacob, et al. &quot;Bert: Pre-training of deep bidirectional transformers for language understanding.&quot;Â <em>arXiv preprint arXiv:1810.04805</em>Â (2018).</p><p>[3] Brown, Tom B., et al. &quot;Language models are few-shot learners.&quot; arXiv preprint arXiv:2005.14165 (2020).</p><p>[4] Wang, Xiaohui, et al. &quot;LightSeq: A High Performance Inference Library for Transformers.&quot; arXiv preprint arXiv:2010.13887 (2020).</p><p>[5] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters.Â <a href="https://dl.acm.org/doi/10.1145/3394486.3406703" target="_blank" rel="noopener noreferrer">In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD &#39;20, Tutorial)<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>.</p></div><!--[--><!----><!--]--><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/lileicc/blog/edit/main/dl4mt/2021/lightseq/README.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item git-info"><div class="update-time"><span class="label">Last update: </span><!----></div><div class="contributors"><span class="label">Contributors: </span><!--[--><!--[--><span class="contributor" title="email: lileicc@gmail.com">Lei Li</span><!--]--><!--]--></div></div></footer><!----><div id="comment" class="giscus-wrapper input-top" style="display:block;"><div class="loading-icon-wrapper" style="display:flex;align-items:center;justify-content:center;height:96px"><svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" preserveAspectRatio="xMidYMid" viewBox="25 25 50 50"><animateTransform attributeName="transform" type="rotate" dur="2s" keyTimes="0;1" repeatCount="indefinite" values="0;360"></animateTransform><circle cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="4" stroke-linecap="round"><animate attributeName="stroke-dasharray" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="1,200;90,200;1,200"></animate><animate attributeName="stroke-dashoffset" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="0;-35px;-125px"></animate></circle></svg></div></div><!--[--><!----><!--]--><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">Li Lab</div><div class="vp-copyright">Copyright Â© 2023 Bowen Zhang</div></footer></div><!--]--><!----><!----><!--]--></div>
    <script type="module" src="/blog/assets/app-TCeo_ajs.js" defer></script>
  </body>
</html>
