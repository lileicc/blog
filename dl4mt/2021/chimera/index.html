<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.0" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.1" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://lileicc.github.io/blog/dl4mt/2021/chimera/"><meta property="og:site_name" content="Li-Lab Blog"><meta property="og:title" content="Learning Shared Semantic Space for Speech-to-Text Translation"><meta property="og:description" content="How to develop a translation model that can take both speech and text as input and translate to target language? Can we borrow inspiration from human brain study to improve the speech translation models? Reading Time: About 15 minutes."><meta property="og:type" content="article"><meta property="og:image" content="https://lileicc.github.io/blog/"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2022-09-13T03:45:15.000Z"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image:alt" content="Learning Shared Semantic Space for Speech-to-Text Translation"><meta property="article:author" content="Xianjun Yang"><meta property="article:tag" content="Speech Translation"><meta property="article:tag" content="Shared Semantic Memory"><meta property="article:tag" content="Chimera"><meta property="article:published_time" content="2021-11-13T00:00:00.000Z"><meta property="article:modified_time" content="2022-09-13T03:45:15.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Learning Shared Semantic Space for Speech-to-Text Translation","image":["https://lileicc.github.io/blog/"],"datePublished":"2021-11-13T00:00:00.000Z","dateModified":"2022-09-13T03:45:15.000Z","author":[{"@type":"Person","name":"Xianjun Yang"}]}</script><title>Learning Shared Semantic Space for Speech-to-Text Translation | Li-Lab Blog</title><meta name="description" content="How to develop a translation model that can take both speech and text as input and translate to target language? Can we borrow inspiration from human brain study to improve the speech translation models? Reading Time: About 15 minutes.">
    <link rel="preload" href="/blog/assets/style-e7wIrRON.css" as="style"><link rel="stylesheet" href="/blog/assets/style-e7wIrRON.css">
    <link rel="modulepreload" href="/blog/assets/app-ZUeO6846.js"><link rel="modulepreload" href="/blog/assets/index.html-alyCt1nA.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-x3n3nnut.js"><link rel="modulepreload" href="/blog/assets/index.html-gq5IQW34.js">
    <link rel="prefetch" href="/blog/assets/index.html-uWfE6SbM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nE6gO_7K.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rddA2LN3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wZ8zj1zx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PlQK4o96.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9Xh1KOjV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-VxJu2QC1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-U1VQl2Eb.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Eum09ixv.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-k_z2qdXd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nDgcuDE0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-n_sa__IF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-02N2Cr31.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-aOh-NUrG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-YsvHpHbM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SwXHc6hE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hkym9iFT.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-qCUSFaKS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--SW4lVd9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ejNKoiVJ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ezBeMfjC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Z1t3jQDg.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-RYhmq5TF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oWDVHkT8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ROvhtggM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-JUJ-rlbb.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Ptx4qWCt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-umnWKYQq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rP7ssLIy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-pXPA5v4g.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-vstHpLDr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7boXnOP-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-E_bwP14Y.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-II3fUI6m.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CQrB_jON.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mLIoTvBR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-n7XOGv9Z.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-zfNzFZSH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-i3ALVI_i.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0sOpWKpl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-1cll9k9d.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7HN2H0UB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZDqg7rQw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-pdN0LDKr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-eNcYnvl4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-t2PsU7-4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-G00w86XT.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-U5Qrn136.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bcJG7kH0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-IKJ4tLEW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BL0MJvIF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-8Z5PG7OQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-V5Y8hLj3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cRwt7pxi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-qsNk41eE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GOpL5FJN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_duZqh60.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OrLT7NEC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-lJG6txrw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oLHMXl4g.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-EMVGsE7D.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-vv4O3-5x.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-be7D4hpM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BT0eMqel.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SLiRoZsw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-1NcWQ1JA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ekyShiFQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-sgk1m2_n.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kb8GfeRD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xXnQV_mG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-INKfADYC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZCH3VU2M.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_hKQG08R.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bNOG1auN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mePQ6M8X.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_TMm0KxD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-yOfJdJjS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UuPjVtzZ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OSsGQgwA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-F-1swd0X.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-3VpGka6i.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0RmZIe05.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-dfto9pA8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZyARDoaW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hZw0zm8r.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-MUyEG-PQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DAVVl3VH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-IdaGtdrc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-avaQIInj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-10ucmgpa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GuDzJn7n.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Qoedz0LN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jUB232VF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-k7iZ1WwL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-YVtWIqJj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UynhTBPw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cKaS2noG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-m2iLe1Ny.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-Jv_xiBow.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-qADD1hw9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4xtbQFqX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-3O0O3j9m.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-lk6CsJ-H.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hxz8XnrS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-EKvCc2Zj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-HKqkuVwo.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-k-kwHGjI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-f1xKiysW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Ra8MnxQJ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-2DslZA5n.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cQWHKk_e.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-u18n-YU6.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-58UZzGiY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-qPPJ1zX2.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-YD3mxX7L.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-EJqExgrB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-AQdezg3Q.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-6_iPRxqW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0HjUdIAX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ywQ0A9ko.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-vuvTdEvb.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--9FUVir8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-tx7LHlkh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-eGKEni4q.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kH2mcvw8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-e4dynlGS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-yi3X8h3m.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7orAK8Wu.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OUX6SIps.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-puTdgEJ4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TKv-KSIl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kkRS9aIk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-QgGBJIDk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hrarLtLD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-L0kca5PH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BTL0aHbP.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-o7NYRJ51.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-z3244R6N.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-tqOZhMzN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nTnZS2uy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ky_bx-Qh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wzkoOwtU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-s34K_u4y.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ODKQ-otV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-yJ4tTDta.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OU9PEtxt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-uVAlFVSH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9LI4D4WS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-z4uKW9wl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-QlPmXdgI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-F6F9dC88.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-u0VZs0gs.js" as="script"><link rel="prefetch" href="/blog/assets/giscus-unEZQsJ0.js" as="script"><link rel="prefetch" href="/blog/assets/auto-HRhNfH8L.js" as="script"><link rel="prefetch" href="/blog/assets/index-rBp-GJb9.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-i2ohwMnJ.js" as="script"><link rel="prefetch" href="/blog/assets/SearchResult-QdFdceI3.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container no-sidebar has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/blog/"><img class="vp-nav-logo" src="/blog/logo.svg" alt="Li-Lab Blog"><!----><span class="vp-site-name hide-in-pad">Li-Lab Blog</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="Blog Home" class="vp-link nav-link nav-link" href="/blog/"><span class="font-icon icon fa-fw fa-sm fas fa-home" style=""></span>Blog Home<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Category" class="vp-link nav-link nav-link" href="/blog/category/"><span class="font-icon icon fa-fw fa-sm fas fa-categoryselected" style=""></span>Category<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Tags" class="vp-link nav-link nav-link" href="/blog/tag/"><span class="font-icon icon fa-fw fa-sm fas fa-tag" style=""></span>Tags<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Timeline" class="vp-link nav-link nav-link" href="/blog/timeline/"><span class="font-icon icon fa-fw fa-sm fas fa-time" style=""></span>Timeline<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/lileicc/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" class="outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="outlook-dropdown"><!----></div></button></div><!--[--><button type="button" class="search-pro-button" role="search" aria-label="Search"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">Search</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Learning Shared Semantic Space for Speech-to-Text Translation</h1><div class="page-info"><span class="page-author-info" aria-label="Authorüñä" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Xianjun Yang</span></span><span property="author" content="Xianjun Yang"></span></span><!----><span class="page-date-info" aria-label="Writing DateüìÖ" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2021-11-13T00:00:00.000Z"></span><span class="page-category-info" aria-label="Categoryüåà" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category5 clickable" role="navigation">ST</span><span class="page-category-item category1 clickable" role="navigation">DL4MT</span><!--]--><meta property="articleSection" content="ST,DL4MT"></span><span class="page-tag-info" aria-label="Tagüè∑" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag5 clickable" role="navigation">Speech Translation</span><span class="page-tag-item tag4 clickable" role="navigation">Shared Semantic Memory</span><span class="page-tag-item tag2 clickable" role="navigation">Chimera</span><!--]--><meta property="keywords" content="Speech Translation,Shared Semantic Memory,Chimera"></span><span class="page-reading-time-info" aria-label="Reading Time‚åõ" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 10 min</span><meta property="timeRequired" content="PT10M"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#introduction">Introduction</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#challenges-in-speech-translation">Challenges in speech translation</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#motivation-and-techniques-of-chimera">Motivation and Techniques of Chimera</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#shared-semantic-memory">Shared semantic memory</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#dataset-and-preprocessing">Dataset and preprocessing</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#effectiveness-of-chimera">Effectiveness of Chimera</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_1-benchmark-experiments">1. Benchmark Experiments</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_2-visualizations">2. Visualizations</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#summary">Summary</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#references">References</a></li><!----><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><p>How to develop a translation model that can take both speech and text as input and translate to target language? Can we borrow inspiration from human brain study to improve the speech translation models?</p><p>Reading Time: About 15 minutes.</p><!-- more --><p>PaperÔºö<a href="https://arxiv.org/pdf/2105.03095.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2105.03095.pdf<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>Github: <a href="https://github.com/Glaciohound/Chimera-ST" target="_blank" rel="noopener noreferrer">https://github.com/Glaciohound/Chimera-ST<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction" aria-hidden="true">#</a> Introduction</h2><p><img src="/blog/assets/c1-AEId0Z_v.png" alt="image1"></p><p>Although it seems difficult for normal people to acquire more than two languages, according to the Wikipedia there are many polyglots who can speak tens of languages. For example, Ziad Fazah, speaking a total of 59 world languages, is believed to be the world‚Äôs greatest living polyglot. However, compared with others in the history his record does not stand out. It was recorded that Cardinal Giuseppe Caspar Mezzofanti, who was born in 1774, could speak 38 languages and 40 dialects. Meanwhile, the 10th-century Muslim polymath Al-Farabi was claimed to know 70 languages. The German Hans Conon von der Gabelentz, born in 1807, researched and published books about grammars for 80 languages. The highest record probably belongs to Sir John Bowring, Governor of Hong Kong from 1854 to 1859, who was said to know 200 languages, and capable of speaking 100. But it turns out extremely difficult for machine to learn natural languages like humans, in particular, despite its huge potential applications, speech translation has always been a challenge for machine. One common advantage is that those polyglots all benefit from not only text, but also audio corpus. However, how to utilize both audio and text information to help machine speech translation has not been fully exploited. The challenge comes from the intrinsic difference of modality between audio and text.</p><p>A recent work Chimera from ByteDance AI Lab and UIUC aims to draw strengths from both modalities for speech translation [1]. Their key idea is to represent text and audio inputs differently, then fuse them together by projecting audio and text features into a common semantic representation to boost the ST performance.</p><p>There are two main advantages for including audio and text data together for training one ST model. First, humans learn languages simultaneously from audio, text and videos rather than pure text. Inspired by this observation, it is believed that text knowledge can provide additional insights for ST. Second, since MT corpus is much larger compared with small corpus of ST, which is also expensive to create, incorporating MT text provides much fruitful training data and is expected to yield improvements on ST when bridging the modality gap properly.</p><p>Taking those benefits into consideration, the Chimera model showed significant improvements by unifying MT and ST tasks on the benchmark ST datasets containing more than ten languages pairs.</p><p>Unlike previous translation models, Chimera has established a successful paradigm of bridging the modality gap between text and audio for speech translation. This is similar to multi-modality MT, in which images can improve the text translation quality. Considering the pixel level information in images is accurately described, the audio is even noisier and leads to more challenges.</p><p>Chimera is designed for an end-to-end speech-to-text translation task. It has two advantages. First, the translation quality can be consistently improved by leveraging a large amount of external machine translation data. In rich-resource directions, such as the largest speech translation dataset MuST-C, which already contains translations from English to eight languages with each pair consisting of at least 385 hours of audio recordings, Chimera can still significantly improve the quality, reaching a new State-Of-The-Art(SOTA) BLEU score on all language pairs. In low-resource directions, such as LibriSpeech dataset containing only 100 hours of training data, Chimera also performs surprisingly well and consistently outperforms the previous best results. Finally, they verified the common knowledge conveyed between these audio and text tasks indeed comes from the shared semantic space and thus paves a new way for augmenting training resources across modalities.</p><p>Chimera can be thought of as a multimodal in the field of speech translation in general. When you need to develop a speech translation model but don&#39;t have enough audio data, you may consider using Chimera, and it can turn out to be better than your expectation! The research data, codes and resources are also kindly published by the authors.</p><p>Next, we will introduce and analyze Chimera from three aspects: 1) the challenges of bridging the gap between audio and text for speech translation; 2) the motivation and methods of Chimera; 3) the performance and analysis of Chimera.</p><h2 id="challenges-in-speech-translation" tabindex="-1"><a class="header-anchor" href="#challenges-in-speech-translation" aria-hidden="true">#</a> Challenges in speech translation</h2><p>The current paradigm of artificial intelligence highly depends on training on a large dataset, and thus make predictions on a small test set. However, different modalities of data have been labeled for various tasks but seldom been utilized together due to the modality gap of data representations. For example, billions of parallel MT corpora have been ignored as additional training data for SP for a long time. At the meanwhile, the data of ST is always in the dearth due to the difficulty of collection and high cost. Looking at the relatively smaller amount of parallel data for ST compared with MT, it is natural to have the idea of combining them together. Unfortunately, although they both encode human languages, they are dissimilar in both coding attributes(pitch, volume, and intonation versus words, suffixes, and punctuation) and length(thousands of time frames versus tens of words). Therefore, it has always been a challenge to unify representations from audio and text. The recent evidence from functional neuroimaging identifies certain regions in brain that the processing stream for speech sounds and visual text correlates positively with the subjects&#39; reading ability. This finding provides the intuition for developing a multi-modality converged representation of audio and text in language activities. But only little previous research explored this direction possibly due to the difficulties of modality fusion and marginal improvements. Surprisingly, Chimera establishes a new bridge to fill the modality gap between speech and text and can serve as a new foundation in this area.</p><p><img src="/blog/assets/c3-C5OLzHms.png" alt="image2"></p><p>In the above figure[2], the color represents relative contribution of unimodal visual (green), unimodal auditory (red), and similar contribution of visual and auditory predictors (yellow) to explaining signal variance. The lateral and inferior occipital-temporal cortex were active by unimodal presentation of letters, while Heschl&#39;s gyrus (primary auditory cortex) and portions of the superotemporal cortex were activated by unimodal presentation of speech sounds. This neuroimaging evidence convinces the connection of audio and text in human brain, serving as theoretical foundation of modality fusion.</p><h2 id="motivation-and-techniques-of-chimera" tabindex="-1"><a class="header-anchor" href="#motivation-and-techniques-of-chimera" aria-hidden="true">#</a> Motivation and Techniques of Chimera</h2><p><img src="/blog/assets/c5-vGcsO6x-.png" alt="image4"></p><p>‚ÄãFor language learners, a very interesting phenomenon is that they learn better by audio and text together rather than text only. The aforementioned famous polyglots also stated that their success originated from various interactions with native speakers in other languages, such as listening, reading, and speaking. Nowadays, many people also learn new languages by watching movies, in which they will be immersed in audio and subtitles. In particular, the above figure is Ioannis Ikonomou, who was born in Greek and now works as translator for European Union. Ikonomou can speak 32 languages and could also translate those languages to each other in daily work. According to his interview, he was born in a famous tourism city, where people from all over tha world visit there every day. Under the influence of different tourists, he learned English at 5 years old, German at 7 years old, Italian at 10 years old, Russian at 13 years old, East African Swahili at the age of 14, and Turkish at the age of 16. He said &quot;Learning Polish can make Polish dumplings better. Learning Russian is to understand Dostoyevsky, Persian is to appreciate ancient Persian poetry, and Hungarian is to understand Hungarian folk songs. For German, it is to understand the veteran show &quot;Mirror of the World&quot; every Sunday evening.&quot;[3]. It is worthy to point that he learned most languages when he was still a child, demonstrating strong language ability of human children.</p><p><img src="/blog/assets/c6-yaPNAQcY.png" alt="image5"></p><p>The above figure shows the developmental milestones of human children learners. Children first learns how to speak from their family and thus how to speak fluently, read and write formally at school.</p><p>All those evidence demonstrates that combining audio and text could help humans learn a new language. A natural next step is to apply this idea in speech machine translation.</p><p>The design goal of Chimera is based on such considerations: design a general framework to learn the combination of audio and text from languages, and then it will benefit from this combined pre-training when migrating to the new speech translation direction. Just like language learners, after learning two modalities, the one modality becomes easier. The design of Chimera follows three basic principles: first, learn the shared semantic memory module to bridge the gap between the representation of audio and text; Second, the training objective of pre-training comes from three parts: 1) the speech-to-text translation training, 2) the text machine translation training, and 3) Bi-modal contrastive training; Third, train on external MT corpora and then apply the model in ST task.</p><h2 id="shared-semantic-memory" tabindex="-1"><a class="header-anchor" href="#shared-semantic-memory" aria-hidden="true">#</a> Shared semantic memory</h2><p><img src="/blog/assets/c7-C7EqVCir.png" alt="image6"> Chimera follows a transformer based encoder-decoder framework and above is an overview. Word embedding for text input and the Wav2vec2[4] sub-module for speech input are both included in the Encoder Module. The shared semantic projection Module generates semantic memory with fixed-size representation from contextual features using its memory query. The Decoder Module decodes semantic memory translation.</p><p>Since the encoder and decoder are actually standard modules based on transformer[5], which have been proven to be the SOTA design in many natural language processing tasks, the shared semantic memory module is the key to success of Chimera. So here we will mainly discuss the fantastic design of this shared module. Among this framework, the module relies heavily on shared semantic projection. In fact, contextual elements of speech and text can have a wide range of distributions and lengths. The shared semantic projection should, in theory, compute a fixed number of semantic features as output semantic memories. This module takes the contextual information extracted from the encoding module as input and outputs semantic memories with a set length of m. It is made up of n layers of attention. It stores a tuple of m trainable input-dependent memory queries as the initial &quot;memories&quot; to represent the categories of required semantic information. Attention &quot;keys&quot; and &quot;values&quot; are provided by unimodal contextual features, but attention &quot;queries&quot; are provided by memories. Memories are fed into the n shared semantic projection layers in an iterative fashion, with each layer&#39;s output being used as input to the next layer. The semantic memory is created from the final output.</p><h2 id="dataset-and-preprocessing" tabindex="-1"><a class="header-anchor" href="#dataset-and-preprocessing" aria-hidden="true">#</a> Dataset and preprocessing</h2><p>Two datasets were used for conducting experiments to verify the effectiveness of Chimera. One is called MuST-C, the largest ST corpus, which contains translations from English(EN) to 8 languages: Dutch (NL), French (FR), German (DE), Italian (IT), Portuguese (PT), Romanian(RO), Russian (RU), and Spanish (ES). With each pair consisting of at least 385 hours of audio recordings. Another popular one is Augmented LibriSpeech Dataset (En-Fr), which is composed of aligned e-books in French and their human reading in English of 100 hours. They also incorporate data from WMT, OpenSubtitles and OPUS100 translation tasks as pretraining corpora.</p><p>In practice, speech input, the 16-bit raw wave sequences are normalized by a factor of 215 to the range of [-1, 1), which uses the Wav2Vec2 Module following the base configuration in [4]. The shared Transformer encoder consists of 6 layers. The memory queries are 64 512- dimensional vectors. The parameters of shared semantic projection resemble a 3-layer Transformer encoder. The Transformer decoder has 6 layers. Each of these Transformer layers, except for those in the Wav2Vec2 module, has an embedding dimension of 512, a hidden dimension of 512, and 8 attention heads.</p><p>Chimera contains around 165M parameters. The whole training process for one trial on 8 Nvidia Tesla-V100 GPUs generally takes 20 ‚Äì40 hours according to the translation direction.</p><h2 id="effectiveness-of-chimera" tabindex="-1"><a class="header-anchor" href="#effectiveness-of-chimera" aria-hidden="true">#</a> Effectiveness of Chimera</h2><p>In summary, Chimera has the following advantages:</p><ol><li>New state-of-the-art performance on all language pairs</li></ol><p>Even though they did not use Google Translate results on Augmented Librispeech as most baselines, Chimera obtains state-of-the-art performance on all language pairs. Chimera&#39;s EN-DE results use WMT14+OpenSubtitles for MT pretraining, whereas the original paper also contains a full ablation research on the effect of MT data. It&#39;s worth noting that the improvement in EN-PT isn&#39;t as dramatic as it is in EN-DE and EN-FR. This is due to a mismatch in data between OPUS100 and MuST-C. OPUS100 has a high amount of sentences from movie subtitles, which are more informal, feature repeated lines, and address issues that are not covered in MuST-C public speeches.</p><ol start="2"><li>Successfully share knowledge across tasks</li></ol><p>Additional trial findings corroborate their design of auxiliary tasks by demonstrating its ability to acquire a well-structured shared semantic space as well as successfully exchange learned knowledge between MT and ST.</p><p>Here are some representative experimental results:</p><h3 id="_1-benchmark-experiments" tabindex="-1"><a class="header-anchor" href="#_1-benchmark-experiments" aria-hidden="true">#</a> 1. Benchmark Experiments</h3><p>The below two tables demonstrate the main results on tst-COMMON subset on all 8 languages in MuST-C dataset amd on LibriSpeech English-French dataset.</p><p><img src="/blog/assets/c8-KOkqR7cQ.png" alt="image7"></p><p>Table 1: Main results on tst-COMMON subset on all 8 languages in MuST-C dataset.</p><p><img src="/blog/assets/c9-mPwY4cu3.png" alt="image8"></p><p>Table 2: Results on LibriSpeech English-French dataset.</p><h3 id="_2-visualizations" tabindex="-1"><a class="header-anchor" href="#_2-visualizations" aria-hidden="true">#</a> 2. Visualizations</h3><p><img src="/blog/assets/c10-ZlmOQc3z.png" alt="image9"></p><p>Regardless of the input modality, the shared semantic projection is designed to extract only the semantic categories of information required for decoding. To validate this hypothesis, a 2-dimensional PCA projection was performed in the semantic memories across different samples.</p><p>In the above figure, each colored cluster (circled out) represents a semantic memory element. A &#39;.&#39; corresponds to a speech semantic memory, and a ‚Äú+‚Äù marks a text one. It is obvious that semantic memories are strongly clustered, with each individual learning a specific region. The model&#39;s capacity to overlook representation disparities and bridge the modality gap is demonstrated by the close distance of speech and text representations inside the same region.</p><p><img src="/blog/assets/c11-EySzCPZV.png" alt="image9"></p><p>One randomly selected semantic memory subspace was analyzed by PCA to its related cluster to get a better look at the structure of each semantic memory subspace.</p><p>The above figure is the visualization of one specific semantic memory with no different samples or modalities. &quot;+&quot; denotes text representations, while &quot;.&quot; denotes speech representations. Marks of the same color are linked by dashed lines and come from the same speech-transcript pair. Some speech-transcript pairs have been circled and their transcripts have been annotated. Three different fonts denote three different sets of transcripts with similar patterns. As can be seen from the circles, the matched speech and transcript inputs are indeed close to each other. Such results provide strong evidence of the efficacy of the semantic memory module, especially when considering audio and text represent different modalities.</p><h2 id="summary" tabindex="-1"><a class="header-anchor" href="#summary" aria-hidden="true">#</a> Summary</h2><p>Going back to the polyglots, who successfully learn new languages through various kinds of interactions with the environments such as speaking, reading and writing, Chimera makes one important step towards drawing strength from text machine translation to advance speech translation. In general, Chimera unites MT and ST tasks by projecting audio and text data to a shared semantic representation, boosting performance on ST benchmarks MuST-C and Augmented Librispeech to a new state-of-the-art. Further visualizations show that the shared semantic space does indeed convey common knowledge between these two tasks, paving the path for novel ways to supplement training materials across modalities.</p><p>In the future, we are looking forward to more advanced techniques to solve two additional problems: 1) how to tightly align speech and text representations and 2) how to make the workflows of MT and ST fully shared. Since ST is an exciting new area, there are a lot of interesting research and progress almost every week. In the near future, we believe a beautiful new world, where the real time speech translation comes true and the language barriers among countries and nations are broken, is waiting fo us.</p><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references" aria-hidden="true">#</a> References</h2><p>[1] Han, Chi, Mingxuan Wang, Heng Ji, and Lei Li. &quot;Learning Shared Semantic Space for Speech-to-Text Translation.&quot; ACL 2021.</p><p>[2] Van Atteveldt, Nienke, Elia Formisano, Rainer Goebel, and Leo Blomert. &quot;Integration of letters and speech sounds in the human brain.&quot; Neuron 43, no. 2 (2004): 271-282.</p><p>[3] <a href="https://en.wikipedia.org/wiki/Ioannis_Ikonomou" target="_blank" rel="noopener noreferrer">https://en.wikipedia.org/wiki/Ioannis_Ikonomou<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>[4] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Advances in Neural Information Processing Systems, volume 33, pages 12449‚Äì12460. Curran Associates, Inc.</p><p>[5] Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. &quot;Attention is all you need.&quot; In Advances in neural information processing systems, pp. 5998-6008. 2017.</p></div><!--[--><!----><!--]--><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/lileicc/blog/edit/main/dl4mt/2021/chimera/README.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item git-info"><div class="update-time"><span class="label">Last update: </span><!----></div><div class="contributors"><span class="label">Contributors: </span><!--[--><!--[--><span class="contributor" title="email: lileicc@gmail.com">Lei Li</span><!--]--><!--]--></div></div></footer><!----><div id="comment" class="giscus-wrapper input-top" style="display:block;"><div class="loading-icon-wrapper" style="display:flex;align-items:center;justify-content:center;height:96px"><svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" preserveAspectRatio="xMidYMid" viewBox="25 25 50 50"><animateTransform attributeName="transform" type="rotate" dur="2s" keyTimes="0;1" repeatCount="indefinite" values="0;360"></animateTransform><circle cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="4" stroke-linecap="round"><animate attributeName="stroke-dasharray" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="1,200;90,200;1,200"></animate><animate attributeName="stroke-dashoffset" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="0;-35px;-125px"></animate></circle></svg></div></div><!--[--><!----><!--]--><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">Li Lab</div><div class="vp-copyright">Copyright ¬© 2023 Xianjun Yang</div></footer></div><!--]--><!----><!----><!--]--></div>
    <script type="module" src="/blog/assets/app-ZUeO6846.js" defer></script>
  </body>
</html>
