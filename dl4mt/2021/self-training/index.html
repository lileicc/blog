<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.0" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.1" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://lileicc.github.io/blog/dl4mt/2021/self-training/"><meta property="og:site_name" content="Li-Lab Blog"><meta property="og:title" content="Revisiting Self-training for Neural Sequence Generation"><meta property="og:description" content="Self-training is a very prevalent semi-supervised method. Its key idea is to augment the original labeled dataset with unlabeled data paired with the model's prediction (i.e. the pseudo-parallel data). Self-training has been widely used in classification tasks. However, will it work on sequence generation tasks (e.g. machine translation)? If so, how does it work? This blog introduces a work [1] which investigates these questions and gives the answers."><meta property="og:type" content="article"><meta property="og:image" content="https://lileicc.github.io/blog/"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2022-09-13T03:45:15.000Z"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image:alt" content="Revisiting Self-training for Neural Sequence Generation"><meta property="article:author" content="Zekun Li"><meta property="article:tag" content="Self-training"><meta property="article:published_time" content="2021-12-05T00:00:00.000Z"><meta property="article:modified_time" content="2022-09-13T03:45:15.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Revisiting Self-training for Neural Sequence Generation","image":["https://lileicc.github.io/blog/"],"datePublished":"2021-12-05T00:00:00.000Z","dateModified":"2022-09-13T03:45:15.000Z","author":[{"@type":"Person","name":"Zekun Li"}]}</script><title>Revisiting Self-training for Neural Sequence Generation | Li-Lab Blog</title><meta name="description" content="Self-training is a very prevalent semi-supervised method. Its key idea is to augment the original labeled dataset with unlabeled data paired with the model's prediction (i.e. the pseudo-parallel data). Self-training has been widely used in classification tasks. However, will it work on sequence generation tasks (e.g. machine translation)? If so, how does it work? This blog introduces a work [1] which investigates these questions and gives the answers.">
    <link rel="preload" href="/blog/assets/style-e7wIrRON.css" as="style"><link rel="stylesheet" href="/blog/assets/style-e7wIrRON.css">
    <link rel="modulepreload" href="/blog/assets/app-TCeo_ajs.js"><link rel="modulepreload" href="/blog/assets/index.html-ZeEjo3Po.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-x3n3nnut.js"><link rel="modulepreload" href="/blog/assets/index.html-ezBeMfjC.js">
    <link rel="prefetch" href="/blog/assets/index.html-uWfE6SbM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nE6gO_7K.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rddA2LN3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wZ8zj1zx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PlQK4o96.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9Xh1KOjV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-VxJu2QC1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-U1VQl2Eb.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-gq5IQW34.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Eum09ixv.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-k_z2qdXd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nDgcuDE0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-n_sa__IF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-02N2Cr31.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-aOh-NUrG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-YsvHpHbM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SwXHc6hE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hkym9iFT.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-qCUSFaKS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--SW4lVd9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ejNKoiVJ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Z1t3jQDg.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-RYhmq5TF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oWDVHkT8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ROvhtggM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-JUJ-rlbb.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Ptx4qWCt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-umnWKYQq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rP7ssLIy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-pXPA5v4g.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-vstHpLDr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7boXnOP-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-E_bwP14Y.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-II3fUI6m.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CQrB_jON.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mLIoTvBR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-n7XOGv9Z.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-zfNzFZSH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-i3ALVI_i.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0sOpWKpl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-1cll9k9d.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7HN2H0UB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZDqg7rQw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-pdN0LDKr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-eNcYnvl4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-t2PsU7-4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-G00w86XT.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-U5Qrn136.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bcJG7kH0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-IKJ4tLEW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BL0MJvIF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-8Z5PG7OQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-V5Y8hLj3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cRwt7pxi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-qsNk41eE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GOpL5FJN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_duZqh60.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-OrLT7NEC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-lJG6txrw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oLHMXl4g.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-EMVGsE7D.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-vv4O3-5x.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-be7D4hpM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BT0eMqel.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SLiRoZsw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-1NcWQ1JA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ekyShiFQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-sgk1m2_n.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-kb8GfeRD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xXnQV_mG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-INKfADYC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZCH3VU2M.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_hKQG08R.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-bNOG1auN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-mePQ6M8X.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_TMm0KxD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-gx5foYfd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5Mhhvs5z.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-95M6igPl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jk41IluR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-m9EbEEr8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-k7_BPpe1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Ex4rTbEQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-XXOQbmSB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-x11OWh_8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5jEY1mB9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-HKCgalpW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-PxIjFQ4M.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Slp92PgS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-QXd7khlA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-zWDW-dE0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-w1429E4w.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-62MXtNkc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Tbf_mkls.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hJ-E417x.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FuZtKTKb.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Tv61iqe7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cw-4yGRl.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-bKNj4RYx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-iYZB-JbA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-RKPNe-DY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-pSPa82Ht.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-h4ofnM77.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-o5XIGgxU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-1KgcEp7d.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rgDmtxzC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-zptFsrns.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-L_Vpusdt.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oFmu9Riz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C-F0YMcE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ePYWVr43.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-VGTCbEKH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-m613z7qy.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-K3vd68-z.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Pe24msDw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GA1jJEr_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7tVkGo2o.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-h4DrqiSa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-oLGgogi3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FuPET0ka.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-3ImcXxNM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-fAiBxV-C.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wfv4V-9y.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0NhC4spl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-hubfQiTd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-SaTzCUQS.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4fMtu3Sn.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nzkrpgLA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GKUqQ3rT.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-yDhhzzRG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html--p6oz0Q4.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jqd-2qdZ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Q2YhSatR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-UY4YFkOl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xcmj5LbI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7I0SQuFR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TsTfjUqR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TvPj7CxK.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-uxnPR0Lp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-NcCekipx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-HL1RCy7q.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-JvCUhiEm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-wfqH0YcV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9bvCn9s_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Z3xS8x1W.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-reiNbN8M.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jbYZ4as8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-WR6QlAin.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-WaFgOK5R.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-VUvPSPw2.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-0b-Nw4qY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FSdd7GgV.js" as="script"><link rel="prefetch" href="/blog/assets/giscus-unEZQsJ0.js" as="script"><link rel="prefetch" href="/blog/assets/auto-HRhNfH8L.js" as="script"><link rel="prefetch" href="/blog/assets/index-rBp-GJb9.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-i2ohwMnJ.js" as="script"><link rel="prefetch" href="/blog/assets/SearchResult-qkUJdFfb.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container no-sidebar has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/blog/"><img class="vp-nav-logo" src="/blog/logo.svg" alt="Li-Lab Blog"><!----><span class="vp-site-name hide-in-pad">Li-Lab Blog</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="Blog Home" class="vp-link nav-link nav-link" href="/blog/"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="home" width="1em" height="1em"></iconify-icon>Blog Home<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Category" class="vp-link nav-link nav-link" href="/blog/category/"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="baseline-category" width="1em" height="1em"></iconify-icon>Category<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Tags" class="vp-link nav-link nav-link" href="/blog/tag/"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="tag" width="1em" height="1em"></iconify-icon>Tags<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Timeline" class="vp-link nav-link nav-link" href="/blog/timeline/"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="time" width="1em" height="1em"></iconify-icon>Timeline<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/lileicc/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" class="outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="outlook-dropdown"><!----></div></button></div><!--[--><button type="button" class="search-pro-button" role="search" aria-label="Search"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">Search</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Revisiting Self-training for Neural Sequence Generation</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Zekun Li</span></span><span property="author" content="Zekun Li"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2021-12-05T00:00:00.000Z"></span><span class="page-category-info" aria-label="Category🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category4 clickable" role="navigation">MT</span><span class="page-category-item category1 clickable" role="navigation">DL4MT</span><!--]--><meta property="articleSection" content="MT,DL4MT"></span><span class="page-tag-info" aria-label="Tag🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag0 clickable" role="navigation">Self-training</span><!--]--><meta property="keywords" content="Self-training"></span><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 5 min</span><meta property="timeRequired" content="PT5M"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_1-introduction">1. Introduction</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_2-case-study-on-machine-translation">2. Case Study on Machine Translation</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_3-the-secret-behind-self-training">3. The Secret Behind Self-training</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_4-the-proposed-method-noisy-self-training">4. The Proposed Method: Noisy Self-training</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_5-experiments">5. Experiments</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#machine-translation">Machine Translation</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#analysis">Analysis</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#summary">Summary</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#references">References</a></li><!----><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><p>Self-training is a very prevalent semi-supervised method. Its key idea is to augment the original labeled dataset with unlabeled data paired with the model&#39;s prediction (i.e. the <em>pseudo-parallel</em> data). Self-training has been widely used in classification tasks. However, will it work on sequence generation tasks (e.g. machine translation)? If so, how does it work? This blog introduces a work [1] which investigates these questions and gives the answers.</p><!-- more --><p>Reading Time: About 10 minutes.</p><p>Paper：https://arxiv.org/abs/1909.13788</p><p>Github: https://github.com/jxhe/self-training-text-generation</p><h2 id="_1-introduction" tabindex="-1"><a class="header-anchor" href="#_1-introduction" aria-hidden="true">#</a> 1. Introduction</h2><p><img src="/blog/assets/self-training-YcFz26Zd.jpg" alt="image1"> Deep neural networks often require large amounts of labeled data to achieve good performance. However, it is very costly to acquire labels. So what if there is not enough labeled data? Researchers try to fully utilize the unlabeled data to improve the model performance. Self-training is a simple but effective method. As can be seen in the figure above, in self-training, a base model trained with labeled data acts as a “teacher” to label the unannotated data, which is then used to augment the original small training set. Then, a “student” model is trained with this new training set to yield the final model. Self-training is originally designed for classification problems, and it is believed that this method may be effective only when a good fraction of the predictions on unlabeled samples are correct, otherwise errors will be accumulated.</p><p>However, self-training has not been studied extensively in neural sequence generation tasks like machine translation, where the target output is natural language. So the question arises: can self-training still be useful in this case? Here we introduce a work [1] which investigate the problem and answer the two questions:</p><ol><li>How does self-training perform in sequence generation tasks like machine translation?</li><li>If self-training helps improving the baseline, what contributes to its success?</li></ol><h2 id="_2-case-study-on-machine-translation" tabindex="-1"><a class="header-anchor" href="#_2-case-study-on-machine-translation" aria-hidden="true">#</a> 2. Case Study on Machine Translation</h2><p>The authors first analyze the machine translation task, and then perform ablation analysis to understand the contributing factors of the performance gains.</p><p>They work with the standard WMT 2014 English-German dataset. As a preliminary experiment, they randomly sample 100K sentences from the training set (WMT100K) and use the remaining English sentences as the unlabeled monolingual data. They train with the Base Transformer architecture and use beam search decoding (beam size 5).</p><p><img src="/blog/assets/bar-fG6-PXLe.png" alt="image2"> Green bars in the above figure shows the result of applying self-training for three iterations, which includes:</p><ol><li>Pseudo-training (PT): the first step of self-training where we train a new model (from scratch) using only the pseudo parallel data generated by the current model</li><li>Fine-tuning (FT): the fine-tuned system using real parallel data based on the pretrained model from the PT step.</li></ol><p>It is surprising that the pseudo-training step at the first iteration is able to improve BLEU even if the model is only trained on its own predictions, and fine-tuning further boosts the performance. An explanation is that the added pseudo-parallel data might implicitly change the training trajectory towards a (somehow) better local optimum, given that we train a new model from scratch at each iteration.</p><table><thead><tr><th style="text-align:left;">Methods</th><th style="text-align:center;">PT</th><th style="text-align:center;">FT</th></tr></thead><tbody><tr><td style="text-align:left;">baseline</td><td style="text-align:center;">-</td><td style="text-align:center;">15.6</td></tr><tr><td style="text-align:left;">baseline (w/o dropout)</td><td style="text-align:center;">-</td><td style="text-align:center;">5.2</td></tr><tr><td style="text-align:left;">ST (beam search, w/ dropout)</td><td style="text-align:center;">16.5</td><td style="text-align:center;">17.5</td></tr><tr><td style="text-align:left;">ST (sampling, w/ dropout)</td><td style="text-align:center;">16.1</td><td style="text-align:center;">17.0</td></tr><tr><td style="text-align:left;">ST (beam search, w/o dropout)</td><td style="text-align:center;">15.8</td><td style="text-align:center;">16.3</td></tr><tr><td style="text-align:left;">ST (sampling, w/o dropout)</td><td style="text-align:center;">15.5</td><td style="text-align:center;">16.0</td></tr><tr><td style="text-align:left;">Noisy ST (beam search, w/o dropout)</td><td style="text-align:center;">15.8</td><td style="text-align:center;">17.9</td></tr><tr><td style="text-align:left;">Noisy ST (beam search, w/ dropout)</td><td style="text-align:center;"><strong>16.6</strong></td><td style="text-align:center;"><strong>19.3</strong></td></tr></tbody></table><h2 id="_3-the-secret-behind-self-training" tabindex="-1"><a class="header-anchor" href="#_3-the-secret-behind-self-training" aria-hidden="true">#</a> 3. The Secret Behind Self-training</h2><p>To decode the secret of self-training and understand where the gain comes from, they formulate two hypotheses:</p><ol><li><p><strong>Decoding Strategy</strong>: According to this hypothesis, the gains come from the use of beam search for decoding unlabeled data. The above table shows the performance using different decoding strategies. As can be seen, the performance drops by 0.5 BLEU when the decoding strategy is changed to sampling, which implies that beam search does contribute a bit to the performance gains. This phenomenon makes sense intuitively since beam search tends to generate higher-quality pseudo targets than sampling. However, the decoding strategy hypothesis does not fully explain it, as there is still a gain of 1.4 BLEU points over the baseline from sampling decoding with dropout.</p></li><li><p><strong>Dropout</strong>: The results in the above table indicate that without dropout the performance of beam search decoding drops by 1.2 BLEU, just 0.7 BLEU higher than the baseline. Moreover, the pseudo-training performance of sampling without dropout is almost the same as the baseline.</p></li></ol><p>In summary, beam-search decoding contributes only partially to the performance gains, while the implicit perturbation i.e., dropout accounts for most of it. The authors also conduct experiment on a toy dataset to show that noise is beneficial for self-training because it enforces local smoothness for this task, that is, semantically similar inputs are mapped to the same or similar targets.</p><h2 id="_4-the-proposed-method-noisy-self-training" tabindex="-1"><a class="header-anchor" href="#_4-the-proposed-method-noisy-self-training" aria-hidden="true">#</a> 4. The Proposed Method: Noisy Self-training</h2><p>To further improve performance, the authors considers a simple model-agnostic perturbation process - perturbing the input, which is referred to as <em>noisy self-training</em>. Note that they apply both input perturbation and dropout in the pseudo-training step for noisy ST. They first apply noisy ST to the WMT100K translation task. Two different perturbation function are tested:</p><ol><li>Synthetic noise: the input tokens are randomly dropped, masked, and shuffled.</li><li>Paraphrase: they translate the source English sentences to German and translate it back to obtain a paraphrase as the perturbation.</li></ol><p>Figure 2 shows the results over three iterations. Noisy ST greatly outperforms the supervised baseline and normal ST, while synthetic noise does not exhibit much difference from paraphrase. Since synthetic noise is much simpler and more general, it is defaulted in Noisy ST. Table 1 also reports an ablation study of Noisy ST when removing dropout at the pseudo-training step. Noisy ST without dropout improves the baseline by 2.3 BLEU points and is comparable to normal ST with dropout. When combined together, noisy ST with dropout produces another 1.4 BLEU improvement, indicating that the two perturbations are complementary.</p><h2 id="_5-experiments" tabindex="-1"><a class="header-anchor" href="#_5-experiments" aria-hidden="true">#</a> 5. Experiments</h2><h3 id="machine-translation" tabindex="-1"><a class="header-anchor" href="#machine-translation" aria-hidden="true">#</a> Machine Translation</h3><p>The author test the proposed noisy ST on a high-resource MT benchmark: WMT14 English-German and a low-resource one: FloRes English-Nepali.</p><table><thead><tr><th style="text-align:left;">Methods</th><th style="text-align:center;">WMT14 100K</th><th style="text-align:center;">WMT14 3.9M</th><th style="text-align:center;">FloRes En-Origin</th><th style="text-align:center;">FloRes Ne-Origin</th><th style="text-align:center;">FloRes Overall</th></tr></thead><tbody><tr><td style="text-align:left;">baseline</td><td style="text-align:center;">15.6</td><td style="text-align:center;">28.3</td><td style="text-align:center;">6.7</td><td style="text-align:center;">2.3</td><td style="text-align:center;">4.8</td></tr><tr><td style="text-align:left;">BT</td><td style="text-align:center;">20.5</td><td style="text-align:center;">-</td><td style="text-align:center;">8.2</td><td style="text-align:center;"><strong>4.5</strong></td><td style="text-align:center;"><strong>6.5</strong></td></tr><tr><td style="text-align:left;">Noisy ST</td><td style="text-align:center;"><strong>21.4</strong></td><td style="text-align:center;"><strong>29.3</strong></td><td style="text-align:center;"><strong>8.9</strong></td><td style="text-align:center;">3.5</td><td style="text-align:center;"><strong>6.5</strong></td></tr></tbody></table><p>The overall results are shown in the above table. For almost all cases in both datasets, the noisy ST outperforms the baselines by a large margin, and noisy ST still improves the baseline even when this is very weak.</p><h4 id="comparison-with-back-translation" tabindex="-1"><a class="header-anchor" href="#comparison-with-back-translation" aria-hidden="true">#</a> Comparison with Back Translation</h4><p>It can be seen that noisy ST is able to beat BT on WMT100K and on the en-origin test set of FloRes. In contrast, BT is more effective on the ne-origin test set according to BLEU, which is not surprising as the ne-origin test is likely to benefit more from Nepali than English monolingual data.</p><p><img src="/blog/assets/analysis-FWBy7mkV.png" alt="image3"></p><h3 id="analysis" tabindex="-1"><a class="header-anchor" href="#analysis" aria-hidden="true">#</a> Analysis</h3><p>The authors analyze the effect of the following three factors on noisy self-training on the WMT14 dataset:</p><ol><li>Parallel dat size</li><li>Monolingual dat size</li><li>Noise level The result is shown in the above figure. In (a) we see that the performance gain is larger for intermediate value of the size of the parallel dataset, as expected. (b) illustrates that the performance keeps improving as the monolingual data size increases, albeit with diminishing returns. (c) demonstrates that performance is quite sensitive to noise level, and that intermediate values work best. It is still unclear how to select the noise level a priori, besides the usual hyper-parameter search to maximize BLEU on the validation set.</li></ol><h2 id="summary" tabindex="-1"><a class="header-anchor" href="#summary" aria-hidden="true">#</a> Summary</h2><p>This work revisit self-training for neural sequence generation, especially machine translation task. It is shown that self-training can be an effective method to improve generalization, particularly when labeled data is scarce. Through comprehensive experiments, they prove that noise injected during self-training is critical and thus propose to perturb the input to obtain a variant of self-training, named noisy self-training, which show great power on machine translation and also text summarization tasks.</p><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references" aria-hidden="true">#</a> References</h2><p>[1] He, Junxian, et al. &quot;Revisiting Self-Training for Neural Sequence Generation.&quot; International Conference on Learning Representations. 2019.</p></div><!--[--><!----><!--]--><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/lileicc/blog/edit/main/dl4mt/2021/self-training/README.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item git-info"><div class="update-time"><span class="label">Last update: </span><!----></div><div class="contributors"><span class="label">Contributors: </span><!--[--><!--[--><span class="contributor" title="email: lileicc@gmail.com">Lei Li</span><!--]--><!--]--></div></div></footer><!----><div id="comment" class="giscus-wrapper input-top" style="display:block;"><div class="loading-icon-wrapper" style="display:flex;align-items:center;justify-content:center;height:96px"><svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" preserveAspectRatio="xMidYMid" viewBox="25 25 50 50"><animateTransform attributeName="transform" type="rotate" dur="2s" keyTimes="0;1" repeatCount="indefinite" values="0;360"></animateTransform><circle cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="4" stroke-linecap="round"><animate attributeName="stroke-dasharray" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="1,200;90,200;1,200"></animate><animate attributeName="stroke-dashoffset" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="0;-35px;-125px"></animate></circle></svg></div></div><!--[--><!----><!--]--><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">Li Lab</div><div class="vp-copyright">Copyright © 2023 Zekun Li</div></footer></div><!--]--><!----><!----><!--]--></div>
    <script type="module" src="/blog/assets/app-TCeo_ajs.js" defer></script>
  </body>
</html>
