<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.11" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.43" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://lileicc.github.io/blog/dl4mt/2021/gpt/"><meta property="og:site_name" content="Li-Lab Blog"><meta property="og:title" content="Generative Pre-trained Transformer (GPT)"><meta property="og:description" content="In the past couple years, we have seen the rise of Transformer architectures in Natural Language Processing. Transformers revolutionized the speed and accuracy of machine transl..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2024-05-25T23:06:05.000Z"><meta property="article:author" content="Alex Rasla"><meta property="article:tag" content="Pre-training"><meta property="article:tag" content="Language Modelling"><meta property="article:published_time" content="2021-11-01T00:00:00.000Z"><meta property="article:modified_time" content="2024-05-25T23:06:05.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Generative Pre-trained Transformer (GPT)","image":[""],"datePublished":"2021-11-01T00:00:00.000Z","dateModified":"2024-05-25T23:06:05.000Z","author":[{"@type":"Person","name":"Alex Rasla"}]}</script><title>Generative Pre-trained Transformer (GPT) | Li-Lab Blog</title><meta name="description" content="In the past couple years, we have seen the rise of Transformer architectures in Natural Language Processing. Transformers revolutionized the speed and accuracy of machine transl...">
    <link rel="preload" href="/blog/assets/style-BEIjLUpX.css" as="style"><link rel="stylesheet" href="/blog/assets/style-BEIjLUpX.css">
    <link rel="modulepreload" href="/blog/assets/app-DvxBpnKI.js"><link rel="modulepreload" href="/blog/assets/index.html-CbDJmTDz.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/blog/assets/index.html-BroJAVs1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DqRRrdxN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-geg8hdaA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-T38F122l.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BU0zenT9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DRY_Rg1V.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-yeU1vgED.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DWy6xs1J.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-pCrE8yHf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BJ_AghC5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-k3b_T-SP.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5PV3NlK2.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rqHrn9XR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DXKear5F.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DDGXC_ws.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DQsqSk5b.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nXBkKbPa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZWqMQjDf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BmkCcctU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CO-pLJt9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DpYleMCC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Dc1M0ATi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CauSBX8J.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-Cw-U9o5J.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B_ynrIRb.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-QtilJd5B.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BNCIUQPO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DdBHs2Us.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DOSYHKuw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-qalJTdOm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Chsz9oWa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ChJmO1W9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B4SXWPkk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Brw2XJ0u.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DkfNdZ8V.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CSREm_rq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-8X7fVq8Q.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-lG0YXER3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Dx4J44pa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-T9BHFuQN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Y-i8XmH5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DK7Fp7xG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CRTjq6AW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CoiG0KoW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DRJCQVaX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TJWQZwkM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_9uD0-ss.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Bo--ZP-D.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9Zna8f0T.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DF9aEpdE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DVXDgIxX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FdwJo6wG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-tsChVsQB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DLTaczOx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-COn1v7J7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cFqiIE9S.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DkU8CzUY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-AfrCKcc-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BiQLQhAV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BPrRjTfB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Y_uyyvMq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-JDfCOuVV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CCFENeZX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DYUMSDb3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CPfa70U7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DQUwiUh-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GAJuCCKc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B3hZ6uy3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CqQ0-pfF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7yOzg9-j.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-v8rZf8KZ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ukHX_v8t.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BFYa-2at.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BOt7aLMO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DJKc1PBD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BPUrixrx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Dnwk3GJG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CxLTGcig.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DyAaHm9G.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-D5yXusUE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-x5aSH1Ah.js" as="script"><link rel="prefetch" href="/blog/assets/giscus--_FS5kYt.js" as="script"><link rel="prefetch" href="/blog/assets/auto-CAdRPfCH.js" as="script"><link rel="prefetch" href="/blog/assets/index-wZ-hXvzw.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-SzV8tJDW.js" as="script"><link rel="prefetch" href="/blog/assets/SearchResult-Rjd_xNPH.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container no-sidebar external-link-icon has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/blog/"><img class="vp-nav-logo" src="/blog/logo.svg" alt><!----><span class="vp-site-name hide-in-pad">Li-Lab Blog</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/" aria-label="Blog Home"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="home" width="1em" height="1em"></iconify-icon>Blog Home<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/category/" aria-label="Category"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="baseline-category" width="1em" height="1em"></iconify-icon>Category<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/tag/" aria-label="Tags"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="tag" width="1em" height="1em"></iconify-icon>Tags<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/timeline/" aria-label="Timeline"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="time" width="1em" height="1em"></iconify-icon>Timeline<!----></a></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/lileicc/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon" name="outlook"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="vp-outlook-dropdown"><!----></div></button></div><!--[--><button type="button" class="search-pro-button" aria-label="Search"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon" name="search"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">Search</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!----><ul class="vp-sidebar-links"></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Generative Pre-trained Transformer (GPT)</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Alex Rasla</span></span><span property="author" content="Alex Rasla"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2021-11-01T00:00:00.000Z"></span><span class="page-category-info" aria-label="Category🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color6 clickable" role="navigation">NLG</span><span class="page-category-item color0 clickable" role="navigation">DL4MT</span><!--]--><meta property="articleSection" content="NLG,DL4MT"></span><span class="page-tag-info" aria-label="Tag🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color1 clickable" role="navigation">Pre-training</span><span class="page-tag-item color1 clickable" role="navigation">Language Modelling</span><!--]--><meta property="keywords" content="Pre-training,Language Modelling"></span><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 8 min</span><meta property="timeRequired" content="PT8M"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!----><div class="vp-toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#novelty">Novelty</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#evaluation-on-various-tasks">Evaluation on Various Tasks</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#translation">Translation</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#word-prediction-and-text-generation">Word Prediction and Text Generation</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#comprehension">Comprehension</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#case-study">Case Study</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#conclusion">Conclusion</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#references">References</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content"><p>In the past couple years, we have seen the rise of Transformer architectures in Natural Language Processing. Transformers revolutionized the speed and accuracy of machine translation systems, and alleviated the need for Recurrent Neural Networks and LSTMs to derive context and meaning for sequence to sequence modeling. Since the <em>Attention Is All You Need</em> paper was published in 2017, there have been many experimental application and fine-tuning improvements made upon the original model. The latest such improvement is the Generative Pre-Trained Transformer 3, or GPT-3.</p><p>GPT-3 is the third generation of the autoregressive language modeling GPT-n series created by OpenAI. Its architecture is exactly the same as the previous generation, GPT-2, with the exception of using alternating dense and locally banded sparce attention patterns. For this generation, OpenAI trained 8 different models with a variable number of parameters, layers, dimensions, learning rates, and batch sizes. Furthermore, unlike the GPT-2 which was trained on the WebText dataset, GPT-3 was trained on Common Crawl, which consists of nearly a trillion words. However, because this dataset is so large and unfiltered, various techniques had to be used to prevent overfitting the model and data contaimination.</p><h2 id="novelty" tabindex="-1"><a class="header-anchor" href="#novelty"><span>Novelty</span></a></h2><p>Aside from the changes made above to the new model, the novelty of GPT-3 stems from its ability to use one-shot and few-shot demonstrations — it is not limited to zero-shot like GPT-2. To be more specific, this means that before actually testing GPT-3 on a specific task, the model can take a certain amount of examples <em>K</em> of the desired task as a form of conditioning before letting the model perform testing and inference on its own. For one-shot demonstrations, <em>K</em> is 1, and for few-shot demonstrations, <em> K </em> can be anywhere between 10 and 100 demonstrations of the task. This may sounds similar to fine-tuning a model, but there is one key difference: for these n-shot demonstrations, the model is <em> NOT </em> allowed to update its weights whereas for fine-tuning, the model is <em> supposed </em> to update its weights. This novel approach of providing demonstrations before actual testing was specifically chosen because it best simulates human learning and behavior. Typically, when people are told to complete a task, they are usually offered one or more examples before having to attempt the task on their own. The table below illustrates the advantages and disadvantages of each of the different types of n-shots for GPT-3.</p><table><thead><tr><th>Type</th><th>Advantages</th><th>Disadvantages</th></tr></thead><tbody><tr><td>Fine Tuning</td><td>Strong performance on a specific task</td><td>The need for an new, large dataset for the specific task</td></tr><tr><td>Few-Shot</td><td>Major reduction in the need for task specific data and examples</td><td>Potential for much worse performance than state-of-the-art fine tuned models</td></tr><tr><td>One-Shot</td><td>Most closely related to the way tasks are communicated to humans</td><td>Performs worse than Few-Shot</td></tr><tr><td>Zero-Shot</td><td>Most convenient and no need for additional data</td><td>Most challenging setting, humans cannot even perform some tasks without examples</td></tr></tbody></table><h2 id="evaluation-on-various-tasks" tabindex="-1"><a class="header-anchor" href="#evaluation-on-various-tasks"><span>Evaluation on Various Tasks</span></a></h2><p>With these new types of testing and inference methods, GPT-3 was evaluated on a variety of NLP tasks that include text translation, generation, prediction, and comprehension. This fact alone — that one single model is able to be evaluated on a range of tasks — shows how powerful and exciting GPT-3 is. Prior to GTP-3, models had to be explicitly trained to do a specific task. In the following subsections, when describing the evaluations and accuracy for the different use cases, you will see that GPT-3 is powerful enough to match or even outperform some state-of-the-art models for NLP tasks.</p><h3 id="translation" tabindex="-1"><a class="header-anchor" href="#translation"><span>Translation</span></a></h3><p>One of the tasks GPT-3 was tested on was its ability to perform sequence to sequence translation. The Common Crawl dataset OpenAI used to train their model contained English words 93% of the time and words from foreign languages 7% of the time. The words from the foreign natural languages were seamlessly blended together with the English text on a word, sentence, and document level. This allowed for the model to extract meaning and context from the foreign words, and be able to accurately translate them when asked to do so. From this translation test, GPT-3 resulted in a similar BLEU score compared to prior unsupervised NMT work. While zero-shot underperformed compared to prior works, one-shot improved the BLEU score by 4 and few-shot improved the BLEU score by 11. BLEU score improvement was especially noticed when translating phrases into English as opposed to translating sentence from English. In fact, some translations from English even outperform the best supervised results. The table and graph below shows the comparison of BLEU scores for each of the different n-shots and other NMT models, and how BLEU score changes with model size.</p><p><img src="/blog/assets/bleu_table-CJizH49a.png" alt=""></p><p><img src="/blog/assets/bleu_graph-Cb3FcaKf.png" alt=""></p><h3 id="word-prediction-and-text-generation" tabindex="-1"><a class="header-anchor" href="#word-prediction-and-text-generation"><span>Word Prediction and Text Generation</span></a></h3><p>Another task GPT-3 was tested on was predicting the next word given a few sentences or a passage. A demonstration along the lines of <em> Alice was friends with Bob. Alice went to visit her friend __. → Bob </em> was given to the model as an example along with a task description. From examples like these, the largest GPT-3 model (with 175 billion parameters) outperformed the current state-of-the-art model by 20%, predicting the next word correctly 86.4% of the time. Even if a model was trained on this specific task and outperformed a state-of-the-art model by 20%, it would still be extraordinary.</p><p>Another simliar test using GPT-3 was news article generation. The model was given a number of news articles written by humans as conditioning, and asked to generate a news article itself. Because this task is difficult to measure quantitatively, they measured the performance of the task based on whether or not a human could distinguish GPT-3 generated articles from human written one. As expected, as the size of the model increased, the ability for humans to distinguish the difference between a generated article and a human written one decreased. Astoundingly though, the mean human accuracy for detecting generated articles for the largest GPT-3 model was just slightly above chance at 52%.</p><h3 id="comprehension" tabindex="-1"><a class="header-anchor" href="#comprehension"><span>Comprehension</span></a></h3><p>One of the most extensive tasks GPT-3 was trained on was its ability to comprehend and extract meaning from text. This was tested in a varitey of ways including utilizing new words, answering questions based on prompts and facts, and general reasoning.</p><p>To evaluate the model&#39;s ability to utilize new words, the OpenAI team came up with some made up words and provided its definition to the model. From this information, the model was tasked to use the word in a sentence. Since there is no quantitative metric to measure how well a made-up word is used in a sentence, they again used a qualitative human evaluation. In most cases, the GPT-3 generated sentences that included the made-up word seemed to be proficient at using the novel word correctly. A similar test with a quantitative metric had to do with recognizing novel patterns by using SAT analogies. The model was provided SAT style analogy questions and tasked to determine which pair of words in the answer choices had the same relationship as the words in the question. In this task, GPT-3 was correct 65.2% of the time in the few-shot setting. While this may not seem too accurate, take into account that the average college applicant was correct only 57% of the time.</p><p>Another comprehension task GPT-3 was tested on was knowledge-based question answering. The model was given the PhysicalQA dataset which contains a set of common sense questions about how the world works. In this experiment, GPT-3 achieved an accuracy of 82.8% in the few-shot setting on the easy questions. Compared to a previous fine-tuned state-of-the-art model RoBERTa, GPT-3&#39;s accuracy was 3.4% higher, again showcasing its ability to outperform fine-tuned models.</p><p>The last comprehension test I wanted to mention had to do with relationships between sentences and pronoun references. GPT-3 was tested to see if it could determine which subject a pronoun was referring to. This Winograd Style task is a baseline task in natural language processing, where pronoun references are very clear to humans but ambiguous in grammatical context. Fine-tuned models designed for this task achieve almost perfect accuracy now, but GPT-3 still only falls a few points below state-of-the-art models at 88.6%. A similar task has to do with natural language inference between two sentences. Here, the model was tested to understand the relationship between two sentences. Usually, this involves determining whether or not a sentence can logically follow a given sentence. This task was evaluated on SuperGLUE&#39;s RTE dataset, where GPT-3&#39;s accuracy performed just slightly above random at 56%. I mention this test to show that while GPT-3 is a very powerful model, it cannot be generalized to every aspect of natural language processing.</p><h2 id="case-study" tabindex="-1"><a class="header-anchor" href="#case-study"><span>Case Study</span></a></h2><p>As an experiment, I decided to test the BLEU score when translating a few phrases from Slovak, my native language, into English. Since Slovak isn&#39;t a particularly highly studied language in terms of machine translation, I thought it would be interesting to see if GPT-3 could accurately translate Slovak into English. In order to test this, I came up with a set of phrases in Slovak and their respective translations into English using Google Translate. From this set of translations, I took the Slovak phrases, fed them through the GPT-3 model completion task with a few-shot prompt, and did a BLEU score evaluation from the output of the GPT-3 model and the Google Translate reference text. One of the things I noticed throughout this experiment was that the model sometimes generates different translated sentences for the same input. Nevertheless, the BLEU score still hovers around the same values. The prompt and results are shown below. The average BLEU score between GPT-3&#39;s translation and Google Translate for the ten translations was 0.7196.</p><p><a href="https://github.com/alexrasla/GPT-3-Blog" target="_blank" rel="noopener noreferrer">Case Study Code</a></p><blockquote><p>Prompt: Slovak: Dobré ráno!</p><p>English: Good morning!</p><p>Slovak: Ako sa máš?</p><p>English: How are you?</p><p>Slovak: Odkiaľ si?</p><p>English: Where are you from?</p><p>Slovak:</p></blockquote><table><thead><tr><th>Slovak (Input)</th><th>Reference (Google Translate)</th><th>Candidate (GPT-3)</th><th>BLEU Score</th></tr></thead><tbody><tr><td>Dnes si môžeme ísť kúpiť oblečenie do nákupného centra, ale nebudeme sa tam môcť najesť</td><td>Today we can go to the mall to buy clothes, but we will not be able to eat there</td><td>Today we can go to the mall to buy clothes, but we cannot eat there</td><td>0.6313</td></tr><tr><td>O druhej poobede budem doma na obed</td><td>I will be home for lunch at two in the afternoon</td><td>I&#39;ll be home for lunch at 2 o&#39;clock</td><td>0.3553</td></tr><tr><td>Môžete mi poslať e-mail s dátumom a časom stretnutia?</td><td>Can you send me an email with the date and time of the meeting?</td><td>Can you send me an email with the date and time of our meeting?</td><td>0.8555</td></tr><tr><td>Keď sa skončí hokej, mali by sme ísť hrať hokej</td><td>When hockey is over, we should go play hockey</td><td>When hockey ends we should go play hockey</td><td>0.5247</td></tr><tr><td>Kúpim chladničku za sto dolárov</td><td>I&#39;ll buy a refrigerator for a hundred dollars</td><td>I want to buy a refrigerator for $100</td><td>0.3457</td></tr><tr><td>Musím sa učiť na test z biológie</td><td>I have to study for a biology test</td><td>I have to study for a biology test</td><td>1.0</td></tr><tr><td>Poďme do parku a zahrajme si futbal</td><td>Let&#39;s go to the park and play football</td><td>Let&#39;s go to the park and play football</td><td>1.0</td></tr><tr><td>Môžeme mať na večeru kuracie mäso?</td><td>Can we have chicken for dinner?</td><td>Can we have chicken for dinner?</td><td>1.0</td></tr><tr><td>Tento víkend som išiel do domu svojich priateľov a hral videohry</td><td>This weekend I went to my friends&#39; house and played video games</td><td>This weekend I went to my friends&#39; house and played video games</td><td>1.0</td></tr><tr><td>Veľmi sa teším, keď ich zajtra uvidím hrať na koncerte</td><td>I&#39;m very happy to see them play at the concert tomorrow</td><td>I&#39;m very looking forward to seeing them play at the concert tomorrow</td><td>0.4833</td></tr></tbody></table><h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="#conclusion"><span>Conclusion</span></a></h2><p>From this blog, we see that GPT-3 is extremely powerful and can perform better on some NLP tasks than even the state-of-the-art fine-tuned models. GPT-3 is not trained to do anything specific task; rather, it is pretrained on a massive dataset consisting of over a trillion words. When used for a specific task, the model is able to take zero, one, or a few demonstrations of the task depending on the type of experiment you want to conduct. This is ultimately what gives it the flexiblity to be applied to a variety of different domains within NLP.</p><p>For language translation specifically, it achieved BLEU scores slightly below that of some supervised NMT models. This is incredible, given it is not fine-tuned or trained for machine tranlation. When increasing from zero-shot to one-shot to few-shot, GPT-3 performed significantly better and better in translating between languages. It performed best when translating to English because the CommonCrawl dataset used to train GPT-3 contained English words 93% of the time, and foreign words 7% of the time. In my specific case study of translating Slovak into English, the model achieved an average BLEU score of 0.7196 using a few-shot setting.</p><p>In my opinion, even though GPT-3 is the latest and greatest language model to date, there is still room for improvement for these novel large-scale models. While GPT-3 proved that one pre-trained model can perform many NLP tasks and generate impressive results, it still lacks the ability to extract meaning and comprehension from text. Its synthesis techniques are gramatically sound and readable, but its contextual meaning is often repetitive, incoherent, and occasionaly contradictory. There are hypotheses that bidirectional encoder models could solve these sort of comprehension issues and create another incredible model, but until we figure out a way to truly extract meaning and generate long text that gives us coherent information, I believe GPT-3 will remain the status qou for generative language models.</p><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references"><span>References</span></a></h2><p>[1]<a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener noreferrer"><em> Language Models are Few-Shot Learners </em></a></p><p>[2]<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer"><em> Attention Is All You Need </em></a></p></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="vp-link vp-external-link-icon vp-meta-label" href="https://github.com/lileicc/blog/edit/main/dl4mt/2021/gpt/README.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page<!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">Last update: </span><!----></div><div class="contributors"><span class="vp-meta-label">Contributors: </span><!--[--><!--[--><span class="vp-meta-info" title="email: lileicc@gmail.com">Lei Li</span><!--]--><!--]--></div></div></footer><!----><div id="vp-comment" class="giscus-wrapper input-top" style="display:block;"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" preserveAspectRatio="xMidYMid" viewBox="0 0 100 100"><circle cx="28" cy="75" r="11" fill="currentColor"><animate attributeName="fill-opacity" begin="0s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></circle><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 47a28 28 0 0 1 28 28"><animate attributeName="stroke-opacity" begin="0.1s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 25a50 50 0 0 1 50 50"><animate attributeName="stroke-opacity" begin="0.2s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path></svg></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">Li Lab</div><div class="vp-copyright">Copyright © 2024 Alex Rasla </div></footer></div><!--]--><!--[--><!----><!----><!--]--><!--]--></div>
    <script type="module" src="/blog/assets/app-DvxBpnKI.js" defer></script>
  </body>
</html>
