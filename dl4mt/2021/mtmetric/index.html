<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.11" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.43" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://lileicc.github.io/blog/dl4mt/2021/mtmetric/"><meta property="og:site_name" content="Li-Lab Blog"><meta property="og:title" content="Learned Metrics for Machine Translation"><meta property="og:description" content="How to automatically evaluate the quality of a machine translation system? Human evaluation is accurate, but expensive. It is not suitable for MT model development. Reading Time..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2024-05-25T23:17:24.000Z"><meta property="article:author" content="Huake He"><meta property="article:tag" content="MT Evaluation"><meta property="article:tag" content="BERTScore"><meta property="article:tag" content="COMET"><meta property="article:tag" content="BERT"><meta property="article:published_time" content="2021-11-01T00:00:00.000Z"><meta property="article:modified_time" content="2024-05-25T23:17:24.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Learned Metrics for Machine Translation","image":[""],"datePublished":"2021-11-01T00:00:00.000Z","dateModified":"2024-05-25T23:17:24.000Z","author":[{"@type":"Person","name":"Huake He"}]}</script><title>Learned Metrics for Machine Translation | Li-Lab Blog</title><meta name="description" content="How to automatically evaluate the quality of a machine translation system? Human evaluation is accurate, but expensive. It is not suitable for MT model development. Reading Time...">
    <link rel="preload" href="/blog/assets/style-Cq8eyGeZ.css" as="style"><link rel="stylesheet" href="/blog/assets/style-Cq8eyGeZ.css">
    <link rel="modulepreload" href="/blog/assets/app-DiCjo-Va.js"><link rel="modulepreload" href="/blog/assets/index.html-dZRpMDcp.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/blog/assets/index.html-n7qbu3dk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DsZ03cqp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-r3wL3lYW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BTmZnjzu.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B3lBlHt5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DkOZIlpg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CFyAOJxa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Dy--DCvM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BGcY7GRM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CnEewnS_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B-7kV_Pa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Dxt46wjr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ailBi3IV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CLkHMPSR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BcxjbFyQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B5tQQqUs.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7GEx1xL_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-X1I--pzi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Cqxm4VEw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rdxcuHG-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B0QaSsCk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-lbmWsbm6.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BL20rJEw.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-DwhX0xCD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-eyD8xvL-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4T5RsOJr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C4oYB-TX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DT0Vw-X1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ChpH1XCa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CG4zWerc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cDcguak7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BLmhJl3H.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BUnffEMv.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-K8vN4nvk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-izeVRWR8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-p4XBto_y.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-l1eZfZeq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DggTdQeV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BqVwiDmI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ChpRCIc8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5hhedaTR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CIWPrBfh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BYBCTXZh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DWU1ODhk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BbKNZ18w.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BkwXzOb3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jhqGT4dL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CS6gUaSX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CGvZjib9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DkRkbNQ6.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Ic0xQiYz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Bi8bMppE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DPfrum7j.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BYsu2jJc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-chNhRanf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BXwcVWaJ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B6PjGDAO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nmkosgT0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C-1PzENR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BaWZvSMh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-iCm852ld.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BDc3S-C2.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CoxTWtqI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BW3VPrzj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DLbZgarF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DZPCdxOC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CvBuB7g8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BQK4OLG0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-fRbI1bsI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Byllwfsd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DNApUOep.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BHiIQnkE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BuHhtC_q.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DW1rLQdB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CFKdo3WH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B8BC64BL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xb2wQrjg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B6uJhqIN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DrA_NyPl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C7_gn4qD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DPhNyCil.js" as="script"><link rel="prefetch" href="/blog/assets/giscus--_FS5kYt.js" as="script"><link rel="prefetch" href="/blog/assets/auto-CAdRPfCH.js" as="script"><link rel="prefetch" href="/blog/assets/index-wZ-hXvzw.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-SzV8tJDW.js" as="script"><link rel="prefetch" href="/blog/assets/SearchResult-75TAtN8b.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container no-sidebar external-link-icon has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/blog/"><img class="vp-nav-logo" src="/blog/logo.svg" alt><!----><span class="vp-site-name hide-in-pad">Li-Lab Blog</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/" aria-label="Blog Home"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="home" width="1em" height="1em"></iconify-icon>Blog Home<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/category/" aria-label="Category"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="baseline-category" width="1em" height="1em"></iconify-icon>Category<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/tag/" aria-label="Tags"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="tag" width="1em" height="1em"></iconify-icon>Tags<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/timeline/" aria-label="Timeline"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="time" width="1em" height="1em"></iconify-icon>Timeline<!----></a></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/lileicc/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon" name="outlook"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="vp-outlook-dropdown"><!----></div></button></div><!--[--><button type="button" class="search-pro-button" aria-label="Search"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon" name="search"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">Search</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!----><ul class="vp-sidebar-links"></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Learned Metrics for Machine Translation</h1><div class="page-info"><span class="page-author-info" aria-label="Authorüñä" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Huake He</span></span><span property="author" content="Huake He"></span></span><!----><span class="page-date-info" aria-label="Writing DateüìÖ" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2021-11-01T00:00:00.000Z"></span><span class="page-category-info" aria-label="Categoryüåà" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color0 clickable" role="navigation">MT</span><span class="page-category-item color0 clickable" role="navigation">DL4MT</span><!--]--><meta property="articleSection" content="MT,DL4MT"></span><span class="page-tag-info" aria-label="Tagüè∑" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color3 clickable" role="navigation">MT Evaluation</span><span class="page-tag-item color0 clickable" role="navigation">BERTScore</span><span class="page-tag-item color1 clickable" role="navigation">COMET</span><span class="page-tag-item color3 clickable" role="navigation">BERT</span><!--]--><meta property="keywords" content="MT Evaluation,BERTScore,COMET,BERT"></span><span class="page-reading-time-info" aria-label="Reading Time‚åõ" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 12 min</span><meta property="timeRequired" content="PT12M"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!----><div class="vp-toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#a-brief-history-of-mt-evaluation-metrics">A brief history of MT evaluation metrics</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#human-evaluation">Human evaluation</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#automatic-evaluation">Automatic evaluation</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#bertscore">BERTScore</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#motivation">Motivation</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#technique">Technique</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#effectiveness">Effectiveness</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#comet">COMET</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#motivation-1">Motivation</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#technique-1">Technique</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#effectiveness-1">Effectiveness</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#case-study">Case Study</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#conclusion">Conclusion</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#code">Code</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#reference">Reference</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content"><p>How to automatically evaluate the quality of a machine translation system? Human evaluation is accurate, but expensive. It is not suitable for MT model development.</p><p>Reading Time: About 15 minutes.</p><h2 id="a-brief-history-of-mt-evaluation-metrics" tabindex="-1"><a class="header-anchor" href="#a-brief-history-of-mt-evaluation-metrics"><span>A brief history of MT evaluation metrics</span></a></h2><h3 id="human-evaluation" tabindex="-1"><a class="header-anchor" href="#human-evaluation"><span>Human evaluation</span></a></h3><p>In 1966, United States, the Automatic Language Processing Advisory Committee (ALPAC) conducted a large scale study on the evaluation of the state-of-the-art Russian-to-English Machine Translation (MT) systems at that time [1]. Indeed, the ALPAC report was infamous for holding a negative opinion toward the development of MT, and caused the suspension of research into related fields for two decodes. However, one of the first practical method for the evaluation of translation quality was developed from the study. Basically, six trained translators were each assigned to evaluate 144 sentences from 4 passages. The evaluation was based on &quot;intelligibility&quot; and &quot;fidelity&quot;. &quot;Intelligibility&quot; measures to what extent the sentence can be understood, and &quot;fidelity&quot; measures how much information the translated sentence retained compared to the source. Human evaluation was based on these two variables by giving a score on the scale of 1-9. This is one of the earlest systematic MT evaluation metrics based on human judgement. <img src="/blog/assets/ALPAC-CzEsqYM3.png" alt="image1"></p><h3 id="automatic-evaluation" tabindex="-1"><a class="header-anchor" href="#automatic-evaluation"><span>Automatic evaluation</span></a></h3><p>Even though employing human judgement as measuring metric is the most effective approach, purely depending on human is expensive as well as slow in face of the growing size of data, which promoted the need for automation. In 2002, the most commonly used evaluation metric, Bilingual Evaluation Understudy (BLEU), was developed by Kishore et al. [2]. BLEU measures the difference between references and machine translation candidates through n-grams and brevity penalty. Based on the preliminary that the ‚Äúhighest correlation with monolingual human judgements‚Äù is four, n-grams measure the exact word segment correspondence of length one to four in the sentence pair. The brevity penalty is included to avoid short candidates receiving unreasonably high BLEU scores. BLEU remains popular till today due to its light-weightedness and fastness. A simple example [3] of word-level BLEU is demonstrated below. <img src="/blog/assets/bleu_exm-CCfZ9EJm.png" alt="image2"></p><h2 id="bertscore" tabindex="-1"><a class="header-anchor" href="#bertscore"><span>BERTScore</span></a></h2><p>Recent works on MT quality evaluation have provided stronger metrics and supports to the increased research interest in neural methods for training MT models and systems. BERTScore, which appeared in the 2020 International Conference on Learning Representations, aims to develop ‚Äúan automatic evaluation metric for text generation‚Äù [4]. As a high level summary, BERTScore is one step forward from the commonly used BLEU, because BERTScore incorporates the additional contextual information into consideration to calculate the degree of difference between source and target sentence.</p><h3 id="motivation" tabindex="-1"><a class="header-anchor" href="#motivation"><span>Motivation</span></a></h3><p>Generally speaking, there are two drawbacks in the n-gram-based metrics. Firstly, semantically-correct translations or paraphrases are excessively penalized for the n-gram metrics. In other words, different usage of words on the surface level will result in a low BLEU score. In the paper, the authors give the example of the source reference sentence ‚Äúpeople like foreign cars,‚Äù and two of the candidates are ‚Äúpeople like visiting places abroad‚Äù and ‚Äúconsumers prefer imported cars.‚Äù The latter uses synonyms to replace certain words in the reference, while preserving the original semantic meanings. However, n-gram-based metrics like BLEU will give higher score to the former candidate, even though the meaning is far from that of the reference sentence, since the exact string match of unigram and bigram values are higher. In face of this pitfall, the BERTScore authors are motivated to break the restrictions of n-grams, and to take advantage of contextualized token embedding as the matching metric, by calculating cosine similarities of all pairs in the reference and candidate.</p><p>Secondly, n-gram metrics cannot capture semantic dependencies of distant words or penalize semantically-critical word order changes. For example, for short sentences, BLEU is able to capture the swap of cause and effect clauses, like ‚ÄúA results in B.‚Äù However, when A and B are long phrases, even the longest four-gram will fail to capture the cause-effect semantic dependencies of A and B if their order change. The n-gram metrics measures the similarity in a shallow way, which motivates the authros to develop a metric that is more effective in tackling the distant dependencies and ordering problems.</p><h3 id="technique" tabindex="-1"><a class="header-anchor" href="#technique"><span>Technique</span></a></h3><p>The workflow of BERTScore computation is illustrated in the diagram below. Having a reference sentence x tokenized to (x1, ‚Ä¶, xk) and a candidate sentence xÃÇ tokenized to (xÃÇ1, ..., xÃÇk), the technique transforms the tokens into contextual imbeddings, and compute the match among all takens by cosine similarity. As an option, multiplying an additional weight based on the inverse document frequency of matching words can be helpful in some scenarios. The outcome includes a precision (R_BERT), recall (P_BERT), and combined metric scores(F1). <img src="/blog/assets/bert_tech-oz1d8NMw.png" alt="image3"></p><p>BERTScore uses the BERT model to generate contextual embeddings for each token. BERT tokenizes the input text into a sequence of word pieces, and splits the unknown words into commonly observed sequences of characters. The Transformer encoder computes the representation for each word piece by repeatedly applying self-attention and nonlinear transformation alternatively. The resulting contextual embedding from word piece will generate different vector representation for the same word piece in different contexts with regard to surrounding words, which is significantly different from the exact string match metric in BLEU.</p><p>Due to the vector representation of word embedding, BERTScore is able to perform a soft measure of similarity compared to exact-string matching in BLEU. The cosine similarity of a reference token xi and a candidate token xÃÇj is : <img src="/blog/assets/bert_e1-CmbZ26o-.png" alt="image4"></p><p>With similarity measurement of each pair of reference token and candidate token in preparation, we can move on to compute precision and recall. In the greedy match perspective, we match each token in x with the highest similarity score in xÃÇ.Recall is computed by matching each token in x to a token in xÃÇ, while precision is by matching each token in x to the corresponding token in xÃÇ. F1 score is calculated by combining precision and recall with the formular listed below. Extensive experiments indicate that F1 score performs reliably well across different settings, and therefore is the most recommended score to be used for evaluation. <img src="/blog/assets/bert_e2-BGOJhIVT.png" alt="image5"></p><p>Optionally, we can add an importance weighting to different words to optimize the metric, because previous works indicated that ‚Äúare words can be more indicative for sentence similarity than common words‚Äù [5]. From experiments, apply idf-based weight can render small benefits in some scenarios, but have limited contribution in other cases. The authors use the inverse document frequency (idf) scores to assign higher weights to rare words. Because there is limited preformance improvement when applying importance weighting, details about this optional stage will not be discussed further.</p><p>A simple example of BERTScore calculation without importance from the ref-cand cosine similarity matrix is illustrated below. Basically, R_BERT is calculated by the sum of maximum values in each row divided by the number of rows, and P_BERT is calculated by the sum of the maximum values in each column divided by the number of columns. F1 is computed by 2 times the product of R_BERT and P_BERT divided by their sum. The BERTScore with importance weighting can be computed by multiplying the corresponding weight to each cosine similarity. <img src="/blog/assets/bert_exm-BC1SngEY.png" alt="image6"></p><h3 id="effectiveness" tabindex="-1"><a class="header-anchor" href="#effectiveness"><span>Effectiveness</span></a></h3><p>For the evaluation of BERTScore, this blog will focus on the machine translation task in the original paper. The experiment‚Äôs main evaluation corpus is the WMT18 metric evaluation dataset, containing predictions of 149 translation systems across 14 language pairs, gold references, and two types of human judgment scores. The evaluation is completed with regard to both segment-level and system-level. The Segment-level human judgment score is for each reference-candidate pair, while the system-level human judgments score is based on all pairs in the test set.</p><p>Table below demonstrates the system-level correlation to human judgements. The higher the score is, the closer the system evaluation is to human evaluation. Focusing on FBERT score (F1 score), we can see a large number of bold correlations of metrics for FBERT, indicating it is the top performance system compared to the others. <img src="/blog/assets/bert_t1-BGeBv6KV.png" alt="image7"></p><p>Apart from system-level correlation, the table below illustrating the segment-level correlations, BERTScore shows a considerably higher performance compared to the others. The outperformance in segment-level correlations further exhibits the quality of BERTScore for sentence level evaluation. <img src="/blog/assets/bert_t4-CuGG9bHz.png" alt="image8"></p><h2 id="comet" tabindex="-1"><a class="header-anchor" href="#comet"><span>COMET</span></a></h2><p>In 2020, Rei et al. presented ‚Äúa neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements‚Äù at the 2020 Conference of Empirical Methods in Natural Language Processing [6]. The system, COMET, employs a different approach in improving evaluation metric. COMET builds an additional regression model to exploit information from source, hypothesis, and reference embeddings, and training the model to give a prediction on the quality of translation that highly correlates with human judgement.</p><h3 id="motivation-1" tabindex="-1"><a class="header-anchor" href="#motivation-1"><span>Motivation</span></a></h3><p>As the authors point out, ‚Äúthe MT research community still relies largely on outdated metrics and no new, widely-adopted standard has emerged‚Äù. This creates motivation for a metric scheme that uses a network model to actually learn and predict how well a machine translation will be in a human rating perspective. We knew that BLEU transformed MT quality evaluation from human rating to automated script, BERTScore improved the evaluation scheme by incoporating context, whereas COMET is motivated to learn how human will evaluate the quality of the translation, specifically scores from direct assessment (DA), human-mediated translation edit rate (HTER), and metrics compliant with multidimensional quality metric framework (MQM). After all, humans are the best to evaluate the translation quality of our own language. In short, COMET aims at closing the gap between automated metric with actual human evaluation.</p><h3 id="technique-1" tabindex="-1"><a class="header-anchor" href="#technique-1"><span>Technique</span></a></h3><p>The first step of COMET score computation is to encode the source, MT hypothesis, and reference sentence into token embeddings. The authors take advantage of a pretrained, cross-lingual encoder model, XLM_RoBERTa, to generate the three sequences (src, hyp, ref) into token embeddings. For each input sequence x = [x0, x1, ‚Ä¶, xn], the encoder will produce an embedding e_j(l) for each token xj and each layer l ‚àà {0, 1, ‚Ä¶, k}.</p><p>The word embeddings from the last layer of the encoders are fed into a pooling layer. Using a layer-wise attention mechanism, the information from the most important encoder layers are pooled into a single embedding for each token ej. Œº is a trainable weight coefficient, E_j = [e_j(0), e_j(1), ..., e_j(k)] corresponds to the vector of layer embeddings for token xj, and Œ± = softmax([Œ±(1), Œ±(2), . . . , Œ±(k)]) is a vector corresponding to the layer-wise trainable weights. <img src="/blog/assets/comet_t1-B81D_Tbi.png" alt="image9"></p><p>After applying an average pooling to the resulting word embeddings, a sentence embedding can be concatenated into a single vector from segments. The process is repeated three times for source, hypothesis, and reference sequences. Specifically, two models, the Estimator model and the Translation Ranking model, were developed for different usages.</p><p>For the Estimator model, a single vector x is computed from the three sentence embeddings s, h, and r specified below: <img src="/blog/assets/comet_t2-2z_wTVM3.png" alt="image10"></p><p>Where h‚äôs and h‚äôr denotes the element-wise source product and reference product, and |h-s| and |h-r| denotes the absolute element-wise source difference and reference difference. The combined feature x serves as input to a feed-forward regression network. The network is trained to minimize the mean squared error loss between its predicted scores and human quality assessment scores (DA, HTER or MQM).</p><p>The Translation Ranking model, on the other hand, has different inputs {s,h+,h-,r}, i.e. a source, a higher-ranked hypothesis h+, a lower-ranked hypothesis h-, and a reference. After transforming them into sentence embeddings <strong>{s,h+,h-,r}</strong>, the triplet margin loss in relation to the source and reference is calculated: <img src="/blog/assets/comet_t3-CtYM802Z.png" alt="image11"></p><p>d(u, v) denotes the euclidean distance between u and v and Œµ is a margin. In this way during training, the model will optimize the embedding space so the distance between the anchors (s and r) and the ‚Äúworse‚Äù hypothesis h‚àí is larger by at least Œµ than the distance between the anchors and ‚Äúbetter‚Äù hypothesis.</p><p>In the inference stage, the model will receive a triplet input (s,ƒ•,r) with only one hypothesis, and the quality score will be the harmonic mean between the distance to the source d(s,ƒ•) and that to the reference d(r,ƒ•), and normalized it to a 0 to 1 range: <img src="/blog/assets/comet_t4-B6FdAoXe.png" alt="image12"><img src="/blog/assets/comet_t5-C8CCEehw.png" alt="image13"></p><p>In short, the Translation Ranking model is trained to minimize the distance between a ‚Äúbetter‚Äù hypothesis and both its corresponding reference and its original source. <img src="/blog/assets/comet_t6-DuyofFQ2.png" alt="image14"></p><h3 id="effectiveness-1" tabindex="-1"><a class="header-anchor" href="#effectiveness-1"><span>Effectiveness</span></a></h3><p>To test the effectiveness of COMET, the authors trained 3 MT translations models that target different types of human judgment (DA, HTER, and MQM) from the corresponding datasets: the QT21 corpus, the WMT DARR corpus, and the MQM corpus. Two Estimator models and one Translation Ranking model are trained. One regressed on HTER (COMET-HTER) is trained with the QT21 corpus, and another model regressed on MQM (COMET-MQM) is trained with the MQM corpus. COMET-RANK is trained with the WMT DARR corpus. The evaluation method employed is the official Kendall‚Äôs Tau-like formulation. Concordant is the number of times the metric gives a higher score to the defined &quot;better&quot; hypothesis, while Discordant is the number of times the metrics give a higher score to the &quot;worse&quot; hypothesis or the scores is the same for the two hypothesis. <img src="/blog/assets/comet_e1-_HxpHJYb.png" alt="image15"></p><p>As shown in table the first table below, for as much as seven in eight language pair evaluation with English as source, COMET-RANK outperforms all other evaluation systems, including BLEU, two encoder models of BERTScore, and its two Estimator models, to a large extent. Similarly, for the language pair evaluation with English as target in the second table below, COMET also exceeds the other metrics in the overall performance, including the 2019 task winning metric YISI-1. <img src="/blog/assets/comet_e2-C-HBhvyX.png" alt="image16"><img src="/blog/assets/comet_e3-B81cF4q6.png" alt="image17"></p><h2 id="case-study" tabindex="-1"><a class="header-anchor" href="#case-study"><span>Case Study</span></a></h2><p>In order to measure how well BLEU, BERTScore, and COMET can evaluate on existing MT systems, I managed to find a dataset with human judgment scores (e.g DA) [7]. Unfortunately, the MT systems that have the DA score is not available to the public, e.g. I cannot access the Baidu-system.6940 with the highest DA score in WMT19. With this preliminary, the experiment to compare how our evaluation metrics scores with a human judgement score (e.g. DA) is unattainable. Another simpler case study for the metrics is initialized instead.</p><p>For the setup, a group of 10 source-reference sentence pairs were prepared from a Chinese-English parallel Yiyan corpus [8]. The source Chinese sentences are fed to two common NMT systems: Google translate which uses Google Neural Machine Translation (GNMT) [9] and SYSTRAN translate [10], and the output of translation is stored as their corresponding hypthesis.</p><p>For BERTScore, we use the encoder from roberta without the importance weighting, and F1 score to evaluate the translated hypothesis. For COMET, we use the reference-free Estimation model ‚Äúwmt20-comet-qe-da‚Äù, trained based on DA and used Quality Estimation (QE) as a metric, for the evaluation on GNMT and SYSTRAN. The scores from BLEU, BERTScore, and COMET are illustrated in the table below. With the limited 10 data samples, BERTScore and COMET consider Google Translator performing better, while the BLEU score for SYSTRAN Translator is higher.</p><table><thead><tr><th>System-level score</th><th>Google</th><th>SYSTRAN</th></tr></thead><tbody><tr><td>BLEU</td><td>33.96</td><td>37.60</td></tr><tr><td>BERTScore F1</td><td>0.7934</td><td>0.7562</td></tr><tr><td>COMET</td><td>0.7215</td><td>0.6418</td></tr></tbody></table><p>The limitation of BLEU as compared to BERTScore and COMET is mostly exposed in the second sentence, as illustrated in the table below. The BLEU score for Google is 19.29, while that of SYSTRAN is 44.96. Though there is no DA scores from experts, the meanings of the two hypothesis and the reference are very similar, and the difference mostly lies on the different choice of same-meaning words. The n-gram&#39;s measurement based on the exact string match causes the large difference in the evaluation result. In comparison, the context-based BERTScore and human-judgement-trained COMET do not have a significant difference in their scores, and this example suggests the outdatedness of n-gram-based metrics to some extent.</p><table><thead><tr><th>Type</th><th>Sentence</th></tr></thead><tbody><tr><td>Src</td><td>Êàë‰ª¨Âú®ÁΩëÁªúÊêúÁ¥¢ÂíåÂπøÂëäÁöÑÂàõÊñ∞ÔºåÂ∑≤‰ΩøÊàë‰ª¨ÁöÑÁΩëÁ´ôÊàê‰∏∫ÂÖ®‰∏ñÁïåÁöÑÈ°∂Á∫ßÁΩëÁ´ôÔºå‰ΩøÊàë‰ª¨ÁöÑÂìÅÁâåÊàêÂÖ®‰∏ñÁïåÊúÄËé∑ËÆ§ÂèØÁöÑÂìÅÁâå„ÄÇ</td></tr><tr><td>Ref</td><td>Our innovations in web search and advertising have made our web site a top internet property and our brand one of the most recognized in the world.</td></tr><tr><td>Hyp_Google</td><td>Our innovation in online search and advertising has made our website a top website in the world, and our brand has become the most recognized brand in the world.</td></tr><tr><td>Hyp_SYSTRAN</td><td>Our innovations in online search and advertising have made our website the world&#39;s top website and made our brand the most recognized in the world.</td></tr></tbody></table><table><thead><tr><th>Segment-level score for 2nd sentence</th><th>Google</th><th>SYSTRAN</th></tr></thead><tbody><tr><td>BLEU</td><td>19.29</td><td>44.96</td></tr><tr><td>BERTScore F1</td><td>0.7515</td><td>0.7820</td></tr><tr><td>COMET</td><td>0.7399</td><td>0.7396</td></tr></tbody></table><p>Let‚Äôs take a closer look at the 8th sentence shown below. Because the SYSTRAN&#39;s translation exactly matched the reference sentence, BLEU for this sentence is 100. In BERTScore, SYSTRAN also receives a score 0.2 higher than GNMT, because the former&#39;s translation matched more with the reference. However, we can clearly see that the result from Google Translate matches more with the source sentence in Chinese, especially the choice of word of ‚Äúregistered‚Äù instead of ‚Äúincorporated‚Äù for &quot;Ê≥®ÂÜå&quot;, and ‚ÄúDelaware, USA‚Äù instead of ‚ÄúDelaware‚Äù for &quot;ÁæéÂõΩÁâπÊãâÂçéÂ∑û&quot;. The COMET score for this sentence is 0.5144 for GNMT versus 0.3090 for SYSTRAN, which correlates more with human judgement. This is because COMET does not take the reference sentences but the source sentences in Chinese as input. COMET aims to mimic how human judgement (DA under this experimental setup) evaluates the translation, and clearly the Google translation provides a more exact translation from source. This example can be used to illustrate the limitation of metrics that purely depend on the reference sentence.</p><table><thead><tr><th>Type</th><th>Sentence</th></tr></thead><tbody><tr><td>Src</td><td>Êàë‰ª¨‰∫é1998Âπ¥9ÊúàÂú®Âä†Âà©Á¶èÂ∞º‰∫öÂ∑ûÊ≥®ÂÜåÊàêÁ´ã 2003Âπ¥8ÊúàÂú®ÁæéÂõΩÁâπÊãâÂçéÂ∑ûÈáçÊñ∞Ê≥®ÂÜå„ÄÇ</td></tr><tr><td>Ref</td><td>We were incorporated in California in September 1998 and reincorporated in Delaware in August 2003.</td></tr><tr><td>Hyp_Google</td><td>We were registered in California in September 1998 and re-registered in Delaware, USA in August 2003.</td></tr><tr><td>Hype_SYSTRAN</td><td>We were incorporated in California in September 1998 and reincorporated in Delaware in August 2003.</td></tr></tbody></table><table><thead><tr><th>Segment-level score for 8th sentence</th><th>Google</th><th>SYSTRAN</th></tr></thead><tbody><tr><td>BLEU</td><td>37.06</td><td>100</td></tr><tr><td>BERTScore F1</td><td>0.7948</td><td>1.0000</td></tr><tr><td>COMET</td><td>0.5144</td><td>0.3090</td></tr></tbody></table><p>Not a trained translator myself, I cannot give my personal judgements on GNMT and SYSTRAN, but through the two examples, we clearly see the limitation of BLEU, and the limitation of BERTScore to some extent. However, it is still debatable if reference sentences should be evaluated in the metric. For COMET, inferring human judgement directly from source is appealing, but free-of-reference may result in loss of information in certain perspectives. Considering the experimental results has proven its effectiveness compared to BLEU and BERTScore, COMET may have pointed another direction for future MT evaluation metrics.</p><h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="#conclusion"><span>Conclusion</span></a></h2><p>To sum up, two more advanced MT metric, BERTScore and COMET, are introduced. BERTScore enriches the information used in evaluation by incorporating contextual embedding to compute the degree of difference, and COMET employs an additional regression model to exploit information to make prediction score that correlates with human judgement. Walking through the history of MT metrics, we start from the most labor-intensive human evaluation, move a step further to automated n-gram-based metrics like BLEU, develop further on taking contextual information into consideration in BERTScore, and finally arrive at training models to evaluate like human in COMET. The development is exciting, but it is also worth noted that comparing to the recent dramatic improvement in MT quality, MT evaluation has fallen behind. In 2019, the WMT News Translation shared Task has 153 submissions, while the Metrics Shared Task only has 24 submissions [6]. The importance of MT evaluation should be the same as the MT techniques. With more advanced evaluation metrics to support and give feedbacks to new MT systems, the future development of MT realm as a whole can prosper.</p><h2 id="code" tabindex="-1"><a class="header-anchor" href="#code"><span>Code</span></a></h2><p>BERTScore: <a href="https://github.com/Tiiiger/bert_score" target="_blank" rel="noopener noreferrer">https://github.com/Tiiiger/bert_score</a> COMET: <a href="https://github.com/Unbabel/COMET" target="_blank" rel="noopener noreferrer">https://github.com/Unbabel/COMET</a></p><h2 id="reference" tabindex="-1"><a class="header-anchor" href="#reference"><span>Reference</span></a></h2><p>[1] ALPAC (1966) Languages and machines: computers in translation and linguistics. A report by the Automatic Language Processing Advisory Committee, Division of Behavioral Sciences, National Academy of Sciences, National Research Council. Washington, D.C.: National Academy of Sciences, National Research Council, 1966. (Publication 1416.)</p><p>[2] Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. Bleu: a method for automatic eval- uation of machine translation. ACL 2002.</p><p>[3] Lei Li. &quot;Data, Vocabulary and Evaluation,&quot; page 35-36. 2021. <a href="https://sites.cs.ucsb.edu/~lilei/course/dl4mt21fa/lecture2evaluation.pdf" target="_blank" rel="noopener noreferrer">https://sites.cs.ucsb.edu/~lilei/course/dl4mt21fa/lecture2evaluation.pdf</a></p><p>[4] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. ‚ÄúBERTScore: Evaluating Text Generation with BERT,‚Äù ICLR 2020. <a href="https://openreview.net/forum?id=SkeHuCVFDr" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=SkeHuCVFDr</a>.</p><p>[5] Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. In IEEvaluation@ACL, 2005.</p><p>[6] Rei, Ricardo, Craig Stewart, Ana C Farinha, and Alon Lavie. ‚ÄúCOMET: A Neural Framework for MT Evaluation.‚Äù EMNLP, 2020. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.213" target="_blank" rel="noopener noreferrer">https://doi.org/10.18653/v1/2020.emnlp-main.213</a>.</p><p>[7] marvinKaster, &quot;global-explainability-metrics,&quot; 2021. <a href="https://github.com/SteffenEger/global-explainability-metrics/blob/main/WMT19/DA-syslevel.csv" target="_blank" rel="noopener noreferrer">https://github.com/SteffenEger/global-explainability-metrics/blob/main/WMT19/DA-syslevel.csv</a></p><p>[8] Corpus Research Group, Beijing Foreign Studies University Foreign Language. &quot;Yiyan English-Chinese Parallel Corpus,&quot; 2020. <a href="http://corpus.bfsu.edu.cn/info/1082/1693.htm" target="_blank" rel="noopener noreferrer">http://corpus.bfsu.edu.cn/info/1082/1693.htm</a></p><p>[9] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google‚Äôs neural machine translation system: Bridging the gap between human and machine translation.</p><p>[10] Guillaume Klein, Dakun Zhang, Clement Chouteau, Josep M Crego, and Jean Senellart. 2020. Efficient and high-quality neural machine translation with opennmt. In Proceedings of the Fourth Workshop on Neural Generation and Translation.</p></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="vp-link vp-external-link-icon vp-meta-label" href="https://github.com/lileicc/blog/edit/main/dl4mt/2021/mtmetric/README.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page<!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">Last update: </span><!----></div><div class="contributors"><span class="vp-meta-label">Contributors: </span><!--[--><!--[--><span class="vp-meta-info" title="email: lileicc@gmail.com">Lei Li</span><!--]--><!--]--></div></div></footer><!----><div id="vp-comment" class="giscus-wrapper input-top" style="display:block;"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" preserveAspectRatio="xMidYMid" viewBox="0 0 100 100"><circle cx="28" cy="75" r="11" fill="currentColor"><animate attributeName="fill-opacity" begin="0s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></circle><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 47a28 28 0 0 1 28 28"><animate attributeName="stroke-opacity" begin="0.1s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 25a50 50 0 0 1 50 50"><animate attributeName="stroke-opacity" begin="0.2s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path></svg></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">Li Lab</div><div class="vp-copyright">Copyright ¬© 2024 Huake He </div></footer></div><!--]--><!--[--><!----><!----><!--]--><!--]--></div>
    <script type="module" src="/blog/assets/app-DiCjo-Va.js" defer></script>
  </body>
</html>
