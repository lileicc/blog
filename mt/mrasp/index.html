<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.11" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.43" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://lileicc.github.io/blog/mt/mrasp/"><meta property="og:site_name" content="Li-Lab Blog"><meta property="og:title" content="What is proper Pre-training for Multilingual Machine Translation?"><meta property="og:description" content="​ In 1920, the great philosopher Bertrand Russell visited China, accompanied by Yuen Ren Chao, a Chinese-American linguist. Mr. Chao was a naturally gifted polyglot. At that tim..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2024-05-25T23:06:05.000Z"><meta property="article:author" content="Xiao Pan"><meta property="article:tag" content="Multilingual MT"><meta property="article:tag" content="Pre-training"><meta property="article:tag" content="Random Aligned Substitution"><meta property="article:tag" content="Zero-shot Translation"><meta property="article:tag" content="mRASP"><meta property="article:published_time" content="2020-12-31T00:00:00.000Z"><meta property="article:modified_time" content="2024-05-25T23:06:05.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"What is proper Pre-training for Multilingual Machine Translation?","image":[""],"datePublished":"2020-12-31T00:00:00.000Z","dateModified":"2024-05-25T23:06:05.000Z","author":[{"@type":"Person","name":"Xiao Pan"}]}</script><title>What is proper Pre-training for Multilingual Machine Translation? | Li-Lab Blog</title><meta name="description" content="​ In 1920, the great philosopher Bertrand Russell visited China, accompanied by Yuen Ren Chao, a Chinese-American linguist. Mr. Chao was a naturally gifted polyglot. At that tim...">
    <link rel="preload" href="/blog/assets/style-BEIjLUpX.css" as="style"><link rel="stylesheet" href="/blog/assets/style-BEIjLUpX.css">
    <link rel="modulepreload" href="/blog/assets/app-DvxBpnKI.js"><link rel="modulepreload" href="/blog/assets/index.html-BU0zenT9.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/blog/assets/index.html-BroJAVs1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DqRRrdxN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-geg8hdaA.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-T38F122l.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DRY_Rg1V.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-yeU1vgED.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DWy6xs1J.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-pCrE8yHf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BJ_AghC5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-k3b_T-SP.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5PV3NlK2.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CbDJmTDz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rqHrn9XR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DXKear5F.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DDGXC_ws.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DQsqSk5b.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nXBkKbPa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ZWqMQjDf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BmkCcctU.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CO-pLJt9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DpYleMCC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Dc1M0ATi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CauSBX8J.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-Cw-U9o5J.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B_ynrIRb.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-QtilJd5B.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BNCIUQPO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DdBHs2Us.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DOSYHKuw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-qalJTdOm.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Chsz9oWa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ChJmO1W9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B4SXWPkk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Brw2XJ0u.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DkfNdZ8V.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CSREm_rq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-8X7fVq8Q.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-lG0YXER3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Dx4J44pa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-T9BHFuQN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Y-i8XmH5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DK7Fp7xG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CRTjq6AW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CoiG0KoW.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DRJCQVaX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-TJWQZwkM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-_9uD0-ss.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Bo--ZP-D.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-9Zna8f0T.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DF9aEpdE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DVXDgIxX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-FdwJo6wG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-tsChVsQB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DLTaczOx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-COn1v7J7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cFqiIE9S.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DkU8CzUY.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-AfrCKcc-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BiQLQhAV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BPrRjTfB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Y_uyyvMq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-JDfCOuVV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CCFENeZX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DYUMSDb3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CPfa70U7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DQUwiUh-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-GAJuCCKc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B3hZ6uy3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CqQ0-pfF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7yOzg9-j.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-v8rZf8KZ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ukHX_v8t.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BFYa-2at.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BOt7aLMO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DJKc1PBD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BPUrixrx.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Dnwk3GJG.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CxLTGcig.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DyAaHm9G.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-D5yXusUE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-x5aSH1Ah.js" as="script"><link rel="prefetch" href="/blog/assets/giscus--_FS5kYt.js" as="script"><link rel="prefetch" href="/blog/assets/auto-CAdRPfCH.js" as="script"><link rel="prefetch" href="/blog/assets/index-wZ-hXvzw.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-SzV8tJDW.js" as="script"><link rel="prefetch" href="/blog/assets/SearchResult-Rjd_xNPH.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container no-sidebar external-link-icon has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/blog/"><img class="vp-nav-logo" src="/blog/logo.svg" alt><!----><span class="vp-site-name hide-in-pad">Li-Lab Blog</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/" aria-label="Blog Home"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="home" width="1em" height="1em"></iconify-icon>Blog Home<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/category/" aria-label="Category"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="baseline-category" width="1em" height="1em"></iconify-icon>Category<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/tag/" aria-label="Tags"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="tag" width="1em" height="1em"></iconify-icon>Tags<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/timeline/" aria-label="Timeline"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="time" width="1em" height="1em"></iconify-icon>Timeline<!----></a></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/lileicc/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon" name="outlook"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="vp-outlook-dropdown"><!----></div></button></div><!--[--><button type="button" class="search-pro-button" aria-label="Search"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon" name="search"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">Search</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!----><ul class="vp-sidebar-links"></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->What is proper Pre-training for Multilingual Machine Translation?</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Xiao Pan</span></span><span property="author" content="Xiao Pan"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2020-12-31T00:00:00.000Z"></span><span class="page-category-info" aria-label="Category🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color0 clickable" role="navigation">MT</span><!--]--><meta property="articleSection" content="MT"></span><span class="page-tag-info" aria-label="Tag🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color8 clickable" role="navigation">Multilingual MT</span><span class="page-tag-item color1 clickable" role="navigation">Pre-training</span><span class="page-tag-item color7 clickable" role="navigation">Random Aligned Substitution</span><span class="page-tag-item color8 clickable" role="navigation">Zero-shot Translation</span><span class="page-tag-item color3 clickable" role="navigation">mRASP</span><!--]--><meta property="keywords" content="Multilingual MT,Pre-training,Random Aligned Substitution,Zero-shot Translation,mRASP"></span><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 13 min</span><meta property="timeRequired" content="PT13M"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!----><div class="vp-toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#introduction">Introduction</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#challenges-in-machine-translation-pre-training">Challenges in machine translation pre-training</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#motivation-and-techniques-of-mrasp">Motivation and Techniques of mRASP</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#effectiveness-of-mrasp">Effectiveness of mRASP</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-en-de-and-en-fr-benchmarks">1. En-De and En-Fr Benchmarks</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-extend-to-language-not-seen-during-the-pre-training-phase">2. Extend to language not seen during the pre-training phase</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-case-study">3. Case study</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#findings-from-mrasp-trained-model">Findings from mRASP trained model</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#summary">Summary</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#references">References</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content"><p>​ In 1920, the great philosopher Bertrand Russell visited China, accompanied by Yuen Ren Chao, a Chinese-American linguist. Mr. Chao was a naturally gifted polyglot. At that time, he could already speak Baoding dialect, Wu dialect, Fuzhou dialect, Nanjing dialect, and English. He accompanied Russell from Shanghai to Changsha by ship. During the trip, he was learning Changsha dialect from Yang Ruiliu, an economist on the same ship. When the ship docked in Changsha, Yuen Ren Chao was already able to translate Russell&#39;s speeches and slang into Changsha dialect. Can our neural network become a model like &quot;Yuen Ren Chao&quot; on machine translation? That is, to create a unified model with multilingual abilities, and when encountering new languages, the model could quickly adapt to translating new ones after training with a small amount of data.</p><p>Paper：<a href="https://arxiv.org/abs/2010.03142" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2010.03142</a></p><p>Github: <a href="https://github.com/linzehui/mRASP" target="_blank" rel="noopener noreferrer">https://github.com/linzehui/mRASP</a></p><h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction"><span>Introduction</span></a></h2><p><img src="/blog/assets/zhao-Co2Ywh84.png" alt="image1"></p><p>A recent work mRASP, which appeared at the 2020 Conference of Empirical Methods in Natural Language Processing, aims to provide a Yuen Ren Chao polyglot model for machine translation [1]. The key idea is to pre-train a model with multilingual capability, and yield any specific translation model by fine-tuning on the corresponding parallel corpus. The model trained with mRASP technique in 32 languages has achieved a comprehensive and significant improvement in 47 translation test sets.</p><p>Unlike previous translation models, mRASP has established a successful paradigm of pre-training and fine-tuning for machine translation. This is similar to BERT&#39;s role on NLU tasks. There were already pretrained models for natural language generation (GPT). However, they are limited in extending their capabilities on multilingual machine translation tasks. The central problem that mRASP wants to solve is, can we develop a unified pre-trained translation model and extend it by fine-tuning on a small amount of parallel corpus on any specific language pair to obtain any-language translation model?</p><p>mRASP is designed for machine translation tasks. It has three advantages. First, the translation quality can be consistently improved regardless of the amount of parallel bilingual corpus. In rich-resource directions, such as the standard English-French wmt2014 translation task, which already has 40 million parallel sentence pairs for training, mRASP can still significantly improve the quality, reaching a BLEU score of 44.3. In low-resource directions, mRASP performs surprisingly well. In extreme cases, when we have only 10,000 training data for fine-tuning, a reasonable translation model can be obtained through 10-minute fine-tuning. Second, It breaks the limit on languages, for any direction, the mRASP can be directly used to fine-tune to get a single-directional translation model. Finally, it is resource-efficient. Some other pre-training paradigms are trained on hundreds of GPUs for a couple of weeks. By contrast, mRASP only needs 8 GPUs for a week. In short, mRASP can be understood as a lightweight BERT in the field of machine translation. When you need a machine translation model, you should try it, it may surprise you! The authors also said that this technology has been used on the Volctrans system developed by ByteDance and has been tested in actual business practice. The authors have kindly published the research data, codes and pre-trained models.</p><p>Next, we will introduce and analyze mRASP from three aspects: 1) the challenges of machine translation pre-training; 2) the motivation and methods of mRASP; 3) the performance and analysis of mRASP.</p><h2 id="challenges-in-machine-translation-pre-training" tabindex="-1"><a class="header-anchor" href="#challenges-in-machine-translation-pre-training"><span>Challenges in machine translation pre-training</span></a></h2><p>At present, the vast majority of AI tasks are basically statistical learning based on data, and the performance of the model depends on the quality and quantity of data to a large extent. It has become a new successful paradigm for NLP to use a large amount of cheap data to pre-train the model, then fine-tune with a small amount of annotation data in specific scenarios. For example, pre-trained on large-scale unlabeled text, BERT[2] can achieve good results on 11 NLU tasks after fine-tuning on limited annotation data. However, in multilingual machine translation, the paradigm of pre-training and fine-tuning has not yet achieved general success. The training objectives of previous NLP pre-training methods such as BERT and GPT[5] have a large gap with machine translation, thus are not easy to use directly. mRASP proposed a new idea: it uses massive bilingual parallel corpus accumulated in multiple languages to jointly train a unified model, and then fine-tune based on it. Therefore the pre-training and fine-tuning objectives are as close as possible, so as to give greater play to the role of the pre-training model.</p><p><img src="/blog/assets/bert-gpt-DoVqmBf3.png" alt="image2"></p><p><img src="/blog/assets/mass-CzIgcnp9.png" alt="image3"></p><p>The above figure compares and analyzes the limitations of the previous NLP pre-training paradigms in machine translation scenarios. BERT and GPT respectively correspond to the pre-training of the Transformer[6] encoder part and the decoder part, while machine translation uses the whole sequence-to-sequence model. Only part of the parameters of the translation model are initialized due the inconsistency in model structure. Therefore it will be difficult to effectively play the role of pre-training. As a result, it requires a lot of special skills to be improved [10].</p><p>Researchers soon proposed frameworks such as MASS [7] and BART [8] to extend pre-training to sequence-to-sequence tasks. They use auto-encoder for self-learning and have achieved significant results in many downstream NLG tasks. However, there are still two important problems when applying them in machine translation: 1) They brings no improvement in rich-resource languages (such as English, German, English and French). 2) There is no way to extend to multilingual translation tasks. This limitation is largely due to the fact that autocoding is a relatively simple task so it is difficult to learn a deeper representation. By contrast, machine translation requires a more complex semantic transformation. The training objective discrepancy between pre-training and fine-tuning makes it difficult for the model to make the best use of training data. It has become an important challenge to overcome the two problems for the application of pre-training paradigms in the field of machine translation.</p><h2 id="motivation-and-techniques-of-mrasp" tabindex="-1"><a class="header-anchor" href="#motivation-and-techniques-of-mrasp"><span>Motivation and Techniques of mRASP</span></a></h2><p>​For language learners, a very interesting phenomenon is that after learning three or four languages, the speed of learning a new language will accelerate. For example, if an English native speaker learns German and French separately, he/she may take one year each. However, if he learns German first and then learns French, he/she may only take one year and three months to learn it. If he/she learns Spanish subsequently, the speed may be faster [3]. The same is true for learning programming languages. Learning C ++ may take one year. Learning Java, Python subsequently may only take one month. A simple explanation is that in the process of multilingual learning, human beings will spontaneously summarize the abstract commonalities among languages and focus on learning the characteristics of new languages. Therefore, in order to improve personal language learning ability, it is often necessary to learn more languages, to have a more accurate grasp of language commonalities, instead of desperately learning one language. By the same token, for machine translation, it has become a very interesting question whether the translation ability can be transferred to different languages so that the information between different languages can be utilized for each other.</p><p><img src="/blog/assets/language-learn-B4oUFU97.png" alt="image4"></p><p><img src="/blog/assets/language-learn-2-C_Sn2vtW.png" alt="image5"></p><p>The design goal of mRASP is based on such considerations: design a general pre-trained model to learn the commonalities of transformation between languages, and then it will be easier to migrate to the new translation direction. Just like language learners, after learning two languages, the third language becomes easier. The design of mRASP follows two basic principles: first, the training objective of pre-training is the same as machine translation, and it is necessary to learn the transformation ability between languages; second, learn the universal representation of the language as much as possible, if the semantics of cross-lingual sentences or words are close, the representation should also be close.</p><p>mRASP follows a common pre-training-fine-tuning framework. In the pre-training stage, unlike the traditional pre-training model in which massive unsupervised monolingual data are used, mRASP takes a different approach: it puts multilingual parallel data into the same model for joint training. The Transformer architecture is adopted, plus a language identifier (Language token) to identify the source language and the target language. In order to ensure that sentences and words in different languages could be embedded in the neighbor space, sentences with the same meaning, random alignment substitution (RAS) is introduced to create a richer context.</p><p>There is a certain probability that &quot;爱&quot;(Chinese) in a Chinese sentence &quot;我 爱 北京 天安门&quot; will be replaced by &quot;aime&quot; (French), and &quot;北京&quot;(Chinese) will also be replaced by &quot;Pékin&quot; (French), so the original sentence becomes &quot;I aime Pékin Tiananmen.&quot; A pair of parallel sentence pairs in the training set can be expanded into two pairs (even three pairs, four pairs,......)</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>我 爱 北京 天安门 ==&gt; I love Beijing Tiananmen Square
我 aime Pékin 天安门 ==&gt; I love Beijing Tiananmen Square
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>For the model, by learning from abundant parallel corpus, it will naturally learn the correspondence between synonyms across different languages according to this &quot;artificially created context&quot;. In fact, RAS based on parallel dictionaries has bridge the representation gap of synonyms across different languages. In the above example, the word vector expectations calculated by &quot;爱&quot;(&#39;love&#39; in Chinese) and &quot;aime&quot;(&#39;love&#39; in French) are as close as possible.</p><p><img src="/blog/assets/uni-representation-D93iM5Cr.png" alt="image6"></p><p>In the fine-tuning stage, we initialize the parameters with mRASP, and then we adopt the same training method as the traditional single-directional machine translation. Therefore, using mRASP does not require any additional skills. For a detailed introduction, please refer to the paper[1].</p><h2 id="effectiveness-of-mrasp" tabindex="-1"><a class="header-anchor" href="#effectiveness-of-mrasp"><span>Effectiveness of mRASP</span></a></h2><p>mRASP uses parallel corpus of 32 languages to-and-from English for pre-training. When we only use parallel corpus of English to French wmt14 for fine-tuning, it achieves 44.3 BLEU without laborious Back Translation of massive monolingual corpus. Moreover, when applied to a new translation direction Dutch (Nl) to Portuguese (Pt), with only 12,000 parallel sentence pairs for fine-tuning, mRASP gets a reasonable (BLEU 10 +) model in ten minutes. By contrast, training any usable MT model from scratch using the equivalent parallel sentence pairs is impossible (BLEU is close to 0).</p><p>In summary, mRASP has the following advantages:</p><ol><li>Easy to reproduce</li></ol><p>The pre-training of mRASP only requires a total of 110 million parallel sentence pairs (the same pair of parallel sentence is applicable to both directions, resulting in a total of 220 million training samples), and the vocabulary only has 64k bpe subword tokens. Compared with other pre-training methods, in which tens of billions of data and dozens of layers are frequently used, the training process is less difficult. We can complete the pre-training process on 32 languages in less than a week using 8 GPUs. By the way, support for more languages can also be simply expanded.</p><ol start="2"><li>Highly Versatile</li></ol><p>Compared with the single-directional machine translation models, mRASP brings a consistent improvement in rich, medium and low-resource scenarios. Even for English to French direction where we have the largest parallel corpus, the translation quality is further improved by 1.1 BLEU. More surprisingly, for Dutch to Portuguese direction that have never been seen in the pre-training data, a significant improvement of 10 + BLEU has also been achieved.</p><p>Here are some representative experimental results:</p><h3 id="_1-en-de-and-en-fr-benchmarks" tabindex="-1"><a class="header-anchor" href="#_1-en-de-and-en-fr-benchmarks"><span>1. En-De and En-Fr Benchmarks</span></a></h3><p>The following figure compares the effect of mRASP on En-De and En-Fr with several concurrent cross-lingual pre-training models. It can be seen that mRASP has certain advantages: it reaches 30.3 (tokenized BLEU) on En-&gt;De wmt 2016 test set, 44.3 (tokenized BLEU) on En-&gt;Fr wmt 2014 test set. CTNMT uses BERT pre-training. MASS introduces large-scale monolingual data. mBERT is a multilingual BERT model. mBART is another pre-training method that is proposed concurrently, it uses massive multilingual monolingual data, and is trained on 256 GPUs for 20 days.</p><p><img src="/blog/assets/en-de-CFq-tNhy.png" alt="image7"></p><p><img src="/blog/assets/en-fr-DqzPo3al.png" alt="image8"></p><h3 id="_2-extend-to-language-not-seen-during-the-pre-training-phase" tabindex="-1"><a class="header-anchor" href="#_2-extend-to-language-not-seen-during-the-pre-training-phase"><span>2. Extend to language not seen during the pre-training phase</span></a></h3><p>Directions that are not included in parallel pairs during the pre-training stage, are also referred as &quot;Exotic Directions&quot;. Whether mRASP is effective on Exotic Directions, determines whether mRASP has good generalization capabilities.</p><p>The Exotic Directions are divided into four situations in the paper:</p><ul><li>Exotic Pair: Both the source language and the target language have been individually pre-trained, but the model has not yet seen the bilingual pairs of them</li><li>Exotic Source: The model has only seen the target language in the pre-training stage, and the source language has not been seen at all</li><li>Exotic Target: The model has only seen the source language in the pre-training stage, and the target language has not been seen at all</li><li>Exotic Full: The model has not seen the source language or the target language at all in the pre-training stage</li></ul><p>It is difficult to train machine translation under the circumstances. Of course, the most difficult one is the last one, which is equivalent to requiring people who have never learned Latin and Hindi to read a few sentences in Latin and Hindi then translate between them.</p><table><thead><tr><th style="text-align:center;">Category</th><th style="text-align:center;">Source language seen during pre-training?</th><th style="text-align:center;">Target language seen during pre-training?</th><th style="text-align:center;">Language pair seen during pre-training?</th></tr></thead><tbody><tr><td style="text-align:center;">Exotic Pair</td><td style="text-align:center;">✔</td><td style="text-align:center;">✔</td><td style="text-align:center;">X</td></tr><tr><td style="text-align:center;">Exotic Source</td><td style="text-align:center;">X</td><td style="text-align:center;">✔</td><td style="text-align:center;">X</td></tr><tr><td style="text-align:center;">Exotic Target</td><td style="text-align:center;">✔</td><td style="text-align:center;">X</td><td style="text-align:center;">X</td></tr><tr><td style="text-align:center;">Exotic Full</td><td style="text-align:center;">X</td><td style="text-align:center;">X</td><td style="text-align:center;">X</td></tr></tbody></table><p>It is worth noting that both sides of Fr-Zh have appeared separately, but they have not appeared as parallel pairs. mRASP achieves a 20 + BLEU score after fine-tuning on 20K parallel corpus.</p><p>For Exotic Full scenario, such as Dutch to Portuguese (Nl-Pt), only 12,000 parallel corpora are used, and after about 10 minutes of training, you can achieve a 10+ BLEU score.</p><p><img src="/blog/assets/exotic-DbcNV_Kk.png" alt="image9"></p><h3 id="_3-case-study" tabindex="-1"><a class="header-anchor" href="#_3-case-study"><span>3. Case study</span></a></h3><p>In order to understand the effect of mRASP more intuitively, the authors also make a case study in the paper.</p><h4 id="french-chinese-fr-zh" tabindex="-1"><a class="header-anchor" href="#french-chinese-fr-zh"><span>French-Chinese(Fr-Zh)</span></a></h4><ul><li>Exotic Pair, 20k Parallel Sentence Pair</li><li>Direct (0.7 BLEU) is much weaker than mRASP (25.8 BLEU)</li></ul><p>The Direct system does not work at all, while the mRASP system translates well.</p><table><thead><tr><th style="text-align:center;"></th><th style="text-align:left;">Original Text</th><th style="text-align:left;">Translation in English</th></tr></thead><tbody><tr><td style="text-align:center;">source</td><td style="text-align:left;">Ordre du jour provisoire de la 7424e séance ( privée ) du Conseil</td><td style="text-align:left;">Provisional agenda for the 7424th (closed) meeting of the Council</td></tr><tr><td style="text-align:center;">target</td><td style="text-align:left;">安全 理事会 第 7424 次 ( 闭门 ) 会议 临时 议程</td><td style="text-align:left;">Security Council, 7424th (closed) meeting, provisional, agenda</td></tr><tr><td style="text-align:center;">Direct</td><td style="text-align:left;">事实上 ， 国际 货币 基金 组织 的 国际 货币 基金 组织 （ IMF ）</td><td style="text-align:left;">In fact, international, monetary, fund, organization, international, monetary, fund, organization (IMF)</td></tr><tr><td style="text-align:center;">mRASP</td><td style="text-align:left;">安理会 第 7424 次 （ 非 公开 ） 会议 临时 议程</td><td style="text-align:left;">Council, 7424th (closed) meeting, provisional, agenda</td></tr></tbody></table><h4 id="dutch-portuguese-nl-pt" tabindex="-1"><a class="header-anchor" href="#dutch-portuguese-nl-pt"><span>Dutch-Portuguese (Nl-Pt)</span></a></h4><ul><li>Exotic Full, 12,000 parallel sentence pairs</li><li>Direct 0 BLEU vs mRASP 14.1 BLEU</li></ul><p>We find that the translation system obtained by mRASP can not successfully translate every detail, but it can grasp the key information of the original text. For example, in the following example (1) date (2) minutes of the meeting &lt;-&gt; news of meeting (3) circulated &lt;-&gt; shared.</p><table><thead><tr><th style="text-align:center;"></th><th style="text-align:left;">Original Text</th><th style="text-align:left;">Translation in English</th></tr></thead><tbody><tr><td style="text-align:center;">source</td><td style="text-align:left;">de notulen van de vergadering van donderdag 21 september zijn rondgedeeld.</td><td style="text-align:left;">The minutes of the meeting on Thursday, 21 September have been circulated.</td></tr><tr><td style="text-align:center;">target</td><td style="text-align:left;">a acta da sessão de quinta feira , 21 de setembro de 2000 , já foi distribuída.</td><td style="text-align:left;">The minutes of the meeting on Thursday, 21 September 2000 have now been distributed.</td></tr><tr><td style="text-align:center;">Direct</td><td style="text-align:left;">Os governos, os líderes mundiais dos seus próprios.</td><td style="text-align:left;">Governments, their own world leaders.</td></tr><tr><td style="text-align:center;">mRASP</td><td style="text-align:left;">As notícias da reunião do dia 21 de Setembro foram partilhadas.</td><td style="text-align:left;">News of the September 21 meeting has been shared.</td></tr></tbody></table><h4 id="english-french-en-fr" tabindex="-1"><a class="header-anchor" href="#english-french-en-fr"><span>English-French (En-Fr)</span></a></h4><ul><li>We found that one of the advantages of the model trained by the mRASP method over the Direct method is that the Direct system tends to ignore meaningless words (such as articles, deixis, etc.), while the mRASP maintains the consistency of articles and deixis.</li></ul><table><thead><tr><th style="text-align:center;"></th><th style="text-align:left;">Text</th></tr></thead><tbody><tr><td style="text-align:center;">source</td><td style="text-align:left;">An investigation is under way to find the cause of the fire .</td></tr><tr><td style="text-align:center;">target</td><td style="text-align:left;">Une enquête est en cours pour trouver la cause de cet incendie .</td></tr><tr><td style="text-align:center;">Direct</td><td style="text-align:left;">enquête est en cours pour déterminer la cause de l&#39; incendie .</td></tr><tr><td style="text-align:center;">mRASP</td><td style="text-align:left;">Une enquête est en cours pour trouver la cause de l&#39; incendie .</td></tr></tbody></table><table><thead><tr><th style="text-align:center;"></th><th style="text-align:left;">Text</th></tr></thead><tbody><tr><td style="text-align:center;">source</td><td style="text-align:left;">After Broadway and London , Paris is finally finding its voice .</td></tr><tr><td style="text-align:center;">target</td><td style="text-align:left;">Après Broadway et Londres , Paris trouve enfin sa voix .</td></tr><tr><td style="text-align:center;">Direct</td><td style="text-align:left;">Broadway et Londres , Paris trouve enfin sa voix .</td></tr><tr><td style="text-align:center;">mRASP</td><td style="text-align:left;">Après Broadway et Londres , Paris trouve enfin sa voix .</td></tr></tbody></table><h4 id="english-chinese-en-zh" tabindex="-1"><a class="header-anchor" href="#english-chinese-en-zh"><span>English-Chinese (En-Zh)</span></a></h4><table><thead><tr><th style="text-align:center;"></th><th style="text-align:left;">Original Text</th><th style="text-align:left;">Translation in English</th></tr></thead><tbody><tr><td style="text-align:center;">source</td><td style="text-align:left;">and for the middle class.</td><td style="text-align:left;"></td></tr><tr><td style="text-align:center;">target</td><td style="text-align:left;">对中产阶级而言。</td><td style="text-align:left;">For the middle class.</td></tr><tr><td style="text-align:center;">Direct</td><td style="text-align:left;">还有中产阶级。</td><td style="text-align:left;">And the middle class.</td></tr><tr><td style="text-align:center;">mRASP</td><td style="text-align:left;">对中产阶级而言。</td><td style="text-align:left;">For the middle class.</td></tr></tbody></table><h3 id="findings-from-mrasp-trained-model" tabindex="-1"><a class="header-anchor" href="#findings-from-mrasp-trained-model"><span>Findings from mRASP trained model</span></a></h3><p>As a general pre-training model, where does the improvements of mRASP for downstream MT tasks come from?</p><p>The author believes that its improvements mainly comes from two aspects:</p><ol><li>mRASP narrows the gap between the vector representation of synonyms across different languages</li><li>mRASP narrows the gap between the vector representation of synonymous sentences across different languages</li></ol><p>The narrowing of the gap between word-level and sentence-level representations means that after learning parallel sentence pairs in a large number of languages in the pre-training stage, mRASP implicitly &quot;mastered&quot; the language-independent representation, which can be migrated to any language, so mRASP can generally improve the effect of downstream machine translation tasks.</p><h4 id="_1-mrasp-draws-word-level-vector-representation-of-different-language-closer" tabindex="-1"><a class="header-anchor" href="#_1-mrasp-draws-word-level-vector-representation-of-different-language-closer"><span>1. mRASP draws word-level vector representation of different language closer</span></a></h4><p>RAS is introduced by making the same context shared between synonyms across different languages. Since the word vector is determined by the context, RAS further draws the representation of synonyms across different languages closer.</p><p>Up: w/o RAS, Down: w/ RAS</p><p>It can be seen that with the RAS method, the embedding distribution between different languages is drawn closer (the angle becomes smaller).</p><p><img src="/blog/assets/analysis-en-zh-direct-DJ1vi00K.png" alt="image10"></p><p><img src="/blog/assets/analysis-en-zh-mrasp-CKsoeG8e.png" alt="image11"></p><h4 id="_2-mrasp-draws-sentence-level-vector-representation-of-different-language-closer" tabindex="-1"><a class="header-anchor" href="#_2-mrasp-draws-sentence-level-vector-representation-of-different-language-closer"><span>2. mRASP draws sentence-level vector representation of different language closer</span></a></h4><p>mRASP narrows the gap between the representation of synonyms, as well as the vector representation of semantics.</p><p>We use the encoder output vector as the representation of the sentence (L2 normalized averaged-pooled encoded output). From the TED parallel test set (filtered 15-way parallel test set, a total of 2284), we match the nearest sentence based on similarity score (cosine similarity), then calculate the Top-1 accuracy (sentence retrieval accuracy).</p><p>Figure 1: The accuracy of mRASP minus the accuracy of mBART [9]. Note that Dutch (Nl) has never appeared in the mRASP pre-training data, and the accuracy in other directions is much higher than that of mBART.</p><ul><li>The average accuracy of mRASP retrieval reached 76%</li></ul><p>Figure 2: Accuracy of mRASP minus the accuracy of mRASP w/o RAS. It can be seen that RAS has obvious benefits on languages (Nl) that did not appear in the pre-training stage.</p><p>Figure 3: After removing the language identifier (Language token) at the beginning of the sentence, the accuracy of Nl can be further improved, at a sacrifice that the accuracy of other languages is greatly reduced.</p><p><img src="/blog/assets/analysis-1-DWWf4lxM.png" alt="image12"></p><p><img src="/blog/assets/analysis-2-BfOj5F8b.png" alt="image13"></p><p><img src="/blog/assets/analysis-3-Bs1HOuHY.png" alt="image14"></p><p>It can be seen that RAS does further draws closer the semantic vector representation, and synonymous sentences will be closely represented after mRASP.</p><h2 id="summary" tabindex="-1"><a class="header-anchor" href="#summary"><span>Summary</span></a></h2><p>Back to the beginning of the article, Mr. Chao, a language genius, has mastered 33 dialects plus 7 foreign languages in his life. From Baoding in the north China to Fuzhou in the south, from the upper reaches to the lower reaches of the Yangtze River, from Berkeley in the United States to Paris in France, he can speak local languages with a local accent. And the establishment of a unified multilingual and cross-domain translation model is one of the ultimate goals of machine translation research. mRASP, which is in line with the language genius Yuen Ren Chao, has established a successful path from multilingual pre-training to fine-tuning to multiple machine translation models, which will also become a new paradigm of machine translation. ByteDance has applied this technology to the Volctrans system and you can try it in the web page attached at the end of the text. We are looking forward to the continuous emergence of new methods in this direction, making great strides towards the ultimate machine translation goal. In the next few years, the progress of machine translation can help everyone in dozens of countries become &quot;Yuen Ren Chao&quot; and truly communicate without language barriers.</p><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references"><span>References</span></a></h2><p>[1] Lin, Zehui, et al. &quot;Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information.&quot; In the Conference on Empirical Methods in Natural Language Processing (2020).</p><p>[2] Devlin, Jacob, et al. &quot;Bert: Pre-training of deep bidirectional transformers for language understanding.&quot; NAACL-HLT (1) 2019: 4171-4186.</p><p>[3] Thomas, Reed, and Callie Mady. &quot;Teaching for transfer: Insights from theory and practices in primary-level French-second-language classrooms.&quot; McGill Journal of Education/Revue des sciences de l&#39;éducation de McGill 49.2 (2014): 399-416.</p><p>[4] Johnson, Melvin, et al. &quot;Google’s multilingual neural machine translation system: Enabling zero-shot translation.&quot; Transactions of the Association for Computational Linguistics 5 (2017): 339-351.</p><p>[5] Radford, Alec, et al. &quot;Improving language understanding by generative pre-training.&quot; (2018): 12.</p><p>[6] Vaswani, Ashish, et al. &quot;Attention is all you need.&quot; Advances in neural information processing systems. 2017.</p><p>[7] Song, Kaitao, et al. &quot;MASS: Masked Sequence to Sequence Pre-training for Language Generation.&quot; ICML. 2019.</p><p>[8] Lewis, Mike, et al. &quot;Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.&quot; ACL 2020: 7871-7880</p><p>[9] Liu, Yinhan, et al. &quot;Multilingual denoising pre-training for neural machine translation.&quot; TACL.2020</p><p>[10] Yang, et al. &quot;Towards Making the Most of BERT in Neural Machine Translation&quot; AAAI.2020</p></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="vp-link vp-external-link-icon vp-meta-label" href="https://github.com/lileicc/blog/edit/main/mt/mrasp/README.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page<!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">Last update: </span><!----></div><div class="contributors"><span class="vp-meta-label">Contributors: </span><!--[--><!--[--><span class="vp-meta-info" title="email: lileicc@gmail.com">Lei Li</span><!--]--><!--]--></div></div></footer><!----><div id="vp-comment" class="giscus-wrapper input-top" style="display:block;"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" preserveAspectRatio="xMidYMid" viewBox="0 0 100 100"><circle cx="28" cy="75" r="11" fill="currentColor"><animate attributeName="fill-opacity" begin="0s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></circle><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 47a28 28 0 0 1 28 28"><animate attributeName="stroke-opacity" begin="0.1s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 25a50 50 0 0 1 50 50"><animate attributeName="stroke-opacity" begin="0.2s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path></svg></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">Li Lab</div><div class="vp-copyright">Copyright © 2024 Xiao Pan </div></footer></div><!--]--><!--[--><!----><!----><!--]--><!--]--></div>
    <script type="module" src="/blog/assets/app-DvxBpnKI.js" defer></script>
  </body>
</html>
