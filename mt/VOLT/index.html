<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.11" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.43" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://lileicc.github.io/blog/mt/VOLT/"><meta property="og:site_name" content="Li-Lab Blog"><meta property="og:title" content="Learning Optimal Vocabularies for Machine Translation with only CPU"><meta property="og:description" content="Constructing a vocabulary is a fisrt step for any NLP tasks. How can we efficiently learn an optimal vocabulary for machine translation? In this blog, I will explain the VOLT al..."><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2024-05-25T23:17:24.000Z"><meta property="article:author" content="Ahmed Elkordy"><meta property="article:tag" content="Multilingual MT"><meta property="article:tag" content="Vocabulary Learning"><meta property="article:tag" content="Optimal Transport"><meta property="article:published_time" content="2022-05-17T00:00:00.000Z"><meta property="article:modified_time" content="2024-05-25T23:17:24.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Learning Optimal Vocabularies for Machine Translation with only CPU","image":[""],"datePublished":"2022-05-17T00:00:00.000Z","dateModified":"2024-05-25T23:17:24.000Z","author":[{"@type":"Person","name":"Ahmed Elkordy"}]}</script><title>Learning Optimal Vocabularies for Machine Translation with only CPU | Li-Lab Blog</title><meta name="description" content="Constructing a vocabulary is a fisrt step for any NLP tasks. How can we efficiently learn an optimal vocabulary for machine translation? In this blog, I will explain the VOLT al...">
    <link rel="preload" href="/blog/assets/style-Cq8eyGeZ.css" as="style"><link rel="stylesheet" href="/blog/assets/style-Cq8eyGeZ.css">
    <link rel="modulepreload" href="/blog/assets/app-DiCjo-Va.js"><link rel="modulepreload" href="/blog/assets/index.html-r3wL3lYW.js"><link rel="modulepreload" href="/blog/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/blog/assets/index.html-n7qbu3dk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DsZ03cqp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BTmZnjzu.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B3lBlHt5.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DkOZIlpg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CFyAOJxa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Dy--DCvM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BGcY7GRM.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CnEewnS_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B-7kV_Pa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Dxt46wjr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ailBi3IV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CLkHMPSR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BcxjbFyQ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B5tQQqUs.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-7GEx1xL_.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-X1I--pzi.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Cqxm4VEw.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-dZRpMDcp.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-rdxcuHG-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B0QaSsCk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-lbmWsbm6.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BL20rJEw.js" as="script"><link rel="prefetch" href="/blog/assets/404.html-DwhX0xCD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-eyD8xvL-.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-4T5RsOJr.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C4oYB-TX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DT0Vw-X1.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ChpH1XCa.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CG4zWerc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-cDcguak7.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BLmhJl3H.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BUnffEMv.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-K8vN4nvk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-izeVRWR8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-p4XBto_y.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-l1eZfZeq.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DggTdQeV.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BqVwiDmI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-ChpRCIc8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-5hhedaTR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CIWPrBfh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BYBCTXZh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DWU1ODhk.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BbKNZ18w.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BkwXzOb3.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-jhqGT4dL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CS6gUaSX.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CGvZjib9.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DkRkbNQ6.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Ic0xQiYz.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Bi8bMppE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DPfrum7j.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BYsu2jJc.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-chNhRanf.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BXwcVWaJ.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B6PjGDAO.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-nmkosgT0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C-1PzENR.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BaWZvSMh.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-iCm852ld.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BDc3S-C2.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CoxTWtqI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BW3VPrzj.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DLbZgarF.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DZPCdxOC.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CvBuB7g8.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BQK4OLG0.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-fRbI1bsI.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-Byllwfsd.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DNApUOep.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BHiIQnkE.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-BuHhtC_q.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DW1rLQdB.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-CFKdo3WH.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B8BC64BL.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-xb2wQrjg.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-B6uJhqIN.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DrA_NyPl.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-C7_gn4qD.js" as="script"><link rel="prefetch" href="/blog/assets/index.html-DPhNyCil.js" as="script"><link rel="prefetch" href="/blog/assets/giscus--_FS5kYt.js" as="script"><link rel="prefetch" href="/blog/assets/auto-CAdRPfCH.js" as="script"><link rel="prefetch" href="/blog/assets/index-wZ-hXvzw.js" as="script"><link rel="prefetch" href="/blog/assets/photoswipe.esm-SzV8tJDW.js" as="script"><link rel="prefetch" href="/blog/assets/SearchResult-75TAtN8b.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><div class="theme-container no-sidebar external-link-icon has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><!--[--><a class="route-link vp-brand" href="/blog/"><img class="vp-nav-logo" src="/blog/logo.svg" alt><!----><span class="vp-site-name hide-in-pad">Li-Lab Blog</span></a><!--]--><!----></div><div class="vp-navbar-center"><!----><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/" aria-label="Blog Home"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="home" width="1em" height="1em"></iconify-icon>Blog Home<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/category/" aria-label="Category"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="baseline-category" width="1em" height="1em"></iconify-icon>Category<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/tag/" aria-label="Tags"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="tag" width="1em" height="1em"></iconify-icon>Tags<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link vp-link" href="/blog/timeline/" aria-label="Timeline"><iconify-icon class="font-icon icon" style="" mode="style" inline icon="time" width="1em" height="1em"></iconify-icon>Timeline<!----></a></div></nav><!--]--><!----></div><div class="vp-navbar-end"><!----><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/lileicc/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon" name="outlook"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="vp-outlook-dropdown"><!----></div></button></div><!--[--><button type="button" class="search-pro-button" aria-label="Search"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon" name="search"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">Search</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!----><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!----><ul class="vp-sidebar-links"></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Learning Optimal Vocabularies for Machine Translation with only CPU</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Ahmed Elkordy</span></span><span property="author" content="Ahmed Elkordy"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2022-05-17T00:00:00.000Z"></span><span class="page-category-info" aria-label="Category🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color0 clickable" role="navigation">MT</span><!--]--><meta property="articleSection" content="MT"></span><span class="page-tag-info" aria-label="Tag🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color8 clickable" role="navigation">Multilingual MT</span><span class="page-tag-item color8 clickable" role="navigation">Vocabulary Learning</span><span class="page-tag-item color4 clickable" role="navigation">Optimal Transport</span><!--]--><meta property="keywords" content="Multilingual MT,Vocabulary Learning,Optimal Transport"></span><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 9 min</span><meta property="timeRequired" content="PT9M"></span></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!----><div class="vp-toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon" name="print"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#introduction">Introduction</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#issues-with-current-tokenization-models">Issues with current tokenization models</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#techniques-of-volt">Techniques of VOLT</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#effectiveness-of-volt">Effectiveness of VOLT</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_1-overall-better-performance-than-widely-used-vocabularies">1.	Overall better performance than widely used vocabularies</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_2-low-resource-consumption">2. Low Resource Consumption</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="#_3-versatile-better-on-a-large-array-of-languages">3.	Versatile, better on a large array of languages</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#summary">Summary</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="#references">References</a></li><!----><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!----></aside></div><!----><div class="theme-hope-content"><p>Constructing a vocabulary is a fisrt step for any NLP tasks. How can we efficiently learn an optimal vocabulary for machine translation? In this blog, I will explain the VOLT algorithm from the paper <em>Vocabulary Leaning via Optimal Transport for Neural Machine Translation</em>, which was awarded <strong>the Best Paper</strong> at ACL 2021.</p><p>Paper: <a href="https://arxiv.org/pdf/2012.15671.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2012.15671.pdf</a></p><p>Code: <a href="https://github.com/Jingjing-NLP/VOLT" target="_blank" rel="noopener noreferrer">https://github.com/Jingjing-NLP/VOLT</a></p><h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction"><span>Introduction</span></a></h2><p>At its core, the paper aims to find a way to reduce resource consumption and computational times of machine translation&#39;s vocabulary building algorithms. The method we will explore today could result in machine translation models running much faster on machines with much less computational power than what is required now. A key part of translating between one language and another is taking the text in the original language and breaking it down into tokens to make the translation process much simpler by using the tokens as a foundation of words or characters to use in the translation. Tokens are just key elements of the text, they can be words, characters, or even sub-words. A sub-word is just a part of a word. For instance, a sub-word for both the words &quot;lower&quot; and &quot;lowly&quot; could be &quot;low&quot;. The set of tokens that get produced from a piece of text is called a token vocabulary. Here is an example of a sentence and some possible token vocabularies from that sentence.</p><p><img src="/blog/assets/Vocab_Example-DAocYjyR.png" alt="image7"></p><p>Which one of these generated token vocabularies is better? The word level vocabulary has the advantage of the smallest size, but it runs into the following issue. If there is a word or character we cannot construct with our token vocabulary, we denote this as an unknown or out of vocabulary token and label it as [UNK] or [OOV]. Obviously, the goal is to have as few [UNK] tokens as possible. With word vocabularies, there is a higher chance of [UNK] tokens since any word not in our training corpus will not be constructable with our generated vocabulary. On the opposite end of the spectrum, the character level token will have very few [UNK] tokens as we can construct most tokens with characters in our vocabulary. However, the size of the token vocabulary, as can be seen from the example above, is quite large. Sub-word vocabularies are a good mix between word and character level vocabularies as they can construct tokens that would otherwise be [UNK] and have a manageable size compared to character level vocabularies. Even though we now know that sub-word vocabularies are the preferred type, we still do not know what vocabulary we should choose from given all the possible sub-word vocabularies. For instance, going back to the previous example we can have many sub-word vocabularies generated.</p><p><img src="/blog/assets/Vocab_Example2-BNEKh32G.png" alt="image8"></p><p>These 3 generated vocabularies are all sub-word vocabularies with different sizes and abilities to construct words. Just by looking at it, there is no way to tell which of these vocabularies is the best. Hence, we must construct a systematic way to find which vocabulary is better. This is where VOLT comes in. VOLT aims to find the best possible token vocabulary that can be generated from a piece of text in an efficient manner. VOLT and other vocabulary generation algorithms use the concept of entropy. Entropy just means the amount of information present in each token. If a certain word or character occurs a lot in a piece of text, then each time we add it as a token we add a relatively small amount of information since its already widely present in the text. However, if a word occurs only once then adding it to our vocabulary would be quite valuable as we only add it that one time. Words or character that occur often have a higher entropy than low occurring words or characters. Intuitively, a vocabulary with the lowest possible entropy would be optimal as it holds a lot of information per token. The main objective of VOLT is to find the best vocabulary in terms of both optimizing size and entropy. As we’ll see later, this often results in much smaller vocabulary sizes than vocabularies generated by other algorithms.</p><p>VOLT has other advantages other than reducing the vocabulary size. Firstly, VOLT does not only reduce size but produces an overall better vocabulary than current methods. We will see performance metrics later that show this off. Secondly, VOLT works well on multilingual MT settings. This means that VOLT performs better on a more diverse range of languages than other methods such as Byte-Pair Encoding (BPE) [1]. Lastly, VOLT uses less resources and computational time than current prevailing methods. For instance, on English to German translation, VOLT uses 30 GPU hours while conventional methods such as BPE-Search take 384 GPU hours. This is because VOLT does not require trail training when it comes to computing the optimal vocabulary. Trail training just means having to iterate through all possible vocabulary sizes to find the best vocabulary at the best size, this methodology is extremely inefficient and so its use is avoided in VOLT.</p><p>The question to now be asked is why is VOLT necessary. Why do we need to reduce vocabulary size in the first place? In this next section, we will discuss the current issues with other tokenization methods and why VOLT is necessary.</p><h2 id="issues-with-current-tokenization-models" tabindex="-1"><a class="header-anchor" href="#issues-with-current-tokenization-models"><span>Issues with current tokenization models</span></a></h2><p>Most current translation models use word-level vocabularies. As discussed above, sub-word vocabularies tend to be overall better. Using a sub-word level encoding can help decrease the sparsity of tokens and increase the shared feature of similar words. My previous example of taking “low” from “lowly” and “lower” previews how we can have these shared features between words. Furthermore, sub-level vocabularies tend to have shorter sentence lengths and no rare words compares to character level vocabularies.</p><p>Sub-word vocabularies seem great, but the issue lies with the methods the use them. Here is an example of an analysis on piece of text done by BPE.</p><p><img src="/blog/assets/BPE-Example-f7JdCgeo.png" alt="image1"></p><p>BPE works by merging frequent character sequences to make sub-words for the token vocabulary. In the above image. A hyphen is placed in every position there is a possible merge of multiple characters into a sub-word. Nonetheless, our concern here is not how BPE works but the fact that its analysis focuses on how often a character sequence occurs and merges frequent character sequences together. However, BPE does not consider any of the features, namely size, of the resulting vocabulary.</p><p>The question here becomes, if the problem with BPE is size why not just run BPE for all possible sizes and pick the best one. To do this, we would have to run BPE-1K, BPE-2K, BPE-3K, … all the way up to BPE-60K. For each one we would have to train the model on a collection of text and then conduct tests to retrieve BLEU score for each model, which is a performance metric, and then pick the vocabulary with the highest score. It is not difficult to see that this would take extremely long to produce the best vocabulary. Just running BPE on 12 different sizes and getting the best vocabulary takes <strong>384</strong> computing hours on the GPU as we will see later. Therefore, VOLT is needed, it allows us to find a vocabulary without having to iterate through every possible vocabulary size to find the optimal one. In the next section, we will discuss how VOLT manages to achieve this.</p><h2 id="techniques-of-volt" tabindex="-1"><a class="header-anchor" href="#techniques-of-volt"><span>Techniques of VOLT</span></a></h2><p>To attempt to optimize both entropy and size, we will use the idea of Marginal Utility. In economics, marginal utility is the amount of satisfaction a consumer attains from consuming a unit of a product. It is used to balance between the benefit and the cost. Let’s look at the following example to make things clear. A high school is buying laptops in bulk for its students, it needs to find the optimal amount and price to buy these laptops at. Here are some of its options</p><p><img src="/blog/assets/Utility_Example-CIt_xXWs.png" alt="image9"></p><p>While it may seem like an easy choice for the school to just buy 400 laptops. 400 laptops might be too much, meaning laptops would go unused and the school would have wasted money. Hence, the optimal choice would be choosing 200 laptops which would fit the school’s needs at a good price. This is the concept of marginal utility. We apply this same concept in the paper by using vocabulary size as the cost and the value is the information per character, which as defined before is entropy. We define a new concept, the Marginal Utility of Vocabularization (MUV), which finds the best trade-off between size and entropy. For VOLT to find the optimal vocabulary, it needs to find the vocabulary with the highest MUV.</p><p><img src="/blog/assets/UtilityMUV_Example-Bfa4jczD.png" alt="MUV_Example"></p><p>Intuitively, the equation for MUV is the derivation of entropy with respect to size as we want to know how much we are sacrificing in terms of a larger vocabulary size with every drop in vocabulary entropy. Now that MUV is defined, we can view VOLT as an optimal transport problem. There have been algorithms developed to efficiently solve optimal transport problems such as the Sinkhorn algorithm which is used here to find the vocabulary with the highest MUV, which is the optimal vocabulary.</p><p>Our initial goal was to find the vocabulary with the highest BLEU score, this was then simplified to the vocabulary with the highest MUV. With an equation for MUV as defined above, we further reduced this to an optimal transport problem which already has well known solution. Lets take a look at how effective the usage of MUV is.</p><p>Initial results show that the usage of MUV has a correlation with two-thirds of tasks performed. Let’s now look at a few figures to clarify the points being made here.</p><p><img src="/blog/assets/VOLT-MUV-Example-C3g5Pmq6.png" alt="VOLT-MUV"><img src="/blog/assets/VOLT-MUVCorrelation-Example-CBKCDC-Y.png" alt="VOLT-MUV-Correlation"></p><p>Firstly, it’s helpful to understand that BLEU and Spearman score are just two performance metrics and not knowing the details of how they work does not affect one’s ability to understand these figures. In the first model, we have entropy on the y-axis and size on the x-axis. Notice the roughly inversely proportional relationship between them. The BLEU score is also graphed, and a star is placed at the vocabulary with highest MUV. Notice that the starred vocabulary is the one corresponding to the vocabulary with the highest BLEU score, meaning it is the best performing vocabulary.</p><p>For the second figure, experiments were conducted on 45 language pairs and the spearman score between MUV and BLEU were recorded for each pair. Spearman score here is just a correlation score where the higher the correlation between MUV and performance, the higher the spearman score. The figure shows the results with the spearman score on the x-axis and the number of generated vocabularies that correspond to a certain spearman score range on the y-axis. The results show that for two-thirds of the generated vocabularies there is a positive spearman score indicating a correlation between MUV and performance.</p><h2 id="effectiveness-of-volt" tabindex="-1"><a class="header-anchor" href="#effectiveness-of-volt"><span>Effectiveness of VOLT</span></a></h2><p>The experiments shown here are conducted from one of the three following datasets. The WMT-14 English-German dataset which has 4.5 M English-German sentence pairs. The TED bilingual dataset where we chose 12 different language pairs that had the most training data. Lastly, the TED multilingual dataset where we chose 52 language pairs on a many-to English setting. Here are some of the main advantages of VOLT:</p><h3 id="_1-overall-better-performance-than-widely-used-vocabularies" tabindex="-1"><a class="header-anchor" href="#_1-overall-better-performance-than-widely-used-vocabularies"><span>1. Overall better performance than widely used vocabularies</span></a></h3><p>In a paper by Shuoyang Ding [2], it was found that among the 42 papers accepted for the Conference of Machine Translation(WMT), the most common size was 30K-40K. Hence, we compare VOLT’s BLEU (performance) scores with a popular method such as Byte Pair Encoding with a 30K vocabulary size. Here are the results:</p><p><img src="/blog/assets/RuEn_Example-BetiW13F.png" alt="RutoEn"><img src="/blog/assets/HeEn_Example-BbJZTfCd.png" alt="HetoEn"><img src="/blog/assets/ArEn_Example-D34Uppv2.png" alt="ArtoEn"><img src="/blog/assets/ItEn_Example-BygBuggM.png" alt="IttoEn"></p><p>In all of these examples, not only does VOLT produce a vocabulary that is almost 10x smaller than that of BPE-30K, but it also actually outperforms it as well in terms of BLEU score. Out of the 24 some language to English translation, VOLT outperformed BPE-30K on 23 of them.</p><h3 id="_2-low-resource-consumption" tabindex="-1"><a class="header-anchor" href="#_2-low-resource-consumption"><span>2. Low Resource Consumption</span></a></h3><p>To explore VOLT’s resource consumption in respect to other methods, we will first run BPE-1K, BPE-2K, BPE-3K, BPE-4K, BPE-5K, BPE-6K, BPE-7K, BPE-8K, BPE-9K, BPE-10K, BPE-20K, and BPE 30K and select the best performing vocabulary out of those produced by these runs. We will cause this method of running several BPE’s and selecting the best one BPE-Search. We will compare BPE-Search with VOLT to get the following results:</p><p><img src="/blog/assets/Green_Example-C5yB3F3c.png" alt="image5"></p><p>In the figure above, GH and CH are GPU Hours and CPU Hours respectively. The results seem to indicate that the performance score of both is extremely similar. However, VOLT seems to be a better option as it produces a vocabulary in 300+ less computing hours than BPE-Search, making it a much more efficient algorithm.</p><h3 id="_3-versatile-better-on-a-large-array-of-languages" tabindex="-1"><a class="header-anchor" href="#_3-versatile-better-on-a-large-array-of-languages"><span>3. Versatile, better on a large array of languages</span></a></h3><p>To test how good VOLT is across as many languages as possible. We conduct experiments to compare VOLT to BPE-60K as that it the most popular size setting for multi-lingual translation tasks. Here are the results:</p><p><img src="/blog/assets/Versatility_Example-CHofUrSs.png" alt="image6"></p><p>The figure shows results for BPE-60K and VOLT on 20 different languages to English translation tasks. VOLT performs better than BPE-60K on 18 of those languages which shows how versatile VOLT is.</p><h2 id="summary" tabindex="-1"><a class="header-anchor" href="#summary"><span>Summary</span></a></h2><p>A key part of machine translation is the process of vocabulary building and tokenization. For one language to be translated to another, we must extract all the key elements of the source language text such as words and characters and use those to translate the text to another language. Traditionally, tokenization methods tend to focus on how frequently a word or character pair occurs to decide whether to make it a token. However, these methods do not consider how big the token set is going to end up being, which also affects the performance of the resulting token set on translation tasks. The method introduced in this paper, VOLT, considers both word frequency and size to produce better performing token sets. In addition, VOLT does not iterate through all possible sizes to find the best one; instead, it treats the problem as an optimal transport problem. This means it applies constraints on the possible optimal sizes and utilizes transport matrices to come up with the optimal size. VOLT is an important method when it comes future machine translation models as it could result in faster training times and better performance all in less computing hours.</p><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references"><span>References</span></a></h2><p>[1] Sennrich, Haddow, et al. “Neural machine translation of rare words with subword units.” In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics</p><p>[2] Ding, Renduchintala, et al. “A call for prudent choice of subword merge operations in neural machine translation.” In Proceedings of Machine Translation Summit XVII Volume 1: Research Track, MTSummit 2019, Dublin, Ireland, August 19-23, 2019, pages 204– 213. European Association for Machine Translation.</p></div><!----><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="vp-link vp-external-link-icon vp-meta-label" href="https://github.com/lileicc/blog/edit/main/mt/VOLT/README.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page<!----></a></div><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">Last update: </span><!----></div><div class="contributors"><span class="vp-meta-label">Contributors: </span><!--[--><!--[--><span class="vp-meta-info" title="email: lileicc@gmail.com">Lei Li</span><!--]--><!--]--></div></div></footer><!----><div id="vp-comment" class="giscus-wrapper input-top" style="display:block;"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" preserveAspectRatio="xMidYMid" viewBox="0 0 100 100"><circle cx="28" cy="75" r="11" fill="currentColor"><animate attributeName="fill-opacity" begin="0s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></circle><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 47a28 28 0 0 1 28 28"><animate attributeName="stroke-opacity" begin="0.1s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path><path fill="none" stroke="#88baf0" stroke-width="10" d="M28 25a50 50 0 0 1 50 50"><animate attributeName="stroke-opacity" begin="0.2s" dur="1s" keyTimes="0;0.2;1" repeatCount="indefinite" values="0;1;1"></animate></path></svg></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">Li Lab</div><div class="vp-copyright">Copyright © 2024 Ahmed Elkordy </div></footer></div><!--]--><!--[--><!----><!----><!--]--><!--]--></div>
    <script type="module" src="/blog/assets/app-DiCjo-Va.js" defer></script>
  </body>
</html>
