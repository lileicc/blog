const e=JSON.parse('{"key":"v-4e141a4b","path":"/dl4mt/2021/cpgmt/","title":"Contextual Parameter Generation for Universal Neural Machine Translation","lang":"en-US","frontmatter":{"title":"Contextual Parameter Generation for Universal Neural Machine Translation","author":"Bairu Hou","date":"2021-11-28T00:00:00.000Z","tag":["Multilingual MT"],"category":["MT","DL4MT"],"description":"Introduction A typical neural machine translation (NMT) system needs to support the translation among various languages, that is, a multilingual(many-to-many) NMT system rather than only support the translation between two languages. However, to support the multilingual translation is still a challenge. One direct idea is to use a separate model to translation one language to another language, which is very easy to implement but brings high costs: To support the translation among N languages, we need train N(N-1)/2 separate models. Such a method does not allow the sharing of information across languages, which can result in overparameterization and sub-optimal in performance. We denote this method as per-language NMT","head":[["meta",{"property":"og:url","content":"https://lileicc.github.io/blog/dl4mt/2021/cpgmt/"}],["meta",{"property":"og:site_name","content":"Li-Lab Blog"}],["meta",{"property":"og:title","content":"Contextual Parameter Generation for Universal Neural Machine Translation"}],["meta",{"property":"og:description","content":"Introduction A typical neural machine translation (NMT) system needs to support the translation among various languages, that is, a multilingual(many-to-many) NMT system rather than only support the translation between two languages. However, to support the multilingual translation is still a challenge. One direct idea is to use a separate model to translation one language to another language, which is very easy to implement but brings high costs: To support the translation among N languages, we need train N(N-1)/2 separate models. Such a method does not allow the sharing of information across languages, which can result in overparameterization and sub-optimal in performance. We denote this method as per-language NMT"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://lileicc.github.io/blog/"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2022-09-13T03:45:15.000Z"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:alt","content":"Contextual Parameter Generation for Universal Neural Machine Translation"}],["meta",{"property":"article:author","content":"Bairu Hou"}],["meta",{"property":"article:tag","content":"Multilingual MT"}],["meta",{"property":"article:published_time","content":"2021-11-28T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2022-09-13T03:45:15.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Contextual Parameter Generation for Universal Neural Machine Translation\\",\\"image\\":[\\"https://lileicc.github.io/blog/\\"],\\"datePublished\\":\\"2021-11-28T00:00:00.000Z\\",\\"dateModified\\":\\"2022-09-13T03:45:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Bairu Hou\\"}]}"]]},"headers":[{"level":2,"title":"Introduction","slug":"introduction","link":"#introduction","children":[]},{"level":2,"title":"Preliminary","slug":"preliminary","link":"#preliminary","children":[]},{"level":2,"title":"Core Idea: Contextual Parameter Generator","slug":"core-idea-contextual-parameter-generator","link":"#core-idea-contextual-parameter-generator","children":[]},{"level":2,"title":"Dive deeper: the design of CPG","slug":"dive-deeper-the-design-of-cpg","link":"#dive-deeper-the-design-of-cpg","children":[]},{"level":2,"title":"Experiment and Performance","slug":"experiment-and-performance","link":"#experiment-and-performance","children":[]},{"level":2,"title":"Language embeddings","slug":"language-embeddings","link":"#language-embeddings","children":[]},{"level":2,"title":"Summary","slug":"summary","link":"#summary","children":[]},{"level":2,"title":"Reference","slug":"reference","link":"#reference","children":[]}],"git":{"createdTime":1663040715000,"updatedTime":1663040715000,"contributors":[{"name":"Lei Li","email":"lileicc@gmail.com","commits":1}]},"readingTime":{"minutes":6.39,"words":1917},"filePathRelative":"dl4mt/2021/cpgmt/README.md","localizedDate":"November 28, 2021","excerpt":"<h2> Introduction</h2>\\n<p>A typical neural machine translation (NMT) system needs to support the translation among various languages, that is, a multilingual(many-to-many) NMT system rather than only support the translation between two languages. However, to support the multilingual translation is still a challenge. One direct idea is to use a separate model to translation one language to another language, which is very easy to implement but brings high costs: To support the translation among N languages, we need train N(N-1)/2 separate models. Such a method does not allow the sharing of information across languages, which can result in overparameterization and sub-optimal in performance. We denote this method as <em>per-language NMT</em></p>","autoDesc":true}');export{e as data};
