const e=JSON.parse('{"key":"v-60c9a2e1","path":"/dl4mt/2021/gpt/","title":"Generative Pre-trained Transformer (GPT)","lang":"en-US","frontmatter":{"title":"Generative Pre-trained Transformer (GPT)","author":"Alex Rasla","date":"2021-11-01T00:00:00.000Z","tag":["Pre-training","Language Modelling"],"category":["NLG","DL4MT"],"description":"In the past couple years, we have seen the rise of Transformer architectures in Natural Language Processing. Transformers revolutionized the speed and accuracy of machine translation systems, and alleviated the need for Recurrent Neural Networks and LSTMs to derive context and meaning for sequence to sequence modeling. Since the Attention Is All You Need paper was published in 2017, there have been many experimental application and fine-tuning improvements made upon the original model. The latest such improvement is the Generative Pre-Trained Transformer 3, or GPT-3.","head":[["meta",{"property":"og:url","content":"https://lileicc.github.io/blog/dl4mt/2021/gpt/"}],["meta",{"property":"og:site_name","content":"Li-Lab Blog"}],["meta",{"property":"og:title","content":"Generative Pre-trained Transformer (GPT)"}],["meta",{"property":"og:description","content":"In the past couple years, we have seen the rise of Transformer architectures in Natural Language Processing. Transformers revolutionized the speed and accuracy of machine translation systems, and alleviated the need for Recurrent Neural Networks and LSTMs to derive context and meaning for sequence to sequence modeling. Since the Attention Is All You Need paper was published in 2017, there have been many experimental application and fine-tuning improvements made upon the original model. The latest such improvement is the Generative Pre-Trained Transformer 3, or GPT-3."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://lileicc.github.io/blog/"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2022-09-13T04:24:06.000Z"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:alt","content":"Generative Pre-trained Transformer (GPT)"}],["meta",{"property":"article:author","content":"Alex Rasla"}],["meta",{"property":"article:tag","content":"Pre-training"}],["meta",{"property":"article:tag","content":"Language Modelling"}],["meta",{"property":"article:published_time","content":"2021-11-01T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2022-09-13T04:24:06.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Generative Pre-trained Transformer (GPT)\\",\\"image\\":[\\"https://lileicc.github.io/blog/\\"],\\"datePublished\\":\\"2021-11-01T00:00:00.000Z\\",\\"dateModified\\":\\"2022-09-13T04:24:06.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Alex Rasla\\"}]}"]]},"headers":[{"level":2,"title":"Novelty","slug":"novelty","link":"#novelty","children":[]},{"level":2,"title":"Evaluation on Various Tasks","slug":"evaluation-on-various-tasks","link":"#evaluation-on-various-tasks","children":[{"level":3,"title":"Translation","slug":"translation","link":"#translation","children":[]},{"level":3,"title":"Word Prediction and Text Generation","slug":"word-prediction-and-text-generation","link":"#word-prediction-and-text-generation","children":[]},{"level":3,"title":"Comprehension","slug":"comprehension","link":"#comprehension","children":[]}]},{"level":2,"title":"Case Study","slug":"case-study","link":"#case-study","children":[]},{"level":2,"title":"Conclusion","slug":"conclusion","link":"#conclusion","children":[]},{"level":2,"title":"References","slug":"references","link":"#references","children":[]}],"git":{"createdTime":1663040715000,"updatedTime":1663043046000,"contributors":[{"name":"Lei Li","email":"lileicc@gmail.com","commits":2}]},"readingTime":{"minutes":8.13,"words":2438},"filePathRelative":"dl4mt/2021/gpt/README.md","localizedDate":"November 1, 2021","excerpt":"<p>In the past couple years, we have seen the rise of Transformer architectures in Natural Language Processing. Transformers revolutionized the speed and accuracy of machine translation systems, and alleviated the need for Recurrent Neural Networks and LSTMs to derive context and meaning for sequence to sequence modeling. Since the <em>Attention Is All You Need</em> paper was published in 2017, there have been many experimental application and fine-tuning improvements made upon the original model. The latest such improvement is the Generative Pre-Trained Transformer 3, or GPT-3.</p>\\n","autoDesc":true}');export{e as data};
