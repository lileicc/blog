const t=JSON.parse('{"key":"v-d12a3a8c","path":"/mt/ctnmt/","title":"机器翻译中的 BERT 应用","lang":"en-US","frontmatter":{"title":"机器翻译中的 BERT 应用","author":"王明轩","date":"2020-12-31T00:00:00.000Z","tag":["BERT","Pre-training","Catastrophic Forgetting"],"category":["MT"],"description":"​\\t预训练技术，比如 BERT等，在自然语言处理领域，尤其是自然语言理解任务取得了巨大的成功。然而目前预训练技术在文本生成领域，比如机器翻译领域，能够取得什么样的效果，还是一个开放问题。CTNMT 这篇论文，从三个方面介绍这个问题： 预训练技术，比如 BERT或者 GPT 在机器翻译中的应用存在什么挑战？ 针对这些调整，需要怎么最大程度利用预训练知识？ 预训练和机器翻译的融合还有什么潜力？","head":[["meta",{"property":"og:url","content":"https://lileicc.github.io/blog/mt/ctnmt/"}],["meta",{"property":"og:site_name","content":"Li-Lab Blog"}],["meta",{"property":"og:title","content":"机器翻译中的 BERT 应用"}],["meta",{"property":"og:description","content":"​\\t预训练技术，比如 BERT等，在自然语言处理领域，尤其是自然语言理解任务取得了巨大的成功。然而目前预训练技术在文本生成领域，比如机器翻译领域，能够取得什么样的效果，还是一个开放问题。CTNMT 这篇论文，从三个方面介绍这个问题： 预训练技术，比如 BERT或者 GPT 在机器翻译中的应用存在什么挑战？ 针对这些调整，需要怎么最大程度利用预训练知识？ 预训练和机器翻译的融合还有什么潜力？"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2022-10-02T00:01:48.000Z"}],["meta",{"property":"article:author","content":"王明轩"}],["meta",{"property":"article:tag","content":"BERT"}],["meta",{"property":"article:tag","content":"Pre-training"}],["meta",{"property":"article:tag","content":"Catastrophic Forgetting"}],["meta",{"property":"article:published_time","content":"2020-12-31T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2022-10-02T00:01:48.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"机器翻译中的 BERT 应用\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2020-12-31T00:00:00.000Z\\",\\"dateModified\\":\\"2022-10-02T00:01:48.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"王明轩\\"}]}"]]},"headers":[{"level":2,"title":"预训练技术在机器翻译领域存在的挑战—-灾难性遗忘","slug":"预训练技术在机器翻译领域存在的挑战—-灾难性遗忘","link":"#预训练技术在机器翻译领域存在的挑战—-灾难性遗忘","children":[]},{"level":2,"title":"渐进式学习策略--缓解灾难性遗忘问题","slug":"渐进式学习策略-缓解灾难性遗忘问题","link":"#渐进式学习策略-缓解灾难性遗忘问题","children":[{"level":3,"title":"梯度控制","slug":"梯度控制","link":"#梯度控制","children":[]},{"level":3,"title":"基于门控制的融合","slug":"基于门控制的融合","link":"#基于门控制的融合","children":[]},{"level":3,"title":"渐进蒸馏策略","slug":"渐进蒸馏策略","link":"#渐进蒸馏策略","children":[]}]},{"level":2,"title":"实验效果和未来方向","slug":"实验效果和未来方向","link":"#实验效果和未来方向","children":[]}],"git":{"createdTime":1663040715000,"updatedTime":1664668908000,"contributors":[{"name":"Lei Li","email":"lileicc@gmail.com","commits":2}]},"readingTime":{"minutes":6.21,"words":1864},"filePathRelative":"mt/ctnmt/README.md","localizedDate":"December 31, 2020","excerpt":"<p>​\\t预训练技术，比如 BERT等，在自然语言处理领域，尤其是自然语言理解任务取得了巨大的成功。然而目前预训练技术在文本生成领域，比如机器翻译领域，能够取得什么样的效果，还是一个开放问题。CTNMT 这篇论文，从三个方面介绍这个问题：</p>\\n<ol>\\n<li>预训练技术，比如 BERT或者 GPT 在机器翻译中的应用存在什么挑战？</li>\\n<li>针对这些调整，需要怎么最大程度利用预训练知识？</li>\\n<li>预训练和机器翻译的融合还有什么潜力？</li>\\n</ol>\\n","autoDesc":true}');export{t as data};
