const e=JSON.parse('{"key":"v-4e089efc","path":"/dl4mt/2021/comet/","title":"Automatic Machine Translation Evaluation - COMET Explained","lang":"en-US","frontmatter":{"title":"Automatic Machine Translation Evaluation - COMET Explained","author":"Xinyi Wang","date":"2021-12-01T00:00:00.000Z","tag":["MT Evaluation","Pre-training"],"category":["MT","DL4MT"],"description":"Motivation While the advance in deep learning has dramatically improved the machine translation quality, there is little development in the evaluation of machine translation models. The most widely-used metrics like BLEU [Papineni et al., 2002] and METEOR [Lavie and Denkowski, 2009] simply match the n-gram between the hypothesis text and reference text, which is too rigid without considering the variance in ground-truth translations and fail to differentiate the current highest performance machine translation models. They also cannot be accurately correlated with human judgment for a piece of text.","head":[["meta",{"property":"og:url","content":"https://lileicc.github.io/blog/dl4mt/2021/comet/"}],["meta",{"property":"og:site_name","content":"Li-Lab Blog"}],["meta",{"property":"og:title","content":"Automatic Machine Translation Evaluation - COMET Explained"}],["meta",{"property":"og:description","content":"Motivation While the advance in deep learning has dramatically improved the machine translation quality, there is little development in the evaluation of machine translation models. The most widely-used metrics like BLEU [Papineni et al., 2002] and METEOR [Lavie and Denkowski, 2009] simply match the n-gram between the hypothesis text and reference text, which is too rigid without considering the variance in ground-truth translations and fail to differentiate the current highest performance machine translation models. They also cannot be accurately correlated with human judgment for a piece of text."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2022-09-13T03:45:15.000Z"}],["meta",{"property":"article:author","content":"Xinyi Wang"}],["meta",{"property":"article:tag","content":"MT Evaluation"}],["meta",{"property":"article:tag","content":"Pre-training"}],["meta",{"property":"article:published_time","content":"2021-12-01T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2022-09-13T03:45:15.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Automatic Machine Translation Evaluation - COMET Explained\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2021-12-01T00:00:00.000Z\\",\\"dateModified\\":\\"2022-09-13T03:45:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Xinyi Wang\\"}]}"]]},"headers":[],"git":{"createdTime":1663040715000,"updatedTime":1663040715000,"contributors":[{"name":"Lei Li","email":"lileicc@gmail.com","commits":1}]},"readingTime":{"minutes":2.57,"words":772},"filePathRelative":"dl4mt/2021/comet/README.md","localizedDate":"December 1, 2021","excerpt":"<h1> Motivation</h1>\\n<p>While the advance in deep learning has dramatically improved the machine translation quality, there is little development in the evaluation of machine translation models. The most widely-used metrics like BLEU <a href=\\"#bleu\\">[Papineni et al., 2002]</a> and METEOR <a href=\\"#meteor\\">[Lavie and Denkowski, 2009]</a> simply match the n-gram between the hypothesis text and reference text, which is too rigid without considering the variance in ground-truth translations and fail to differentiate the current highest performance machine translation models. They also cannot be accurately correlated with human judgment for a piece of text.</p>","autoDesc":true}');export{e as data};
